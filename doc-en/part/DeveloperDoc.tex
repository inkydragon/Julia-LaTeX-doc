
\part{Developer Documentation}


\hypertarget{4173225665417100846}{}


\chapter{Reflection and introspection}



Julia provides a variety of runtime reflection capabilities.



\hypertarget{17771187696170422072}{}


\section{Module bindings}



The exported names for a \texttt{Module} are available using \hyperlink{6473328671144201991}{\texttt{names(m::Module)}}, which will return an array of \hyperlink{18332791376992528422}{\texttt{Symbol}} elements representing the exported bindings. \texttt{names(m::Module, all = true)} returns symbols for all bindings in \texttt{m}, regardless of export status.



\hypertarget{14774754942140904122}{}


\section{DataType fields}



The names of \texttt{DataType} fields may be interrogated using \hyperlink{17481253338332315021}{\texttt{fieldnames}}. For example, given the following type, \texttt{fieldnames(Point)} returns a tuple of \hyperlink{18332791376992528422}{\texttt{Symbol}}s representing the field names:




\begin{minted}{jlcon}
julia> struct Point
           x::Int
           y
       end

julia> fieldnames(Point)
(:x, :y)
\end{minted}



The type of each field in a \texttt{Point} object is stored in the \texttt{types} field of the \texttt{Point} variable itself:




\begin{minted}{jlcon}
julia> Point.types
svec(Int64, Any)
\end{minted}



While \texttt{x} is annotated as an \texttt{Int}, \texttt{y} was unannotated in the type definition, therefore \texttt{y} defaults to the \texttt{Any} type.



Types are themselves represented as a structure called \texttt{DataType}:




\begin{minted}{jlcon}
julia> typeof(Point)
DataType
\end{minted}



Note that \texttt{fieldnames(DataType)} gives the names for each field of \texttt{DataType} itself, and one of these fields is the \texttt{types} field observed in the example above.



\hypertarget{11371291537025953368}{}


\section{Subtypes}



The \emph{direct} subtypes of any \texttt{DataType} may be listed using \hyperlink{13112219412833772146}{\texttt{subtypes}}. For example, the abstract \texttt{DataType} \hyperlink{11465394427882483091}{\texttt{AbstractFloat}} has four (concrete) subtypes:




\begin{minted}{jlcon}
julia> subtypes(AbstractFloat)
4-element Vector{Any}:
 BigFloat
 Float16
 Float32
 Float64
\end{minted}



Any abstract subtype will also be included in this list, but further subtypes thereof will not; recursive application of \hyperlink{13112219412833772146}{\texttt{subtypes}} may be used to inspect the full type tree.



\hypertarget{11957448814310282424}{}


\section{DataType layout}



The internal representation of a \texttt{DataType} is critically important when interfacing with C code and several functions are available to inspect these details. \hyperlink{16222127093346839171}{\texttt{isbitstype(T::DataType)}} returns true if \texttt{T} is stored with C-compatible alignment. \hyperlink{6956980533195055227}{\texttt{fieldoffset(T::DataType, i::Integer)}} returns the (byte) offset for field \emph{i} relative to the start of the type.



\hypertarget{9741782790992203943}{}


\section{Function methods}



The methods of any generic function may be listed using \hyperlink{3025953302266245919}{\texttt{methods}}. The method dispatch table may be searched for methods accepting a given type using \hyperlink{1845157398882896709}{\texttt{methodswith}}.



\hypertarget{10529563115945815287}{}


\section{Expansion and lowering}



As discussed in the \hyperlink{15430858583934124136}{Metaprogramming} section, the \hyperlink{8018172489611994488}{\texttt{macroexpand}} function gives the unquoted and interpolated expression (\hyperlink{17120496304147995299}{\texttt{Expr}}) form for a given macro. To use \texttt{macroexpand}, \texttt{quote} the expression block itself (otherwise, the macro will be evaluated and the result will be passed instead!). For example:




\begin{minted}{jlcon}
julia> macroexpand(@__MODULE__, :(@edit println("")) )
:(InteractiveUtils.edit(println, (Base.typesof)("")))
\end{minted}



The functions \texttt{Base.Meta.show\_sexpr} and \hyperlink{15981569052160951906}{\texttt{dump}} are used to display S-expr style views and depth-nested detail views for any expression.



Finally, the \hyperlink{6644553029841096787}{\texttt{Meta.lower}} function gives the \texttt{lowered} form of any expression and is of particular interest for understanding how language constructs map to primitive operations such as assignments, branches, and calls:




\begin{minted}{jlcon}
julia> Meta.lower(@__MODULE__, :( [1+2, sin(0.5)] ))
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = 1 + 2
│   %2 = sin(0.5)
│   %3 = Base.vect(%1, %2)
└──      return %3
))))
\end{minted}



\hypertarget{5101839880113461479}{}


\section{Intermediate and compiled representations}



Inspecting the lowered form for functions requires selection of the specific method to display, because generic functions may have many methods with different type signatures. For this purpose, method-specific code-lowering is available using \hyperlink{18235967286596219009}{\texttt{code\_lowered}}, and the type-inferred form is available using \hyperlink{14801595959157535515}{\texttt{code\_typed}}. \hyperlink{5565852192659724503}{\texttt{code\_warntype}} adds highlighting to the output of \hyperlink{14801595959157535515}{\texttt{code\_typed}}.



Closer to the machine, the LLVM intermediate representation of a function may be printed using by \hyperlink{1749471484368489435}{\texttt{code\_llvm}}, and finally the compiled machine code is available using \hyperlink{2534314152947301270}{\texttt{code\_native}} (this will trigger JIT compilation/code generation for any function which has not previously been called).



For convenience, there are macro versions of the above functions which take standard function calls and expand argument types automatically:




\begin{minted}{jlcon}
julia> @code_llvm +(1,1)
;  @ int.jl:87 within `+`
; Function Attrs: sspstrong uwtable
define i64 @"julia_+_476"(i64 signext %0, i64 signext %1) #0 {
top:
  %2 = add i64 %1, %0
  ret i64 %2
}
\end{minted}



For more informations see \hyperlink{1376948972689074219}{\texttt{@code\_lowered}}, \hyperlink{6823997547688846780}{\texttt{@code\_typed}}, \hyperlink{8092893264277772840}{\texttt{@code\_warntype}}, \hyperlink{18039596607712979441}{\texttt{@code\_llvm}}, and \hyperlink{2629340111434042067}{\texttt{@code\_native}}.



\hypertarget{4254640328057635583}{}


\subsection{Printing of debug information}



The aforementioned functions and macros take the keyword argument \texttt{debuginfo} that controls the level debug information printed.




\begin{minted}{jlcon}
julia> @code_typed debuginfo=:source +(1,1)
CodeInfo(
    @ int.jl:53 within `+'
1 ─ %1 = Base.add_int(x, y)::Int64
└──      return %1
) => Int64
\end{minted}



Possible values for \texttt{debuginfo} are: \texttt{:none}, \texttt{:source}, and \texttt{:default}. Per default debug information is not printed, but that can be changed by setting \texttt{Base.IRShow.default\_debuginfo[] = :source}.



\chapter{Documentation of Julia's Internals}


\hypertarget{14839170566260018486}{}


\section{Initialization of the Julia runtime}



How does the Julia runtime execute \texttt{julia -e {\textquotesingle}println({\textquotedbl}Hello World!{\textquotedbl}){\textquotesingle}} ?



\hypertarget{6651980781302015874}{}


\subsection{\texttt{main()}}



Execution starts at \href{https://github.com/JuliaLang/julia/blob/master/cli/loader\_exe.c}{\texttt{main()} in \texttt{cli/loader\_exe.c}}, which calls \texttt{jl\_load\_repl()} in \href{https://github.com/JuliaLang/julia/blob/master/cli/loader\_lib.c}{\texttt{cli/loader\_lib.c}} which loads a few libraries, eventually calling \href{https://github.com/JuliaLang/julia/blob/master/src/jlapi.c}{\texttt{repl\_entrypoint()} in \texttt{src/jlapi.c}}.



\texttt{repl\_entrypoint()} calls \href{https://github.com/JuliaLang/julia/blob/master/src/support/libsupportinit.c}{\texttt{libsupport\_init()}} to set the C library locale and to initialize the {\textquotedbl}ios{\textquotedbl} library (see \href{https://github.com/JuliaLang/julia/blob/master/src/support/ios.c}{\texttt{ios\_init\_stdstreams()}} and \hyperlink{3841537160196121279}{Legacy \texttt{ios.c} library}).



Next \href{https://github.com/JuliaLang/julia/blob/master/src/jloptions.c}{\texttt{jl\_parse\_opts()}} is called to process command line options. Note that \texttt{jl\_parse\_opts()} only deals with options that affect code generation or early initialization. Other options are handled later by \href{https://github.com/JuliaLang/julia/blob/master/base/client.jl}{\texttt{exec\_options()} in \texttt{base/client.jl}}.



\texttt{jl\_parse\_opts()} stores command line options in the \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{global \texttt{jl\_options} struct}.



\hypertarget{10951200599627901176}{}


\subsection{\texttt{julia\_init()}}



\href{https://github.com/JuliaLang/julia/blob/master/src/task.c}{\texttt{julia\_init()} in \texttt{task.c}} is called by \texttt{main()} and calls \href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{\_julia\_init()} in \texttt{init.c}}.



\texttt{\_julia\_init()} begins by calling \texttt{libsupport\_init()} again (it does nothing the second time).



\href{https://github.com/JuliaLang/julia/blob/master/src/signals-unix.c}{\texttt{restore\_signals()}} is called to zero the signal handler mask.



\href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{jl\_resolve\_sysimg\_location()}} searches configured paths for the base system image. See \hyperlink{15513456349900674098}{Building the Julia system image}.



\href{https://github.com/JuliaLang/julia/blob/master/src/gc.c}{\texttt{jl\_gc\_init()}} sets up allocation pools and lists for weak refs, preserved values and finalization.



\href{https://github.com/JuliaLang/julia/blob/master/src/ast.c}{\texttt{jl\_init\_frontend()}} loads and initializes a pre-compiled femtolisp image containing the scanner/parser.



\href{https://github.com/JuliaLang/julia/blob/master/src/jltypes.c}{\texttt{jl\_init\_types()}} creates \texttt{jl\_datatype\_t} type description objects for the \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{built-in types defined in \texttt{julia.h}}. e.g.




\begin{lstlisting}
jl_any_type = jl_new_abstracttype(jl_symbol("Any"), core, NULL, jl_emptysvec);
jl_any_type->super = jl_any_type;

jl_type_type = jl_new_abstracttype(jl_symbol("Type"), core, jl_any_type, jl_emptysvec);

jl_int32_type = jl_new_primitivetype(jl_symbol("Int32"), core,
                                     jl_any_type, jl_emptysvec, 32);
\end{lstlisting}



\href{https://github.com/JuliaLang/julia/blob/master/src/task.c}{\texttt{jl\_init\_tasks()}} creates the \texttt{jl\_datatype\_t* jl\_task\_type} object; initializes the global \texttt{jl\_root\_task} struct; and sets \texttt{jl\_current\_task} to the root task.



\href{https://github.com/JuliaLang/julia/blob/master/src/codegen.cpp}{\texttt{jl\_init\_codegen()}} initializes the \href{https://llvm.org}{LLVM library}.



\href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_init\_serializer()}} initializes 8-bit serialization tags for builtin \texttt{jl\_value\_t} values.



If there is no sysimg file (\texttt{!jl\_options.image\_file}) then the \texttt{Core} and \texttt{Main} modules are created and \texttt{boot.jl} is evaluated:



\texttt{jl\_core\_module = jl\_new\_module(jl\_symbol({\textquotedbl}Core{\textquotedbl}))} creates the Julia \texttt{Core} module.



\href{https://github.com/JuliaLang/julia/blob/master/src/intrinsics.cpp}{\texttt{jl\_init\_intrinsic\_functions()}} creates a new Julia module \texttt{Intrinsics} containing constant \texttt{jl\_intrinsic\_type} symbols. These define an integer code for each \href{https://github.com/JuliaLang/julia/blob/master/src/intrinsics.cpp}{intrinsic function}. \href{https://github.com/JuliaLang/julia/blob/master/src/intrinsics.cpp}{\texttt{emit\_intrinsic()}} translates these symbols into LLVM instructions during code generation.



\href{https://github.com/JuliaLang/julia/blob/master/src/builtins.c}{\texttt{jl\_init\_primitives()}} hooks C functions up to Julia function symbols. e.g. the symbol \texttt{Core.:(===)()} is bound to C function pointer \texttt{jl\_f\_is()} by calling \texttt{add\_builtin\_func({\textquotedbl}==={\textquotedbl}, jl\_f\_is)}.



\href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_new\_main\_module()}} creates the global {\textquotedbl}Main{\textquotedbl} module and sets \texttt{jl\_current\_task->current\_module = jl\_main\_module}.



Note: \texttt{\_julia\_init()} \href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{then sets} \texttt{jl\_root\_task->current\_module = jl\_core\_module}. \texttt{jl\_root\_task} is an alias of \texttt{jl\_current\_task} at this point, so the \texttt{current\_module} set by \texttt{jl\_new\_main\_module()} above is overwritten.



\href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{jl\_load({\textquotedbl}boot.jl{\textquotedbl}, sizeof({\textquotedbl}boot.jl{\textquotedbl}))}} calls \href{https://github.com/JuliaLang/julia/blob/master/src/ast.c}{\texttt{jl\_parse\_eval\_all}} which repeatedly calls \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_toplevel\_eval\_flex()}} to execute \href{https://github.com/JuliaLang/julia/blob/master/base/boot.jl}{\texttt{boot.jl}}. <!– TODO – drill down into eval? –>



\href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{jl\_get\_builtin\_hooks()}} initializes global C pointers to Julia globals defined in \texttt{boot.jl}.



\href{https://github.com/JuliaLang/julia/blob/master/src/datatype.c}{\texttt{jl\_init\_box\_caches()}} pre-allocates global boxed integer value objects for values up to 1024. This speeds up allocation of boxed ints later on. e.g.:




\begin{lstlisting}
jl_value_t *jl_box_uint8(uint32_t x)
{
    return boxed_uint8_cache[(uint8_t)x];
}
\end{lstlisting}



\href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{\_julia\_init()} iterates} over the \texttt{jl\_core\_module->bindings.table} looking for \texttt{jl\_datatype\_t} values and sets the type name{\textquotesingle}s module prefix to \texttt{jl\_core\_module}.



\href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_add\_standard\_imports(jl\_main\_module)}} does {\textquotedbl}using Base{\textquotedbl} in the {\textquotedbl}Main{\textquotedbl} module.



Note: \texttt{\_julia\_init()} now reverts to \texttt{jl\_root\_task->current\_module = jl\_main\_module} as it was before being set to \texttt{jl\_core\_module} above.



Platform specific signal handlers are initialized for \texttt{SIGSEGV} (OSX, Linux), and \texttt{SIGFPE} (Windows).



Other signals (\texttt{SIGINFO, SIGBUS, SIGILL, SIGTERM, SIGABRT, SIGQUIT, SIGSYS} and \texttt{SIGPIPE}) are hooked up to \href{https://github.com/JuliaLang/julia/blob/master/src/signals-unix.c}{\texttt{sigdie\_handler()}} which prints a backtrace.



\href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_init\_restored\_modules()}} calls \href{https://github.com/JuliaLang/julia/blob/master/src/module.c}{\texttt{jl\_module\_run\_initializer()}} for each deserialized module to run the \texttt{\_\_init\_\_()} function.



Finally \href{https://github.com/JuliaLang/julia/blob/master/src/signals-unix.c}{\texttt{sigint\_handler()}} is hooked up to \texttt{SIGINT} and calls \texttt{jl\_throw(jl\_interrupt\_exception)}.



\texttt{\_julia\_init()} then returns \href{https://github.com/JuliaLang/julia/blob/master/cli/loader\_exe.c}{back to \texttt{main()} in \texttt{cli/loader\_exe.c}} and \texttt{main()} calls \texttt{repl\_entrypoint(argc, (char**)argv)}.



\begin{quote}
\textbf{sysimg}

If there is a sysimg file, it contains a pre-cooked image of the \texttt{Core} and \texttt{Main} modules (and whatever else is created by \texttt{boot.jl}). See \hyperlink{15513456349900674098}{Building the Julia system image}.

\href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_restore\_system\_image()}} deserializes the saved sysimg into the current Julia runtime environment and initialization continues after \texttt{jl\_init\_box\_caches()} below...

Note: \href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_restore\_system\_image()} (and \texttt{staticdata.c} in general)} uses the \hyperlink{3841537160196121279}{Legacy \texttt{ios.c} library}.

\end{quote}


\hypertarget{11153303243704616360}{}


\subsection{\texttt{repl\_entrypoint()}}



\href{https://github.com/JuliaLang/julia/blob/master/src/jlapi.c}{\texttt{repl\_entrypoint()}} loads the contents of \texttt{argv[]} into \hyperlink{2567473177880607455}{\texttt{Base.ARGS}}.



If a \texttt{.jl} {\textquotedbl}program{\textquotedbl} file was supplied on the command line, then \href{https://github.com/JuliaLang/julia/blob/master/src/jlapi.c}{\texttt{exec\_program()}} calls \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_load(program,len)}} which calls \href{https://github.com/JuliaLang/julia/blob/master/src/ast.c}{\texttt{jl\_parse\_eval\_all}} which repeatedly calls \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_toplevel\_eval\_flex()}} to execute the program.



However, in our example (\texttt{julia -e {\textquotesingle}println({\textquotedbl}Hello World!{\textquotedbl}){\textquotesingle}}), \href{https://github.com/JuliaLang/julia/blob/master/src/module.c}{\texttt{jl\_get\_global(jl\_base\_module, jl\_symbol({\textquotedbl}\_start{\textquotedbl}))}} looks up \href{https://github.com/JuliaLang/julia/blob/master/base/client.jl}{\texttt{Base.\_start}} and \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{\texttt{jl\_apply()}} executes it.



\hypertarget{12561163861893339364}{}


\subsection{\texttt{Base.\_start}}



\href{https://github.com/JuliaLang/julia/blob/master/base/client.jl}{\texttt{Base.\_start}} calls \href{https://github.com/JuliaLang/julia/blob/master/base/client.jl}{\texttt{Base.exec\_options}} which calls \href{https://github.com/JuliaLang/julia/blob/master/src/ast.c}{\texttt{jl\_parse\_input\_line({\textquotedbl}println({\textquotedbl}Hello World!{\textquotedbl}){\textquotedbl})}} to create an expression object and \hyperlink{2345597220715550879}{\texttt{Core.eval(Main, ex)}} to execute the parsed expression \texttt{ex} in the module context of \texttt{Main}.



\hypertarget{4809506034778288898}{}


\subsection{\texttt{Core.eval}}



\hyperlink{2345597220715550879}{\texttt{Core.eval(Main, ex)}} calls \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_toplevel\_eval\_in(m, ex)}}, which calls \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_toplevel\_eval\_flex}}. \texttt{jl\_toplevel\_eval\_flex} implements a simple heuristic to decide whether to compile a given code thunk or run it by interpreter. When given \texttt{println({\textquotedbl}Hello World!{\textquotedbl})}, it would usually decide to run the code by interpreter, in which case it calls \href{https://github.com/JuliaLang/julia/blob/master/src/interpreter.c}{\texttt{jl\_interpret\_toplevel\_thunk}}, which then calls \href{https://github.com/JuliaLang/julia/blob/master/src/interpreter.c}{\texttt{eval\_body}}.



The stack dump below shows how the interpreter works its way through various methods of \hyperlink{783803254548423222}{\texttt{Base.println()}} and \hyperlink{8248717042415202230}{\texttt{Base.print()}} before arriving at \href{https://github.com/JuliaLang/julia/blob/master/base/stream.jl}{\texttt{write(s::IO, a::Array\{T\}) where T}}  which does \texttt{ccall(jl\_uv\_write())}.



\href{https://github.com/JuliaLang/julia/blob/master/src/jl\_uv.c}{\texttt{jl\_uv\_write()}} calls \texttt{uv\_write()} to write {\textquotedbl}Hello World!{\textquotedbl} to \texttt{JL\_STDOUT}. See \hyperlink{11668969309999094552}{Libuv wrappers for stdio}.:




\begin{lstlisting}
Hello World!
\end{lstlisting}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
Stack frame & Source code & Notes \\
\hline
\texttt{jl\_uv\_write()} & \texttt{jl\_uv.c} & called though \hyperlink{14245046751182637566}{\texttt{ccall}} \\
\hline
\texttt{julia\_write\_282942} & \texttt{stream.jl} & function \texttt{write!(s::IO, a::Array\{T\}) where T} \\
\hline
\texttt{julia\_print\_284639} & \texttt{ascii.jl} & \texttt{print(io::IO, s::String) = (write(io, s); nothing)} \\
\hline
\texttt{jlcall\_print\_284639} &  &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_trampoline()} & \texttt{builtins.c} &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_apply\_generic()} & \texttt{gf.c} & \texttt{Base.print(Base.TTY, String)} \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_trampoline()} & \texttt{builtins.c} &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_apply\_generic()} & \texttt{gf.c} & \texttt{Base.print(Base.TTY, String, Char, Char...)} \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_f\_apply()} & \texttt{builtins.c} &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_trampoline()} & \texttt{builtins.c} &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_apply\_generic()} & \texttt{gf.c} & \texttt{Base.println(Base.TTY, String, String...)} \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_trampoline()} & \texttt{builtins.c} &  \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{jl\_apply\_generic()} & \texttt{gf.c} & \texttt{Base.println(String,)} \\
\hline
\texttt{jl\_apply()} & \texttt{julia.h} &  \\
\hline
\texttt{do\_call()} & \texttt{interpreter.c} &  \\
\hline
\texttt{eval\_body()} & \texttt{interpreter.c} &  \\
\hline
\texttt{jl\_interpret\_toplevel\_thunk} & \texttt{interpreter.c} &  \\
\hline
\texttt{jl\_toplevel\_eval\_flex} & \texttt{toplevel.c} &  \\
\hline
\texttt{jl\_toplevel\_eval\_in} & \texttt{toplevel.c} &  \\
\hline
\texttt{Core.eval} & \texttt{boot.jl} &  \\
\hline
\end{tabulary}

\end{table}



Since our example has just one function call, which has done its job of printing {\textquotedbl}Hello World!{\textquotedbl}, the stack now rapidly unwinds back to \texttt{main()}.



\hypertarget{8766302654766843311}{}


\subsection{\texttt{jl\_atexit\_hook()}}



\texttt{main()} calls \href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{jl\_atexit\_hook()}}. This calls \texttt{Base.\_atexit}, then calls \href{https://github.com/JuliaLang/julia/blob/master/src/gc.c}{\texttt{jl\_gc\_run\_all\_finalizers()}} and cleans up libuv handles.



\hypertarget{6367994784846959684}{}


\subsection{\texttt{julia\_save()}}



Finally, \texttt{main()} calls \href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{julia\_save()}}, which if requested on the command line, saves the runtime state to a new system image. See \href{https://github.com/JuliaLang/julia/blob/master/src/gf.c}{\texttt{jl\_compile\_all()}} and \href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_save\_system\_image()}}.



\hypertarget{16235973380684375384}{}


\section{Julia ASTs}



Julia has two representations of code. First there is a surface syntax AST returned by the parser (e.g. the \hyperlink{9794549254908071788}{\texttt{Meta.parse}} function), and manipulated by macros. It is a structured representation of code as it is written, constructed by \texttt{julia-parser.scm} from a character stream. Next there is a lowered form, or IR (intermediate representation), which is used by type inference and code generation. In the lowered form there are fewer types of nodes, all macros are expanded, and all control flow is converted to explicit branches and sequences of statements. The lowered form is constructed by \texttt{julia-syntax.scm}.



First we will focus on the AST, since it is needed to write macros.



\hypertarget{6198433338459689204}{}


\subsection{Surface syntax AST}



Front end ASTs consist almost entirely of \hyperlink{17120496304147995299}{\texttt{Expr}}s and atoms (e.g. symbols, numbers). There is generally a different expression head for each visually distinct syntactic form. Examples will be given in s-expression syntax. Each parenthesized list corresponds to an Expr, where the first element is the head. For example \texttt{(call f x)} corresponds to \texttt{Expr(:call, :f, :x)} in Julia.



\hypertarget{13191950853363974893}{}


\subsubsection{Calls}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{f(x)} & \texttt{(call f x)} \\
\hline
\texttt{f(x, y=1, z=2)} & \texttt{(call f x (kw y 1) (kw z 2))} \\
\hline
\texttt{f(x; y=1)} & \texttt{(call f (parameters (kw y 1)) x)} \\
\hline
\texttt{f(x...)} & \texttt{(call f (... x))} \\
\hline
\end{tabulary}

\end{table}



\texttt{do} syntax:




\begin{minted}{julia}
f(x) do a,b
    body
end
\end{minted}



parses as \texttt{(do (call f x) (-> (tuple a b) (block body)))}.



\hypertarget{16991843071018324380}{}


\subsubsection{Operators}



Most uses of operators are just function calls, so they are parsed with the head \texttt{call}. However some operators are special forms (not necessarily function calls), and in those cases the operator itself is the expression head. In julia-parser.scm these are referred to as {\textquotedbl}syntactic operators{\textquotedbl}. Some operators (\texttt{+} and \texttt{*}) use N-ary parsing; chained calls are parsed as a single N-argument call. Finally, chains of comparisons have their own special expression structure.




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{x+y} & \texttt{(call + x y)} \\
\hline
\texttt{a+b+c+d} & \texttt{(call + a b c d)} \\
\hline
\texttt{2x} & \texttt{(call * 2 x)} \\
\hline
\texttt{a\&\&b} & \texttt{(\&\& a b)} \\
\hline
\texttt{x += 1} & \texttt{(+= x 1)} \\
\hline
\texttt{a ? 1 : 2} & \texttt{(if a 1 2)} \\
\hline
\texttt{a,b} & \texttt{(tuple a b)} \\
\hline
\texttt{a==b} & \texttt{(call == a b)} \\
\hline
\texttt{1<i<=n} & \texttt{(comparison 1 < i <= n)} \\
\hline
\texttt{a.b} & \texttt{(. a (quote b))} \\
\hline
\texttt{a.(b)} & \texttt{(. a (tuple b))} \\
\hline
\end{tabulary}

\end{table}



\hypertarget{15884526073773577025}{}


\subsubsection{Bracketed forms}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{a[i]} & \texttt{(ref a i)} \\
\hline
\texttt{t[i;j]} & \texttt{(typed\_vcat t i j)} \\
\hline
\texttt{t[i j]} & \texttt{(typed\_hcat t i j)} \\
\hline
\texttt{t[a b; c d]} & \texttt{(typed\_vcat t (row a b) (row c d))} \\
\hline
\texttt{t[a b;;; c d]} & \texttt{(typed\_ncat t 3 (row a b) (row c d))} \\
\hline
\texttt{a\{b\}} & \texttt{(curly a b)} \\
\hline
\texttt{a\{b;c\}} & \texttt{(curly a (parameters c) b)} \\
\hline
\texttt{[x]} & \texttt{(vect x)} \\
\hline
\texttt{[x,y]} & \texttt{(vect x y)} \\
\hline
\texttt{[x;y]} & \texttt{(vcat x y)} \\
\hline
\texttt{[x y]} & \texttt{(hcat x y)} \\
\hline
\texttt{[x y; z t]} & \texttt{(vcat (row x y) (row z t))} \\
\hline
\texttt{[x;y;; z;t;;;]} & \texttt{(ncat 3 (nrow 2 (nrow 1 x y) (nrow 1 z t)))} \\
\hline
\texttt{[x for y in z, a in b]} & \texttt{(comprehension (generator x (= y z) (= a b)))} \\
\hline
\texttt{T[x for y in z]} & \texttt{(typed\_comprehension T (generator x (= y z)))} \\
\hline
\texttt{(a, b, c)} & \texttt{(tuple a b c)} \\
\hline
\texttt{(a; b; c)} & \texttt{(block a b c)} \\
\hline
\end{tabulary}

\end{table}



\hypertarget{7735912728489467540}{}


\subsubsection{Macros}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{@m x y} & \texttt{(macrocall @m (line) x y)} \\
\hline
\texttt{Base.@m x y} & \texttt{(macrocall (. Base (quote @m)) (line) x y)} \\
\hline
\texttt{@Base.m x y} & \texttt{(macrocall (. Base (quote @m)) (line) x y)} \\
\hline
\end{tabulary}

\end{table}



\hypertarget{5278796056388981234}{}


\subsubsection{Strings}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{{\textquotedbl}a{\textquotedbl}} & \texttt{{\textquotedbl}a{\textquotedbl}} \\
\hline
\texttt{x{\textquotedbl}y{\textquotedbl}} & \texttt{(macrocall @x\_str (line) {\textquotedbl}y{\textquotedbl})} \\
\hline
\texttt{x{\textquotedbl}y{\textquotedbl}z} & \texttt{(macrocall @x\_str (line) {\textquotedbl}y{\textquotedbl} {\textquotedbl}z{\textquotedbl})} \\
\hline
\texttt{{\textquotedbl}x = \$x{\textquotedbl}} & \texttt{(string {\textquotedbl}x = {\textquotedbl} x)} \\
\hline
\texttt{`a b c`} & \texttt{(macrocall @cmd (line) {\textquotedbl}a b c{\textquotedbl})} \\
\hline
\end{tabulary}

\end{table}



Doc string syntax:




\begin{minted}{julia}
"some docs"
f(x) = x
\end{minted}



parses as \texttt{(macrocall (|.| Core {\textquotesingle}@doc) (line) {\textquotedbl}some docs{\textquotedbl} (= (call f x) (block x)))}.



\hypertarget{13863161852089184826}{}


\subsubsection{Imports and such}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{import a} & \texttt{(import (. a))} \\
\hline
\texttt{import a.b.c} & \texttt{(import (. a b c))} \\
\hline
\texttt{import ...a} & \texttt{(import (. . . . a))} \\
\hline
\texttt{import a.b, c.d} & \texttt{(import (. a b) (. c d))} \\
\hline
\texttt{import Base: x} & \texttt{(import (: (. Base) (. x)))} \\
\hline
\texttt{import Base: x, y} & \texttt{(import (: (. Base) (. x) (. y)))} \\
\hline
\texttt{export a, b} & \texttt{(export a b)} \\
\hline
\end{tabulary}

\end{table}



\texttt{using} has the same representation as \texttt{import}, but with expression head \texttt{:using} instead of \texttt{:import}.



\hypertarget{13439801024488074381}{}


\subsubsection{Numbers}



Julia supports more number types than many scheme implementations, so not all numbers are represented directly as scheme numbers in the AST.




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
Input & AST \\
\hline
\texttt{11111111111111111111} & \texttt{(macrocall @int128\_str nothing {\textquotedbl}11111111111111111111{\textquotedbl})} \\
\hline
\texttt{0xfffffffffffffffff} & \texttt{(macrocall @uint128\_str nothing {\textquotedbl}0xfffffffffffffffff{\textquotedbl})} \\
\hline
\texttt{1111...many digits...} & \texttt{(macrocall @big\_str nothing {\textquotedbl}1111....{\textquotedbl})} \\
\hline
\end{tabulary}

\end{table}



\hypertarget{12573206411049142153}{}


\subsubsection{Block forms}



A block of statements is parsed as \texttt{(block stmt1 stmt2 ...)}.



If statement:




\begin{minted}{julia}
if a
    b
elseif c
    d
else
    e
end
\end{minted}



parses as:




\begin{lstlisting}
(if a (block (line 2) b)
    (elseif (block (line 3) c) (block (line 4) d)
            (block (line 6 e))))
\end{lstlisting}



A \texttt{while} loop parses as \texttt{(while condition body)}.



A \texttt{for} loop parses as \texttt{(for (= var iter) body)}. If there is more than one iteration specification, they are parsed as a block: \texttt{(for (block (= v1 iter1) (= v2 iter2)) body)}.



\texttt{break} and \texttt{continue} are parsed as 0-argument expressions \texttt{(break)} and \texttt{(continue)}.



\texttt{let} is parsed as \texttt{(let (= var val) body)} or \texttt{(let (block (= var1 val1) (= var2 val2) ...) body)}, like \texttt{for} loops.



A basic function definition is parsed as \texttt{(function (call f x) body)}. A more complex example:




\begin{minted}{julia}
function f(x::T; k = 1) where T
    return x+1
end
\end{minted}



parses as:




\begin{lstlisting}
(function (where (call f (parameters (kw k 1))
                       (:: x T))
                 T)
          (block (line 2) (return (call + x 1))))
\end{lstlisting}



Type definition:




\begin{minted}{julia}
mutable struct Foo{T<:S}
    x::T
end
\end{minted}



parses as:




\begin{lstlisting}
(struct true (curly Foo (<: T S))
        (block (line 2) (:: x T)))
\end{lstlisting}



The first argument is a boolean telling whether the type is mutable.



\texttt{try} blocks parse as \texttt{(try try\_block var catch\_block finally\_block)}. If no variable is present after \texttt{catch}, \texttt{var} is \texttt{\#f}. If there is no \texttt{finally} clause, then the last argument is not present.



\hypertarget{15188092119950048030}{}


\subsubsection{Quote expressions}



Julia source syntax forms for code quoting (\texttt{quote} and \texttt{:( )}) support interpolation with \texttt{\$}. In Lisp terminology, this means they are actually {\textquotedbl}backquote{\textquotedbl} or {\textquotedbl}quasiquote{\textquotedbl} forms. Internally, there is also a need for code quoting without interpolation. In Julia{\textquotesingle}s scheme code, non-interpolating quote is represented with the expression head \texttt{inert}.



\texttt{inert} expressions are converted to Julia \texttt{QuoteNode} objects. These objects wrap a single value of any type, and when evaluated simply return that value.



A \texttt{quote} expression whose argument is an atom also gets converted to a \texttt{QuoteNode}.



\hypertarget{9438599814670899276}{}


\subsubsection{Line numbers}



Source location information is represented as \texttt{(line line\_num file\_name)} where the third component is optional (and omitted when the current line number, but not file name, changes).



These expressions are represented as \texttt{LineNumberNode}s in Julia.



\hypertarget{8143615541033323666}{}


\subsubsection{Macros}



Macro hygiene is represented through the expression head pair \texttt{escape} and \texttt{hygienic-scope}. The result of a macro expansion is automatically wrapped in \texttt{(hygienic-scope block module)}, to represent the result of the new scope. The user can insert \texttt{(escape block)} inside to interpolate code from the caller.



\hypertarget{16818744617880081407}{}


\subsection{Lowered form}



Lowered form (IR) is more important to the compiler, since it is used for type inference, optimizations like inlining, and code generation. It is also less obvious to the human, since it results from a significant rearrangement of the input syntax.



In addition to \texttt{Symbol}s and some number types, the following data types exist in lowered form:



\begin{itemize}
\item \texttt{Expr}

Has a node type indicated by the \texttt{head} field, and an \texttt{args} field which is a \texttt{Vector\{Any\}} of subexpressions. While almost every part of a surface AST is represented by an \texttt{Expr}, the IR uses only a limited number of \texttt{Expr}s, mostly for calls and some top-level-only forms.


\item \texttt{Slot}

Identifies arguments and local variables by consecutive numbering. \texttt{Slot} is an abstract type with subtypes \texttt{SlotNumber} and \texttt{TypedSlot}. Both types have an integer-valued \texttt{id} field giving the slot index. Most slots have the same type at all uses, and so are represented with \texttt{SlotNumber}. The types of these slots are found in the \texttt{slottypes} field of their \texttt{CodeInfo} object. Slots that require per-use type annotations are represented with \texttt{TypedSlot}, which has a \texttt{typ} field.


\item \texttt{Argument}

The same as \texttt{SlotNumber}, but appears only post-optimization. Indicates that the referenced slot is an argument of the enclosing function.


\item \texttt{CodeInfo}

Wraps the IR of a group of statements. Its \texttt{code} field is an array of expressions to execute.


\item \texttt{GotoNode}

Unconditional branch. The argument is the branch target, represented as an index in the code array to jump to.


\item \texttt{GotoIfNot}

Conditional branch. If the \texttt{cond} field evaluates to false, goes to the index identified by the \texttt{dest} field.


\item \texttt{ReturnNode}

Returns its argument (the \texttt{val} field) as the value of the enclosing function. If the \texttt{val} field is undefined, then this represents an unreachable statement.


\item \texttt{QuoteNode}

Wraps an arbitrary value to reference as data. For example, the function \texttt{f() = :a} contains a \texttt{QuoteNode} whose \texttt{value} field is the symbol \texttt{a}, in order to return the symbol itself instead of evaluating it.


\item \texttt{GlobalRef}

Refers to global variable \texttt{name} in module \texttt{mod}.


\item \texttt{SSAValue}

Refers to a consecutively-numbered (starting at 1) static single assignment (SSA) variable inserted by the compiler. The number (\texttt{id}) of an \texttt{SSAValue} is the code array index of the expression whose value it represents.


\item \texttt{NewvarNode}

Marks a point where a variable (slot) is created. This has the effect of resetting a variable to undefined.

\end{itemize}


\hypertarget{15302433068188215381}{}


\subsubsection{\texttt{Expr} types}



These symbols appear in the \texttt{head} field of \hyperlink{17120496304147995299}{\texttt{Expr}}s in lowered form.



\begin{itemize}
\item \texttt{call}

Function call (dynamic dispatch). \texttt{args[1]} is the function to call, \texttt{args[2:end]} are the arguments.


\item \texttt{invoke}

Function call (static dispatch). \texttt{args[1]} is the MethodInstance to call, \texttt{args[2:end]} are the arguments (including the function that is being called, at \texttt{args[2]}).


\item \texttt{static\_parameter}

Reference a static parameter by index.


\item \texttt{=}

Assignment. In the IR, the first argument is always a Slot or a GlobalRef.


\item \texttt{method}

Adds a method to a generic function and assigns the result if necessary.

Has a 1-argument form and a 3-argument form. The 1-argument form arises from the syntax \texttt{function foo end}. In the 1-argument form, the argument is a symbol. If this symbol already names a function in the current scope, nothing happens. If the symbol is undefined, a new function is created and assigned to the identifier specified by the symbol. If the symbol is defined but names a non-function, an error is raised. The definition of {\textquotedbl}names a function{\textquotedbl} is that the binding is constant, and refers to an object of singleton type. The rationale for this is that an instance of a singleton type uniquely identifies the type to add the method to. When the type has fields, it wouldn{\textquotesingle}t be clear whether the method was being added to the instance or its type.

The 3-argument form has the following arguments:

\begin{itemize}
\item \texttt{args[1]}

A function name, or \texttt{nothing} if unknown or unneeded. If a symbol, then the expression first behaves like the 1-argument form above. This argument is ignored from then on. It can be \texttt{nothing} when methods are added strictly by type, \texttt{(::T)(x) = x}, or when a method is being added to an existing function, \texttt{MyModule.f(x) = x}.


\item \texttt{args[2]}

A \texttt{SimpleVector} of argument type data. \texttt{args[2][1]} is a \texttt{SimpleVector} of the argument types, and \texttt{args[2][2]} is a \texttt{SimpleVector} of type variables corresponding to the method{\textquotesingle}s static parameters.


\item \texttt{args[3]}

A \texttt{CodeInfo} of the method itself. For {\textquotedbl}out of scope{\textquotedbl} method definitions (adding a method to a function that also has methods defined in different scopes) this is an expression that evaluates to a \texttt{:lambda} expression.

\end{itemize}

\item \texttt{struct\_type}

A 7-argument expression that defines a new \texttt{struct}:

\begin{itemize}
\item \texttt{args[1]}

The name of the \texttt{struct}


\item \texttt{args[2]}

A \texttt{call} expression that creates a \texttt{SimpleVector} specifying its parameters


\item \texttt{args[3]}

A \texttt{call} expression that creates a \texttt{SimpleVector} specifying its fieldnames


\item \texttt{args[4]}

A \texttt{Symbol}, \texttt{GlobalRef}, or \texttt{Expr} specifying the supertype (e.g., \texttt{:Integer}, \texttt{GlobalRef(Core, :Any)}, or \texttt{:(Core.apply\_type(AbstractArray, T, N))})


\item \texttt{args[5]}

A \texttt{call} expression that creates a \texttt{SimpleVector} specifying its fieldtypes


\item \texttt{args[6]}

A Bool, true if \texttt{mutable}


\item \texttt{args[7]}

The number of arguments to initialize. This will be the number of fields, or the minimum number of fields called by an inner constructor{\textquotesingle}s \texttt{new} statement.

\end{itemize}

\item \texttt{abstract\_type}

A 3-argument expression that defines a new abstract type. The arguments are the same as arguments 1, 2, and 4 of \texttt{struct\_type} expressions.


\item \texttt{primitive\_type}

A 4-argument expression that defines a new primitive type. Arguments 1, 2, and 4 are the same as \texttt{struct\_type}. Argument 3 is the number of bits.

\begin{quote}
\textbf{Julia 1.5}

\texttt{struct\_type}, \texttt{abstract\_type}, and \texttt{primitive\_type} were removed in Julia 1.5 and replaced by calls to new builtins.

\end{quote}

\item \texttt{global}

Declares a global binding.


\item \texttt{const}

Declares a (global) variable as constant.


\item \texttt{new}

Allocates a new struct-like object. First argument is the type. The \hyperlink{13888762393600028594}{\texttt{new}} pseudo-function is lowered to this, and the type is always inserted by the compiler.  This is very much an internal-only feature, and does no checking. Evaluating arbitrary \texttt{new} expressions can easily segfault.


\item \texttt{splatnew}

Similar to \texttt{new}, except field values are passed as a single tuple. Works similarly to \texttt{Base.splat(new)} if \texttt{new} were a first-class function, hence the name.


\item \texttt{isdefined}

\texttt{Expr(:isdefined, :x)} returns a Bool indicating whether \texttt{x} has already been defined in the current scope.


\item \texttt{the\_exception}

Yields the caught exception inside a \texttt{catch} block, as returned by \texttt{jl\_current\_exception()}.


\item \texttt{undefcheck}

Temporary node inserted by the compiler and will be processed in \texttt{type\_lift\_pass!}.


\item \texttt{enter}

Enters an exception handler (\texttt{setjmp}). \texttt{args[1]} is the label of the catch block to jump to on error.  Yields a token which is consumed by \texttt{pop\_exception}.


\item \texttt{leave}

Pop exception handlers. \texttt{args[1]} is the number of handlers to pop.


\item \texttt{pop\_exception}

Pop the stack of current exceptions back to the state at the associated \texttt{enter} when leaving a catch block. \texttt{args[1]} contains the token from the associated \texttt{enter}.

\begin{quote}
\textbf{Julia 1.1}

\texttt{pop\_exception} is new in Julia 1.1.

\end{quote}

\item \texttt{inbounds}

Controls turning bounds checks on or off. A stack is maintained; if the first argument of this expression is true or false (\texttt{true} means bounds checks are disabled), it is pushed onto the stack. If the first argument is \texttt{:pop}, the stack is popped.


\item \texttt{boundscheck}

Has the value \texttt{false} if inlined into a section of code marked with \texttt{@inbounds}, otherwise has the value \texttt{true}.


\item \texttt{loopinfo}

Marks the end of the a loop. Contains metadata that is passed to \texttt{LowerSimdLoop} to either mark the inner loop of \texttt{@simd} expression, or to propagate information to LLVM loop passes.


\item \texttt{copyast}

Part of the implementation of quasi-quote. The argument is a surface syntax AST that is simply copied recursively and returned at run time.


\item \texttt{meta}

Metadata. \texttt{args[1]} is typically a symbol specifying the kind of metadata, and the rest of the arguments are free-form. The following kinds of metadata are commonly used:

\begin{itemize}
\item \texttt{:inline} and \texttt{:noinline}: Inlining hints.

\end{itemize}

\item \texttt{foreigncall}

Statically-computed container for \texttt{ccall} information. The fields are:

\begin{itemize}
\item \texttt{args[1]} : name

The expression that{\textquotesingle}ll be parsed for the foreign function.


\item \texttt{args[2]::Type} : RT

The (literal) return type, computed statically when the containing method was defined.


\item \texttt{args[3]::SimpleVector} (of Types) : AT

The (literal) vector of argument types, computed statically when the containing method was defined.


\item \texttt{args[4]::Int} : nreq

The number of required arguments for a varargs function definition.


\item \texttt{args[5]::QuoteNode\{Symbol\}} : calling convention

The calling convention for the call.


\item \texttt{args[6:5+length(args[3])]} : arguments

The values for all the arguments (with types of each given in args[3]).


\item \texttt{args[6+length(args[3])+1:end]} : gc-roots

The additional objects that may need to be gc-rooted for the duration of the call. See \hyperlink{16487312531471662451}{Working with LLVM} for where these are derived from and how they get handled.

\end{itemize}

\item \texttt{new\_opaque\_closure}

Constructs a new opaque closure. The fields are:

\begin{itemize}
\item \texttt{args[1]} : signature

The function signature of the opaque closure. Opaque closures don{\textquotesingle}t participate in dispatch, but the input types can be restricted.


\item \texttt{args[2]} : isva

Indicates whether the closure accepts varargs.


\item \texttt{args[3]} : lb

Lower bound on the output type. (Defaults to \texttt{Union\{\}})


\item \texttt{args[4]} : ub

Upper bound on the output type. (Defaults to \texttt{Any})


\item \texttt{args[5]} : method

The actual method as an \texttt{opaque\_closure\_method} expression.


\item \texttt{args[6:end]} : captures

The values captured by the opaque closure.

\end{itemize}
\begin{quote}
\textbf{Julia 1.7}

Opaque closures were added in Julia 1.7

\end{quote}
\end{itemize}


\hypertarget{10862348325946528961}{}


\subsubsection{Method}



A unique{\textquotesingle}d container describing the shared metadata for a single method.



\begin{itemize}
\item \texttt{name}, \texttt{module}, \texttt{file}, \texttt{line}, \texttt{sig}

Metadata to uniquely identify the method for the computer and the human.


\item \texttt{ambig}

Cache of other methods that may be ambiguous with this one.


\item \texttt{specializations}

Cache of all MethodInstance ever created for this Method, used to ensure uniqueness. Uniqueness is required for efficiency, especially for incremental precompile and tracking of method invalidation.


\item \texttt{source}

The original source code (if available, usually compressed).


\item \texttt{generator}

A callable object which can be executed to get specialized source for a specific method signature.


\item \texttt{roots}

Pointers to non-AST things that have been interpolated into the AST, required by compression of the AST, type-inference, or the generation of native code.


\item \texttt{nargs}, \texttt{isva}, \texttt{called}, \texttt{isstaged}, \texttt{pure}

Descriptive bit-fields for the source code of this Method.


\item \texttt{primary\_world}

The world age that {\textquotedbl}owns{\textquotedbl} this Method.

\end{itemize}


\hypertarget{2584833319372808594}{}


\subsubsection{MethodInstance}



A unique{\textquotesingle}d container describing a single callable signature for a Method. See especially \hyperlink{17047801949293328593}{Proper maintenance and care of multi-threading locks} for important details on how to modify these fields safely.



\begin{itemize}
\item \texttt{specTypes}

The primary key for this MethodInstance. Uniqueness is guaranteed through a \texttt{def.specializations} lookup.


\item \texttt{def}

The \texttt{Method} that this function describes a specialization of. Or a \texttt{Module}, if this is a top-level Lambda expanded in Module, and which is not part of a Method.


\item \texttt{sparam\_vals}

The values of the static parameters in \texttt{specTypes} indexed by \texttt{def.sparam\_syms}. For the \texttt{MethodInstance} at \texttt{Method.unspecialized}, this is the empty \texttt{SimpleVector}. But for a runtime \texttt{MethodInstance} from the \texttt{MethodTable} cache, this will always be defined and indexable.


\item \texttt{uninferred}

The uncompressed source code for a toplevel thunk. Additionally, for a generated function, this is one of many places that the source code might be found.


\item \texttt{backedges}

We store the reverse-list of cache dependencies for efficient tracking of incremental reanalysis/recompilation work that may be needed after a new method definitions. This works by keeping a list of the other \texttt{MethodInstance} that have been inferred or optimized to contain a possible call to this \texttt{MethodInstance}. Those optimization results might be stored somewhere in the \texttt{cache}, or it might have been the result of something we didn{\textquotesingle}t want to cache, such as constant propagation. Thus we merge all of those backedges to various cache entries here (there{\textquotesingle}s almost always only the one applicable cache entry with a sentinel value for max\_world anyways).


\item \texttt{cache}

Cache of \texttt{CodeInstance} objects that share this template instantiation.

\end{itemize}


\hypertarget{17684444820224515276}{}


\subsubsection{CodeInstance}



\begin{itemize}
\item \texttt{def}

The \texttt{MethodInstance} that this cache entry is derived from.

\end{itemize}


\begin{itemize}
\item \texttt{rettype}/\texttt{rettype\_const}

The inferred return type for the \texttt{specFunctionObject} field, which (in most cases) is also the computed return type for the function in general.


\item \texttt{inferred}

May contain a cache of the inferred source for this function, or it could be set to \texttt{nothing} to just indicate \texttt{rettype} is inferred.


\item \texttt{ftpr}

The generic jlcall entry point.


\item \texttt{jlcall\_api}

The ABI to use when calling \texttt{fptr}. Some significant ones include:

\begin{itemize}
\item 0 - Not compiled yet


\item 1 - JL\emph{CALLABLE `jl}value\emph{t \emph{(})(jl}function\emph{t *f, jl}value\emph{t *args[nargs], uint32}t nargs)`


\item 2 - Constant (value stored in \texttt{rettype\_const})


\item 3 - With Static-parameters forwarded \texttt{jl\_value\_t *(*)(jl\_svec\_t *sparams, jl\_function\_t *f, jl\_value\_t *args[nargs], uint32\_t nargs)}


\item 4 - Run in interpreter \texttt{jl\_value\_t *(*)(jl\_method\_instance\_t *meth, jl\_function\_t *f, jl\_value\_t *args[nargs], uint32\_t nargs)}

\end{itemize}

\item \texttt{min\_world} / \texttt{max\_world}

The range of world ages for which this method instance is valid to be called. If max\_world is the special token value \texttt{-1}, the value is not yet known. It may continue to be used until we encounter a backedge that requires us to reconsider.

\end{itemize}


\hypertarget{15595975163128328315}{}


\subsubsection{CodeInfo}



A (usually temporary) container for holding lowered source code.



\begin{itemize}
\item \texttt{code}

An \texttt{Any} array of statements


\item \texttt{slotnames}

An array of symbols giving names for each slot (argument or local variable).


\item \texttt{slotflags}

A \texttt{UInt8} array of slot properties, represented as bit flags:

\begin{itemize}
\item 2  - assigned (only false if there are \emph{no} assignment statements with this var on the left)


\item 8  - const (currently unused for local variables)


\item 16 - statically assigned once


\item 32 - might be used before assigned. This flag is only valid after type inference.

\end{itemize}

\item \texttt{ssavaluetypes}

Either an array or an \texttt{Int}.

If an \texttt{Int}, it gives the number of compiler-inserted temporary locations in the function (the length of \texttt{code} array). If an array, specifies a type for each location.


\item \texttt{ssaflags}

Statement-level flags for each expression in the function. Many of these are reserved, but not yet implemented:

\begin{itemize}
\item 0 = inbounds


\item 1,2 = <reserved> inlinehint,always-inline,noinline


\item 3 = <reserved> strict-ieee (strictfp)


\item 4-6 = <unused>


\item 7 = <reserved> has out-of-band info

\end{itemize}

\item \texttt{linetable}

An array of source location objects


\item \texttt{codelocs}

An array of integer indices into the \texttt{linetable}, giving the location associated with each statement.

\end{itemize}


Optional Fields:



\begin{itemize}
\item \texttt{slottypes}

An array of types for the slots.


\item \texttt{rettype}

The inferred return type of the lowered form (IR). Default value is \texttt{Any}.


\item \texttt{method\_for\_inference\_limit\_heuristics}

The \texttt{method\_for\_inference\_heuristics} will expand the given method{\textquotesingle}s generator if necessary during inference.


\item \texttt{parent}

The \texttt{MethodInstance} that {\textquotedbl}owns{\textquotedbl} this object (if applicable).


\item \texttt{min\_world}/\texttt{max\_world}

The range of world ages for which this code was valid at the time when it had been inferred.

\end{itemize}


Boolean properties:



\begin{itemize}
\item \texttt{inferred}

Whether this has been produced by type inference.


\item \texttt{inlineable}

Whether this should be eligible for inlining.


\item \texttt{propagate\_inbounds}

Whether this should propagate \texttt{@inbounds} when inlined for the purpose of eliding \texttt{@boundscheck} blocks.


\item \texttt{pure}

Whether this is known to be a pure function of its arguments, without respect to the state of the method caches or other mutable global state.

\end{itemize}


\hypertarget{4038509094133832716}{}


\section{More about types}



If you{\textquotesingle}ve used Julia for a while, you understand the fundamental role that types play.  Here we try to get under the hood, focusing particularly on \hyperlink{5611641345231583503}{Parametric Types}.



\hypertarget{8213772846516553388}{}


\subsection{Types and sets (and \texttt{Any} and \texttt{Union\{\}}/\texttt{Bottom})}



It{\textquotesingle}s perhaps easiest to conceive of Julia{\textquotesingle}s type system in terms of sets. While programs manipulate individual values, a type refers to a set of values. This is not the same thing as a collection; for example a \hyperlink{1143189053501747033}{\texttt{Set}} of values is itself a single \texttt{Set} value. Rather, a type describes a set of \emph{possible} values, expressing uncertainty about which value we have.



A \emph{concrete} type \texttt{T} describes the set of values whose direct tag, as returned by the \hyperlink{13440452181855594120}{\texttt{typeof}} function, is \texttt{T}. An \emph{abstract} type describes some possibly-larger set of values.



\hyperlink{15014186392807667022}{\texttt{Any}} describes the entire universe of possible values. \hyperlink{8469131683393450448}{\texttt{Integer}} is a subset of \texttt{Any} that includes \texttt{Int}, \hyperlink{5857518405103968275}{\texttt{Int8}}, and other concrete types. Internally, Julia also makes heavy use of another type known as \texttt{Bottom}, which can also be written as \texttt{Union\{\}}. This corresponds to the empty set.



Julia{\textquotesingle}s types support the standard operations of set theory: you can ask whether \texttt{T1} is a {\textquotedbl}subset{\textquotedbl} (subtype) of \texttt{T2} with \texttt{T1 <: T2}. Likewise, you intersect two types using \hyperlink{1869272868531275554}{\texttt{typeintersect}}, take their union with \hyperlink{5087820771052303592}{\texttt{Union}}, and compute a type that contains their union with \hyperlink{6895589781245489183}{\texttt{typejoin}}:




\begin{minted}{jlcon}
julia> typeintersect(Int, Float64)
Union{}

julia> Union{Int, Float64}
Union{Float64, Int64}

julia> typejoin(Int, Float64)
Real

julia> typeintersect(Signed, Union{UInt8, Int8})
Int8

julia> Union{Signed, Union{UInt8, Int8}}
Union{UInt8, Signed}

julia> typejoin(Signed, Union{UInt8, Int8})
Integer

julia> typeintersect(Tuple{Integer, Float64}, Tuple{Int, Real})
Tuple{Int64, Float64}

julia> Union{Tuple{Integer, Float64}, Tuple{Int, Real}}
Union{Tuple{Int64, Real}, Tuple{Integer, Float64}}

julia> typejoin(Tuple{Integer, Float64}, Tuple{Int, Real})
Tuple{Integer, Real}
\end{minted}



While these operations may seem abstract, they lie at the heart of Julia.  For example, method dispatch is implemented by stepping through the items in a method list until reaching one for which the type of the argument tuple is a subtype of the method signature. For this algorithm to work, it{\textquotesingle}s important that methods be sorted by their specificity, and that the search begins with the most specific methods. Consequently, Julia also implements a partial order on types; this is achieved by functionality that is similar to \texttt{<:}, but with differences that will be discussed below.



\hypertarget{11911810306869937851}{}


\subsection{UnionAll types}



Julia{\textquotesingle}s type system can also express an \emph{iterated union} of types: a union of types over all values of some variable. This is needed to describe parametric types where the values of some parameters are not known.



For example, \hyperlink{15492651498431872487}{\texttt{Array}} has two parameters as in \texttt{Array\{Int,2\}}. If we did not know the element type, we could write \texttt{Array\{T,2\} where T}, which is the union of \texttt{Array\{T,2\}} for all values of \texttt{T}: \texttt{Union\{Array\{Int8,2\}, Array\{Int16,2\}, ...\}}.



Such a type is represented by a \texttt{UnionAll} object, which contains a variable (\texttt{T} in this example, of type \texttt{TypeVar}), and a wrapped type (\texttt{Array\{T,2\}} in this example).



Consider the following methods:




\begin{minted}{julia}
f1(A::Array) = 1
f2(A::Array{Int}) = 2
f3(A::Array{T}) where {T<:Any} = 3
f4(A::Array{Any}) = 4
\end{minted}



The signature - as described in \hyperlink{10803679173955542527}{Function calls} - of \texttt{f3} is a \texttt{UnionAll} type wrapping a tuple type: \texttt{Tuple\{typeof(f3), Array\{T\}\} where T}. All but \texttt{f4} can be called with \texttt{a = [1,2]}; all but \texttt{f2} can be called with \texttt{b = Any[1,2]}.



Let{\textquotesingle}s look at these types a little more closely:




\begin{minted}{jlcon}
julia> dump(Array)
UnionAll
  var: TypeVar
    name: Symbol T
    lb: Union{}
    ub: Any
  body: UnionAll
    var: TypeVar
      name: Symbol N
      lb: Union{}
      ub: Any
    body: Array{T, N} <: DenseArray{T, N}
\end{minted}



This indicates that \texttt{Array} actually names a \texttt{UnionAll} type. There is one \texttt{UnionAll} type for each parameter, nested. The syntax \texttt{Array\{Int,2\}} is equivalent to \texttt{Array\{Int\}\{2\}}; internally each \texttt{UnionAll} is instantiated with a particular variable value, one at a time, outermost-first. This gives a natural meaning to the omission of trailing type parameters; \texttt{Array\{Int\}} gives a type equivalent to \texttt{Array\{Int,N\} where N}.



A \texttt{TypeVar} is not itself a type, but rather should be considered part of the structure of a \texttt{UnionAll} type. Type variables have lower and upper bounds on their values (in the fields \texttt{lb} and \texttt{ub}). The symbol \texttt{name} is purely cosmetic. Internally, \texttt{TypeVar}s are compared by address, so they are defined as mutable types to ensure that {\textquotedbl}different{\textquotedbl} type variables can be distinguished. However, by convention they should not be mutated.



One can construct \texttt{TypeVar}s manually:




\begin{minted}{jlcon}
julia> TypeVar(:V, Signed, Real)
Signed<:V<:Real
\end{minted}



There are convenience versions that allow you to omit any of these arguments except the \texttt{name} symbol.



The syntax \texttt{Array\{T\} where T<:Integer} is lowered to




\begin{minted}{julia}
let T = TypeVar(:T,Integer)
    UnionAll(T, Array{T})
end
\end{minted}



so it is seldom necessary to construct a \texttt{TypeVar} manually (indeed, this is to be avoided).



\hypertarget{10862279169779752699}{}


\subsection{Free variables}



The concept of a \emph{free} type variable is extremely important in the type system. We say that a variable \texttt{V} is free in type \texttt{T} if \texttt{T} does not contain the \texttt{UnionAll} that introduces variable \texttt{V}. For example, the type \texttt{Array\{Array\{V\} where V<:Integer\}} has no free variables, but the \texttt{Array\{V\}} part inside of it does have a free variable, \texttt{V}.



A type with free variables is, in some sense, not really a type at all. Consider the type \texttt{Array\{Array\{T\}\} where T}, which refers to all homogeneous arrays of arrays. The inner type \texttt{Array\{T\}}, seen by itself, might seem to refer to any kind of array. However, every element of the outer array must have the \emph{same} array type, so \texttt{Array\{T\}} cannot refer to just any old array. One could say that \texttt{Array\{T\}} effectively {\textquotedbl}occurs{\textquotedbl} multiple times, and \texttt{T} must have the same value each {\textquotedbl}time{\textquotedbl}.



For this reason, the function \texttt{jl\_has\_free\_typevars} in the C API is very important. Types for which it returns true will not give meaningful answers in subtyping and other type functions.



\hypertarget{9475610527503799038}{}


\subsection{TypeNames}



The following two \hyperlink{15492651498431872487}{\texttt{Array}} types are functionally equivalent, yet print differently:




\begin{minted}{jlcon}
julia> TV, NV = TypeVar(:T), TypeVar(:N)
(T, N)

julia> Array
Array

julia> Array{TV, NV}
Array{T, N}
\end{minted}



These can be distinguished by examining the \texttt{name} field of the type, which is an object of type \texttt{TypeName}:




\begin{minted}{jlcon}
julia> dump(Array{Int,1}.name)
TypeName
  name: Symbol Array
  module: Module Core
  names: empty SimpleVector
  wrapper: UnionAll
    var: TypeVar
      name: Symbol T
      lb: Union{}
      ub: Any
    body: UnionAll
      var: TypeVar
        name: Symbol N
        lb: Union{}
        ub: Any
      body: Array{T, N} <: DenseArray{T, N}
  cache: SimpleVector
    ...

  linearcache: SimpleVector
    ...

  hash: Int64 -7900426068641098781
  mt: MethodTable
    name: Symbol Array
    defs: Nothing nothing
    cache: Nothing nothing
    max_args: Int64 0
    kwsorter: #undef
    module: Module Core
    : Int64 0
    : Int64 0
\end{minted}



In this case, the relevant field is \texttt{wrapper}, which holds a reference to the top-level type used to make new \texttt{Array} types.




\begin{minted}{jlcon}
julia> pointer_from_objref(Array)
Ptr{Cvoid} @0x00007fcc7de64850

julia> pointer_from_objref(Array.body.body.name.wrapper)
Ptr{Cvoid} @0x00007fcc7de64850

julia> pointer_from_objref(Array{TV,NV})
Ptr{Cvoid} @0x00007fcc80c4d930

julia> pointer_from_objref(Array{TV,NV}.name.wrapper)
Ptr{Cvoid} @0x00007fcc7de64850
\end{minted}



The \texttt{wrapper} field of \hyperlink{15492651498431872487}{\texttt{Array}} points to itself, but for \texttt{Array\{TV,NV\}} it points back to the original definition of the type.



What about the other fields? \texttt{hash} assigns an integer to each type.  To examine the \texttt{cache} field, it{\textquotesingle}s helpful to pick a type that is less heavily used than Array. Let{\textquotesingle}s first create our own type:




\begin{minted}{jlcon}
julia> struct MyType{T,N} end

julia> MyType{Int,2}
MyType{Int64, 2}

julia> MyType{Float32, 5}
MyType{Float32, 5}
\end{minted}



When you instantiate a parametric type, each concrete type gets saved in a type cache (\texttt{MyType.body.body.name.cache}). However, instances containing free type variables are not cached.



\hypertarget{15137612054834825471}{}


\subsection{Tuple types}



Tuple types constitute an interesting special case.  For dispatch to work on declarations like \texttt{x::Tuple}, the type has to be able to accommodate any tuple.  Let{\textquotesingle}s check the parameters:




\begin{minted}{jlcon}
julia> Tuple
Tuple

julia> Tuple.parameters
svec(Vararg{Any})
\end{minted}



Unlike other types, tuple types are covariant in their parameters, so this definition permits \texttt{Tuple} to match any type of tuple:




\begin{minted}{jlcon}
julia> typeintersect(Tuple, Tuple{Int,Float64})
Tuple{Int64, Float64}

julia> typeintersect(Tuple{Vararg{Any}}, Tuple{Int,Float64})
Tuple{Int64, Float64}
\end{minted}



However, if a variadic (\texttt{Vararg}) tuple type has free variables it can describe different kinds of tuples:




\begin{minted}{jlcon}
julia> typeintersect(Tuple{Vararg{T} where T}, Tuple{Int,Float64})
Tuple{Int64, Float64}

julia> typeintersect(Tuple{Vararg{T}} where T, Tuple{Int,Float64})
Union{}
\end{minted}



Notice that when \texttt{T} is free with respect to the \texttt{Tuple} type (i.e. its binding \texttt{UnionAll} type is outside the \texttt{Tuple} type), only one \texttt{T} value must work over the whole type. Therefore a heterogeneous tuple does not match.



Finally, it{\textquotesingle}s worth noting that \texttt{Tuple\{\}} is distinct:




\begin{minted}{jlcon}
julia> Tuple{}
Tuple{}

julia> Tuple{}.parameters
svec()

julia> typeintersect(Tuple{}, Tuple{Int})
Union{}
\end{minted}



What is the {\textquotedbl}primary{\textquotedbl} tuple-type?




\begin{minted}{jlcon}
julia> pointer_from_objref(Tuple)
Ptr{Cvoid} @0x00007f5998a04370

julia> pointer_from_objref(Tuple{})
Ptr{Cvoid} @0x00007f5998a570d0

julia> pointer_from_objref(Tuple.name.wrapper)
Ptr{Cvoid} @0x00007f5998a04370

julia> pointer_from_objref(Tuple{}.name.wrapper)
Ptr{Cvoid} @0x00007f5998a04370
\end{minted}



so \texttt{Tuple == Tuple\{Vararg\{Any\}\}} is indeed the primary type.



\hypertarget{5206945150982308765}{}


\subsection{Diagonal types}



Consider the type \texttt{Tuple\{T,T\} where T}. A method with this signature would look like:




\begin{minted}{julia}
f(x::T, y::T) where {T} = ...
\end{minted}



According to the usual interpretation of a \texttt{UnionAll} type, this \texttt{T} ranges over all types, including \texttt{Any}, so this type should be equivalent to \texttt{Tuple\{Any,Any\}}. However, this interpretation causes some practical problems.



First, a value of \texttt{T} needs to be available inside the method definition. For a call like \texttt{f(1, 1.0)}, it{\textquotesingle}s not clear what \texttt{T} should be. It could be \texttt{Union\{Int,Float64\}}, or perhaps \hyperlink{6175959395021454412}{\texttt{Real}}. Intuitively, we expect the declaration \texttt{x::T} to mean \texttt{T === typeof(x)}. To make sure that invariant holds, we need \texttt{typeof(x) === typeof(y) === T} in this method. That implies the method should only be called for arguments of the exact same type.



It turns out that being able to dispatch on whether two values have the same type is very useful (this is used by the promotion system for example), so we have multiple reasons to want a different interpretation of \texttt{Tuple\{T,T\} where T}. To make this work we add the following rule to subtyping: if a variable occurs more than once in covariant position, it is restricted to ranging over only concrete types. ({\textquotedbl}Covariant position{\textquotedbl} means that only \texttt{Tuple} and \texttt{Union} types occur between an occurrence of a variable and the \texttt{UnionAll} type that introduces it.) Such variables are called {\textquotedbl}diagonal variables{\textquotedbl} or {\textquotedbl}concrete variables{\textquotedbl}.



So for example, \texttt{Tuple\{T,T\} where T} can be seen as \texttt{Union\{Tuple\{Int8,Int8\}, Tuple\{Int16,Int16\}, ...\}}, where \texttt{T} ranges over all concrete types. This gives rise to some interesting subtyping results. For example \texttt{Tuple\{Real,Real\}} is not a subtype of \texttt{Tuple\{T,T\} where T}, because it includes some types like \texttt{Tuple\{Int8,Int16\}} where the two elements have different types. \texttt{Tuple\{Real,Real\}} and \texttt{Tuple\{T,T\} where T} have the non-trivial intersection \texttt{Tuple\{T,T\} where T<:Real}. However, \texttt{Tuple\{Real\}} \emph{is} a subtype of \texttt{Tuple\{T\} where T}, because in that case \texttt{T} occurs only once and so is not diagonal.



Next consider a signature like the following:




\begin{minted}{julia}
f(a::Array{T}, x::T, y::T) where {T} = ...
\end{minted}



In this case, \texttt{T} occurs in invariant position inside \texttt{Array\{T\}}. That means whatever type of array is passed unambiguously determines the value of \texttt{T} – we say \texttt{T} has an \emph{equality constraint} on it. Therefore in this case the diagonal rule is not really necessary, since the array determines \texttt{T} and we can then allow \texttt{x} and \texttt{y} to be of any subtypes of \texttt{T}. So variables that occur in invariant position are never considered diagonal. This choice of behavior is slightly controversial – some feel this definition should be written as




\begin{minted}{julia}
f(a::Array{T}, x::S, y::S) where {T, S<:T} = ...
\end{minted}



to clarify whether \texttt{x} and \texttt{y} need to have the same type. In this version of the signature they would, or we could introduce a third variable for the type of \texttt{y} if \texttt{x} and \texttt{y} can have different types.



The next complication is the interaction of unions and diagonal variables, e.g.




\begin{minted}{julia}
f(x::Union{Nothing,T}, y::T) where {T} = ...
\end{minted}



Consider what this declaration means. \texttt{y} has type \texttt{T}. \texttt{x} then can have either the same type \texttt{T}, or else be of type \hyperlink{13508459519898889544}{\texttt{Nothing}}. So all of the following calls should match:




\begin{minted}{julia}
f(1, 1)
f("", "")
f(2.0, 2.0)
f(nothing, 1)
f(nothing, "")
f(nothing, 2.0)
\end{minted}



These examples are telling us something: when \texttt{x} is \texttt{nothing::Nothing}, there are no extra constraints on \texttt{y}. It is as if the method signature had \texttt{y::Any}. Indeed, we have the following type equivalence:




\begin{minted}{julia}
(Tuple{Union{Nothing,T},T} where T) == Union{Tuple{Nothing,Any}, Tuple{T,T} where T}
\end{minted}



The general rule is: a concrete variable in covariant position acts like it{\textquotesingle}s not concrete if the subtyping algorithm only \emph{uses} it once. When \texttt{x} has type \texttt{Nothing}, we don{\textquotesingle}t need to use the \texttt{T} in \texttt{Union\{Nothing,T\}}; we only use it in the second slot. This arises naturally from the observation that in \texttt{Tuple\{T\} where T} restricting \texttt{T} to concrete types makes no difference; the type is equal to \texttt{Tuple\{Any\}} either way.



However, appearing in \emph{invariant} position disqualifies a variable from being concrete whether that appearance of the variable is used or not. Otherwise types can behave differently depending on which other types they are compared to, making subtyping not transitive. For example, consider




\begin{minted}{julia}
Tuple{Int,Int8,Vector{Integer}} <: Tuple{T,T,Vector{Union{Integer,T}}} where T
\end{minted}



If the \texttt{T} inside the \texttt{Union} is ignored, then \texttt{T} is concrete and the answer is {\textquotedbl}false{\textquotedbl} since the first two types aren{\textquotesingle}t the same. But consider instead




\begin{minted}{julia}
Tuple{Int,Int8,Vector{Any}} <: Tuple{T,T,Vector{Union{Integer,T}}} where T
\end{minted}



Now we cannot ignore the \texttt{T} in the \texttt{Union} (we must have \texttt{T == Any}), so \texttt{T} is not concrete and the answer is {\textquotedbl}true{\textquotedbl}. That would make the concreteness of \texttt{T} depend on the other type, which is not acceptable since a type must have a clear meaning on its own. Therefore the appearance of \texttt{T} inside \texttt{Vector} is considered in both cases.



\hypertarget{12600482412457091491}{}


\subsection{Subtyping diagonal variables}



The subtyping algorithm for diagonal variables has two components: (1) identifying variable occurrences, and (2) ensuring that diagonal variables range over concrete types only.



The first task is accomplished by keeping counters \texttt{occurs\_inv} and \texttt{occurs\_cov} (in \texttt{src/subtype.c}) for each variable in the environment, tracking the number of invariant and covariant occurrences, respectively. A variable is diagonal when \texttt{occurs\_inv == 0 \&\& occurs\_cov > 1}.



The second task is accomplished by imposing a condition on a variable{\textquotesingle}s lower bound. As the subtyping algorithm runs, it narrows the bounds of each variable (raising lower bounds and lowering upper bounds) to keep track of the range of variable values for which the subtype relation would hold. When we are done evaluating the body of a \texttt{UnionAll} type whose variable is diagonal, we look at the final values of the bounds. Since the variable must be concrete, a contradiction occurs if its lower bound could not be a subtype of a concrete type. For example, an abstract type like \hyperlink{6514416309183787338}{\texttt{AbstractArray}} cannot be a subtype of a concrete type, but a concrete type like \texttt{Int} can be, and the empty type \texttt{Bottom} can be as well. If a lower bound fails this test the algorithm stops with the answer \texttt{false}.



For example, in the problem \texttt{Tuple\{Int,String\} <: Tuple\{T,T\} where T}, we derive that this would be true if \texttt{T} were a supertype of \texttt{Union\{Int,String\}}. However, \texttt{Union\{Int,String\}} is an abstract type, so the relation does not hold.



This concreteness test is done by the function \texttt{is\_leaf\_bound}. Note that this test is slightly different from \texttt{jl\_is\_leaf\_type}, since it also returns \texttt{true} for \texttt{Bottom}. Currently this function is heuristic, and does not catch all possible concrete types. The difficulty is that whether a lower bound is concrete might depend on the values of other type variable bounds. For example, \texttt{Vector\{T\}} is equivalent to the concrete type \texttt{Vector\{Int\}} only if both the upper and lower bounds of \texttt{T} equal \texttt{Int}. We have not yet worked out a complete algorithm for this.



\hypertarget{37604590457653524}{}


\subsection{Introduction to the internal machinery}



Most operations for dealing with types are found in the files \texttt{jltypes.c} and \texttt{subtype.c}. A good way to start is to watch subtyping in action. Build Julia with \texttt{make debug} and fire up Julia within a debugger. \hyperlink{6515020785971536083}{gdb debugging tips} has some tips which may be useful.



Because the subtyping code is used heavily in the REPL itself – and hence breakpoints in this code get triggered often – it will be easiest if you make the following definition:




\begin{minted}{jlcon}
julia> function mysubtype(a,b)
           ccall(:jl_breakpoint, Cvoid, (Any,), nothing)
           a <: b
       end
\end{minted}



and then set a breakpoint in \texttt{jl\_breakpoint}.  Once this breakpoint gets triggered, you can set breakpoints in other functions.



As a warm-up, try the following:




\begin{minted}{julia}
mysubtype(Tuple{Int, Float64}, Tuple{Integer, Real})
\end{minted}



We can make it more interesting by trying a more complex case:




\begin{minted}{julia}
mysubtype(Tuple{Array{Int,2}, Int8}, Tuple{Array{T}, T} where T)
\end{minted}



\hypertarget{9271541181781970079}{}


\subsection{Subtyping and method sorting}



The \texttt{type\_morespecific} functions are used for imposing a partial order on functions in method tables (from most-to-least specific). Specificity is strict; if \texttt{a} is more specific than \texttt{b}, then \texttt{a} does not equal \texttt{b} and \texttt{b} is not more specific than \texttt{a}.



If \texttt{a} is a strict subtype of \texttt{b}, then it is automatically considered more specific. From there, \texttt{type\_morespecific} employs some less formal rules. For example, \texttt{subtype} is sensitive to the number of arguments, but \texttt{type\_morespecific} may not be. In particular, \texttt{Tuple\{Int,AbstractFloat\}} is more specific than \texttt{Tuple\{Integer\}}, even though it is not a subtype.  (Of \texttt{Tuple\{Int,AbstractFloat\}} and \texttt{Tuple\{Integer,Float64\}}, neither is more specific than the other.)  Likewise, \texttt{Tuple\{Int,Vararg\{Int\}\}} is not a subtype of \texttt{Tuple\{Integer\}}, but it is considered more specific. However, \texttt{morespecific} does get a bonus for length: in particular, \texttt{Tuple\{Int,Int\}} is more specific than \texttt{Tuple\{Int,Vararg\{Int\}\}}.



If you{\textquotesingle}re debugging how methods get sorted, it can be convenient to define the function:




\begin{minted}{julia}
type_morespecific(a, b) = ccall(:jl_type_morespecific, Cint, (Any,Any), a, b)
\end{minted}



which allows you to test whether tuple type \texttt{a} is more specific than tuple type \texttt{b}.



\hypertarget{5831288113328207392}{}


\section{Memory layout of Julia Objects}



\hypertarget{9050896398576860708}{}


\subsection{Object layout (\texttt{jl\_value\_t})}



The \texttt{jl\_value\_t} struct is the name for a block of memory owned by the Julia Garbage Collector, representing the data associated with a Julia object in memory. Absent any type information, it is simply an opaque pointer:




\begin{lstlisting}
typedef struct jl_value_t* jl_pvalue_t;
\end{lstlisting}



Each \texttt{jl\_value\_t} struct is contained in a \texttt{jl\_typetag\_t} struct that contains metadata information about the Julia object, such as its type and garbage collector (gc) reachability:




\begin{lstlisting}
typedef struct {
    opaque metadata;
    jl_value_t value;
} jl_typetag_t;
\end{lstlisting}



The type of any Julia object is an instance of a leaf \texttt{jl\_datatype\_t} object. The \texttt{jl\_typeof()} function can be used to query for it:




\begin{lstlisting}
jl_value_t *jl_typeof(jl_value_t *v);
\end{lstlisting}



The layout of the object depends on its type. Reflection methods can be used to inspect that layout. A field can be accessed by calling one of the get-field methods:




\begin{lstlisting}
jl_value_t *jl_get_nth_field_checked(jl_value_t *v, size_t i);
jl_value_t *jl_get_field(jl_value_t *o, char *fld);
\end{lstlisting}



If the field types are known, a priori, to be all pointers, the values can also be extracted directly as an array access:




\begin{lstlisting}
jl_value_t *v = value->fieldptr[n];
\end{lstlisting}



As an example, a {\textquotedbl}boxed{\textquotedbl} \texttt{uint16\_t} is stored as follows:




\begin{lstlisting}
struct {
    opaque metadata;
    struct {
        uint16_t data;        // -- 2 bytes
    } jl_value_t;
};
\end{lstlisting}



This object is created by \texttt{jl\_box\_uint16()}. Note that the \texttt{jl\_value\_t} pointer references the data portion, not the metadata at the top of the struct.



A value may be stored {\textquotedbl}unboxed{\textquotedbl} in many circumstances (just the data, without the metadata, and possibly not even stored but just kept in registers), so it is unsafe to assume that the address of a box is a unique identifier. The {\textquotedbl}egal{\textquotedbl} test (corresponding to the \texttt{===} function in Julia), should instead be used to compare two unknown objects for equivalence:




\begin{lstlisting}
int jl_egal(jl_value_t *a, jl_value_t *b);
\end{lstlisting}



This optimization should be relatively transparent to the API, since the object will be {\textquotedbl}boxed{\textquotedbl} on-demand, whenever a \texttt{jl\_value\_t} pointer is needed.



Note that modification of a \texttt{jl\_value\_t} pointer in memory is permitted only if the object is mutable. Otherwise, modification of the value may corrupt the program and the result will be undefined. The mutability property of a value can be queried for with:




\begin{lstlisting}
int jl_is_mutable(jl_value_t *v);
\end{lstlisting}



If the object being stored is a \texttt{jl\_value\_t}, the Julia garbage collector must be notified also:




\begin{lstlisting}
void jl_gc_wb(jl_value_t *parent, jl_value_t *ptr);
\end{lstlisting}



However, the \hyperlink{51310765964131637}{Embedding Julia} section of the manual is also required reading at this point, for covering other details of boxing and unboxing various types, and understanding the gc interactions.



Mirror structs for some of the built-in types are \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{defined in \texttt{julia.h}}. The corresponding global \texttt{jl\_datatype\_t} objects are created by \href{https://github.com/JuliaLang/julia/blob/master/src/jltypes.c}{\texttt{jl\_init\_types} in \texttt{jltypes.c}}.



\hypertarget{660883080955975432}{}


\subsection{Garbage collector mark bits}



The garbage collector uses several bits from the metadata portion of the \texttt{jl\_typetag\_t} to track each object in the system. Further details about this algorithm can be found in the comments of the \href{https://github.com/JuliaLang/julia/blob/master/src/gc.c}{garbage collector implementation in \texttt{gc.c}}.



\hypertarget{14420252243983980472}{}


\subsection{Object allocation}



Most new objects are allocated by \texttt{jl\_new\_structv()}:




\begin{lstlisting}
jl_value_t *jl_new_struct(jl_datatype_t *type, ...);
jl_value_t *jl_new_structv(jl_datatype_t *type, jl_value_t **args, uint32_t na);
\end{lstlisting}



Although, \hyperlink{12980593021531333073}{\texttt{isbits}} objects can be also constructed directly from memory:




\begin{lstlisting}
jl_value_t *jl_new_bits(jl_value_t *bt, void *data)
\end{lstlisting}



And some objects have special constructors that must be used instead of the above functions:



Types:




\begin{lstlisting}
jl_datatype_t *jl_apply_type(jl_datatype_t *tc, jl_tuple_t *params);
jl_datatype_t *jl_apply_array_type(jl_datatype_t *type, size_t dim);
\end{lstlisting}



While these are the most commonly used options, there are more low-level constructors too, which you can find declared in \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{\texttt{julia.h}}. These are used in \texttt{jl\_init\_types()} to create the initial types needed to bootstrap the creation of the Julia system image.



Tuples:




\begin{lstlisting}
jl_tuple_t *jl_tuple(size_t n, ...);
jl_tuple_t *jl_tuplev(size_t n, jl_value_t **v);
jl_tuple_t *jl_alloc_tuple(size_t n);
\end{lstlisting}



The representation of tuples is highly unique in the Julia object representation ecosystem. In some cases, a \hyperlink{12342862450082530092}{\texttt{Base.tuple()}} object may be an array of pointers to the objects contained by the tuple equivalent to:




\begin{lstlisting}
typedef struct {
    size_t length;
    jl_value_t *data[length];
} jl_tuple_t;
\end{lstlisting}



However, in other cases, the tuple may be converted to an anonymous \hyperlink{12980593021531333073}{\texttt{isbits}} type and stored unboxed, or it may not stored at all (if it is not being used in a generic context as a \texttt{jl\_value\_t*}).



Symbols:




\begin{lstlisting}
jl_sym_t *jl_symbol(const char *str);
\end{lstlisting}



Functions and MethodInstance:




\begin{lstlisting}
jl_function_t *jl_new_generic_function(jl_sym_t *name);
jl_method_instance_t *jl_new_method_instance(jl_value_t *ast, jl_tuple_t *sparams);
\end{lstlisting}



Arrays:




\begin{lstlisting}
jl_array_t *jl_new_array(jl_value_t *atype, jl_tuple_t *dims);
jl_array_t *jl_new_arrayv(jl_value_t *atype, ...);
jl_array_t *jl_alloc_array_1d(jl_value_t *atype, size_t nr);
jl_array_t *jl_alloc_array_2d(jl_value_t *atype, size_t nr, size_t nc);
jl_array_t *jl_alloc_array_3d(jl_value_t *atype, size_t nr, size_t nc, size_t z);
jl_array_t *jl_alloc_vec_any(size_t n);
\end{lstlisting}



Note that many of these have alternative allocation functions for various special-purposes. The list here reflects the more common usages, but a more complete list can be found by reading the \href{https://github.com/JuliaLang/julia/blob/master/src/julia.h}{\texttt{julia.h} header file}.



Internal to Julia, storage is typically allocated by \texttt{newstruct()} (or \texttt{newobj()} for the special types):




\begin{lstlisting}
jl_value_t *newstruct(jl_value_t *type);
jl_value_t *newobj(jl_value_t *type, size_t nfields);
\end{lstlisting}



And at the lowest level, memory is getting allocated by a call to the garbage collector (in \texttt{gc.c}), then tagged with its type:




\begin{lstlisting}
jl_value_t *jl_gc_allocobj(size_t nbytes);
void jl_set_typeof(jl_value_t *v, jl_datatype_t *type);
\end{lstlisting}



Note that all objects are allocated in multiples of 4 bytes and aligned to the platform pointer size. Memory is allocated from a pool for smaller objects, or directly with \texttt{malloc()} for large objects.



\begin{quote}
\textbf{Singleton Types}

Singleton types have only one instance and no data fields. Singleton instances have a size of 0 bytes, and consist only of their metadata. e.g. \texttt{nothing::Nothing}.

See \hyperlink{14008188290941962431}{Singleton Types} and \hyperlink{16167110558547130797}{Nothingness and missing values}

\end{quote}


\hypertarget{12968133543622581514}{}


\section{Eval of Julia code}



One of the hardest parts about learning how the Julia Language runs code is learning how all of the pieces work together to execute a block of code.



Each chunk of code typically makes a trip through many steps with potentially unfamiliar names, such as (in no particular order): flisp, AST, C++, LLVM, \texttt{eval}, \texttt{typeinf}, \texttt{macroexpand}, sysimg (or system image), bootstrapping, compile, parse, execute, JIT, interpret, box, unbox, intrinsic function, and primitive function, before turning into the desired result (hopefully).



\begin{quote}
\textbf{Definitions}

\begin{itemize}
\item REPL

REPL stands for Read-Eval-Print Loop. It{\textquotesingle}s just what we call the command line environment for short.


\item AST

Abstract Syntax Tree The AST is the digital representation of the code structure. In this form the code has been tokenized for meaning so that it is more suitable for manipulation and execution.

\end{itemize}
\end{quote}


\hypertarget{12349293482799060845}{}


\subsection{Julia Execution}



The 10,000 foot view of the whole process is as follows:



\begin{itemize}
\item[1.  ] The user starts \texttt{julia}.


\item[2.  ] The C function \texttt{main()} from \texttt{cli/loader\_exe.c} gets called. This function processes the command line arguments, filling in the \texttt{jl\_options} struct and setting the variable \texttt{ARGS}. It then initializes Julia (by calling \href{https://github.com/JuliaLang/julia/blob/master/src/task.c}{\texttt{julia\_init} in \texttt{task.c}}, which may load a previously compiled \hyperlink{6082338945993475185}{sysimg}). Finally, it passes off control to Julia by calling \href{https://github.com/JuliaLang/julia/blob/master/base/client.jl}{\texttt{Base.\_start()}}.


\item[3.  ] When \texttt{\_start()} takes over control, the subsequent sequence of commands depends on the command line arguments given. For example, if a filename was supplied, it will proceed to execute that file. Otherwise, it will start an interactive REPL.


\item[4.  ] Skipping the details about how the REPL interacts with the user, let{\textquotesingle}s just say the program ends up with a block of code that it wants to run.


\item[5.  ] If the block of code to run is in a file, \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_load(char *filename)}} gets invoked to load the file and \hyperlink{14838640034628506824}{parse} it. Each fragment of code is then passed to \texttt{eval} to execute.


\item[6.  ] Each fragment of code (or AST), is handed off to \hyperlink{7507639810592563424}{\texttt{eval()}} to turn into results.


\item[7.  ] \hyperlink{7507639810592563424}{\texttt{eval()}} takes each code fragment and tries to run it in \href{https://github.com/JuliaLang/julia/blob/master/src/toplevel.c}{\texttt{jl\_toplevel\_eval\_flex()}}.


\item[8.  ] \texttt{jl\_toplevel\_eval\_flex()} decides whether the code is a {\textquotedbl}toplevel{\textquotedbl} action (such as \texttt{using} or \texttt{module}), which would be invalid inside a function. If so, it passes off the code to the toplevel interpreter.


\item[9.  ] \texttt{jl\_toplevel\_eval\_flex()} then \hyperlink{16669853702383402486}{expands} the code to eliminate any macros and to {\textquotedbl}lower{\textquotedbl} the AST to make it simpler to execute.


\item[10. ] \texttt{jl\_toplevel\_eval\_flex()} then uses some simple heuristics to decide whether to JIT compile the  AST or to interpret it directly.


\item[11. ] The bulk of the work to interpret code is handled by \href{https://github.com/JuliaLang/julia/blob/master/src/interpreter.c}{\texttt{eval} in \texttt{interpreter.c}}.


\item[12. ] If instead, the code is compiled, the bulk of the work is handled by \texttt{codegen.cpp}. Whenever a  Julia function is called for the first time with a given set of argument types, \hyperlink{6510123671388929580}{type inference}  will be run on that function. This information is used by the \hyperlink{526576549562645049}{codegen} step to generate  faster code.


\item[13. ] Eventually, the user quits the REPL, or the end of the program is reached, and the \texttt{\_start()}  method returns.


\item[14. ] Just before exiting, \texttt{main()} calls \href{https://github.com/JuliaLang/julia/blob/master/src/init.c}{\texttt{jl\_atexit\_hook(exit\_code)}}.  This calls \texttt{Base.\_atexit()} (which calls any functions registered to \hyperlink{17479944696971324992}{\texttt{atexit()}} inside  Julia). Then it calls \href{https://github.com/JuliaLang/julia/blob/master/src/gc.c}{\texttt{jl\_gc\_run\_all\_finalizers()}}.  Finally, it gracefully cleans up all \texttt{libuv} handles and waits for them to flush and close.

\end{itemize}


\hypertarget{6801832859572424777}{}


\subsection{Parsing}



The Julia parser is a small lisp program written in femtolisp, the source-code for which is distributed inside Julia in \href{https://github.com/JuliaLang/julia/tree/master/src/flisp}{src/flisp}.



The interface functions for this are primarily defined in \href{https://github.com/JuliaLang/julia/blob/master/src/jlfrontend.scm}{\texttt{jlfrontend.scm}}. The code in \href{https://github.com/JuliaLang/julia/blob/master/src/ast.c}{\texttt{ast.c}} handles this handoff on the Julia side.



The other relevant files at this stage are \href{https://github.com/JuliaLang/julia/blob/master/src/julia-parser.scm}{\texttt{julia-parser.scm}}, which handles tokenizing Julia code and turning it into an AST, and \href{https://github.com/JuliaLang/julia/blob/master/src/julia-syntax.scm}{\texttt{julia-syntax.scm}}, which handles transforming complex AST representations into simpler, {\textquotedbl}lowered{\textquotedbl} AST representations which are more suitable for analysis and execution.



If you want to test the parser without re-building Julia in its entirety, you can run the frontend on its own as follows:




\begin{lstlisting}
$ cd src
$ flisp/flisp
> (load "jlfrontend.scm")
> (jl-parse-file "<filename>")
\end{lstlisting}



\hypertarget{13925460440315781353}{}


\subsection{Macro Expansion}



When \hyperlink{7507639810592563424}{\texttt{eval()}} encounters a macro, it expands that AST node before attempting to evaluate the expression. Macro expansion involves a handoff from \hyperlink{7507639810592563424}{\texttt{eval()}} (in Julia), to the parser function \texttt{jl\_macroexpand()} (written in \texttt{flisp}) to the Julia macro itself (written in - what else - Julia) via \texttt{fl\_invoke\_julia\_macro()}, and back.



Typically, macro expansion is invoked as a first step during a call to \hyperlink{6644553029841096787}{\texttt{Meta.lower()}}/\texttt{jl\_expand()}, although it can also be invoked directly by a call to \hyperlink{8018172489611994488}{\texttt{macroexpand()}}/\texttt{jl\_macroexpand()}.



\hypertarget{5553247398724394157}{}


\subsection{Type Inference}



Type inference is implemented in Julia by \href{https://github.com/JuliaLang/julia/blob/master/base/compiler/typeinfer.jl}{\texttt{typeinf()} in \texttt{compiler/typeinfer.jl}}. Type inference is the process of examining a Julia function and determining bounds for the types of each of its variables, as well as bounds on the type of the return value from the function. This enables many future optimizations, such as unboxing of known immutable values, and compile-time hoisting of various run-time operations such as computing field offsets and function pointers. Type inference may also include other steps such as constant propagation and inlining.



\begin{quote}
\textbf{More Definitions}

\begin{itemize}
\item JIT

Just-In-Time Compilation The process of generating native-machine code into memory right when it is needed.


\item LLVM

Low-Level Virtual Machine (a compiler) The Julia JIT compiler is a program/library called libLLVM. Codegen in Julia refers both to the process of taking a Julia AST and turning it into LLVM instructions, and the process of LLVM optimizing that and turning it into native assembly instructions.


\item C++

The programming language that LLVM is implemented in, which means that codegen is also implemented in this language. The rest of Julia{\textquotesingle}s library is implemented in C, in part because its smaller feature set makes it more usable as a cross-language interface layer.


\item box

This term is used to describe the process of taking a value and allocating a wrapper around the data that is tracked by the garbage collector (gc) and is tagged with the object{\textquotesingle}s type.


\item unbox

The reverse of boxing a value. This operation enables more efficient manipulation of data when the type of that data is fully known at compile-time (through type inference).


\item generic function

A Julia function composed of multiple {\textquotedbl}methods{\textquotedbl} that are selected for dynamic dispatch based on the argument type-signature


\item anonymous function or {\textquotedbl}method{\textquotedbl}

A Julia function without a name and without type-dispatch capabilities


\item primitive function

A function implemented in C but exposed in Julia as a named function {\textquotedbl}method{\textquotedbl} (albeit without generic function dispatch capabilities, similar to a anonymous function)


\item intrinsic function

A low-level operation exposed as a function in Julia. These pseudo-functions implement operations on raw bits such as add and sign extend that cannot be expressed directly in any other way. Since they operate on bits directly, they must be compiled into a function and surrounded by a call to \texttt{Core.Intrinsics.box(T, ...)} to reassign type information to the value.

\end{itemize}
\end{quote}


\hypertarget{16251221274618963128}{}


\subsection{JIT Code Generation}



Codegen is the process of turning a Julia AST into native machine code.



The JIT environment is initialized by an early call to \href{https://github.com/JuliaLang/julia/blob/master/src/codegen.cpp}{\texttt{jl\_init\_codegen} in \texttt{codegen.cpp}}.



On demand, a Julia method is converted into a native function by the function \texttt{emit\_function(jl\_method\_instance\_t*)}. (note, when using the MCJIT (in LLVM v3.4+), each function must be JIT into a new module.) This function recursively calls \texttt{emit\_expr()} until the entire function has been emitted.



Much of the remaining bulk of this file is devoted to various manual optimizations of specific code patterns. For example, \texttt{emit\_known\_call()} knows how to inline many of the primitive functions (defined in \href{https://github.com/JuliaLang/julia/blob/master/src/builtins.c}{\texttt{builtins.c}}) for various combinations of argument types.



Other parts of codegen are handled by various helper files:



\begin{itemize}
\item \href{https://github.com/JuliaLang/julia/blob/master/src/debuginfo.cpp}{\texttt{debuginfo.cpp}}

Handles backtraces for JIT functions


\item \href{https://github.com/JuliaLang/julia/blob/master/src/ccall.cpp}{\texttt{ccall.cpp}}

Handles the ccall and llvmcall FFI, along with various \texttt{abi\_*.cpp} files


\item \href{https://github.com/JuliaLang/julia/blob/master/src/intrinsics.cpp}{\texttt{intrinsics.cpp}}

Handles the emission of various low-level intrinsic functions

\end{itemize}


\begin{quote}
\textbf{Bootstrapping}

The process of creating a new system image is called {\textquotedbl}bootstrapping{\textquotedbl}.

The etymology of this word comes from the phrase {\textquotedbl}pulling oneself up by the bootstraps{\textquotedbl}, and refers to the idea of starting from a very limited set of available functions and definitions and ending with the creation of a full-featured environment.

\end{quote}


\hypertarget{9959120445934014648}{}


\subsection{System Image}



The system image is a precompiled archive of a set of Julia files. The \texttt{sys.ji} file distributed with Julia is one such system image, generated by executing the file \href{https://github.com/JuliaLang/julia/blob/master/base/sysimg.jl}{\texttt{sysimg.jl}}, and serializing the resulting environment (including Types, Functions, Modules, and all other defined values) into a file. Therefore, it contains a frozen version of the \texttt{Main}, \texttt{Core}, and \texttt{Base} modules (and whatever else was in the environment at the end of bootstrapping). This serializer/deserializer is implemented by \href{https://github.com/JuliaLang/julia/blob/master/src/staticdata.c}{\texttt{jl\_save\_system\_image}/\texttt{jl\_restore\_system\_image} in \texttt{staticdata.c}}.



If there is no sysimg file (\texttt{jl\_options.image\_file == NULL}), this also implies that \texttt{--build} was given on the command line, so the final result should be a new sysimg file. During Julia initialization, minimal \texttt{Core} and \texttt{Main} modules are created. Then a file named \texttt{boot.jl} is evaluated from the current directory. Julia then evaluates any file given as a command line argument until it reaches the end. Finally, it saves the resulting environment to a {\textquotedbl}sysimg{\textquotedbl} file for use as a starting point for a future Julia run.



\hypertarget{290466951415325789}{}


\section{Calling Conventions}



Julia uses three calling conventions for four distinct purposes:




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
Name & Prefix & Purpose \\
\hline
Native & \texttt{julia\_} & Speed via specialized signatures \\
\hline
JL Call & \texttt{jlcall\_} & Wrapper for generic calls \\
\hline
JL Call & \texttt{jl\_} & Builtins \\
\hline
C ABI & \texttt{jlcapi\_} & Wrapper callable from C \\
\hline
\end{tabulary}

\end{table}



\hypertarget{4788166660204734834}{}


\subsection{Julia Native Calling Convention}



The native calling convention is designed for fast non-generic calls. It usually uses a specialized signature.



\begin{itemize}
\item LLVM ghosts (zero-length types) are omitted.


\item LLVM scalars and vectors are passed by value.


\item LLVM aggregates (arrays and structs) are passed by reference.

\end{itemize}


A small return values is returned as LLVM return values. A large return values is returned via the {\textquotedbl}structure return{\textquotedbl} (\texttt{sret}) convention, where the caller provides a pointer to a return slot.



An argument or return values that is a homogeneous tuple is sometimes represented as an LLVM vector instead of an LLVM array.



\hypertarget{10962628785392118342}{}


\subsection{JL Call Convention}



The JL Call convention is for builtins and generic dispatch. Hand-written functions using this convention are declared via the macro \texttt{JL\_CALLABLE}. The convention uses exactly 3 parameters:



\begin{itemize}
\item \texttt{F}  - Julia representation of function that is being applied


\item \texttt{args} - pointer to array of pointers to boxes


\item \texttt{nargs} - length of the array

\end{itemize}


The return value is a pointer to a box.



\hypertarget{16764615067045412370}{}


\subsection{C ABI}



C ABI wrappers enable calling Julia from C. The wrapper calls a function using the native calling convention.



Tuples are always represented as C arrays.



\hypertarget{7542203617935209330}{}


\section{High-level Overview of the Native-Code Generation Process}



\hypertarget{6164157073495091365}{}


\subsection{Representation of Pointers}



When emitting code to an object file, pointers will be emitted as relocations. The deserialization code will ensure any object that pointed to one of these constants gets recreated and contains the right runtime pointer.



Otherwise, they will be emitted as literal constants.



To emit one of these objects, call \texttt{literal\_pointer\_val}. It{\textquotesingle}ll handle tracking the Julia value and the LLVM global, ensuring they are valid both for the current runtime and after deserialization.



When emitted into the object file, these globals are stored as references in a large \texttt{gvals} table. This allows the deserializer to reference them by index, and implement a custom manual mechanism similar to a Global Offset Table (GOT) to restore them.



Function pointers are handled similarly. They are stored as values in a large \texttt{fvals} table. Like globals, this allows the deserializer to reference them by index.



Note that \texttt{extern} functions are handled separately, with names, via the usual symbol resolution mechanism in the linker.



Note too that \texttt{ccall} functions are also handled separately, via a manual GOT and Procedure Linkage Table (PLT).



\hypertarget{9352283582715079729}{}


\subsection{Representation of Intermediate Values}



Values are passed around in a \texttt{jl\_cgval\_t} struct. This represents an R-value, and includes enough information to determine how to assign or pass it somewhere.



They are created via one of the helper constructors, usually: \texttt{mark\_julia\_type} (for immediate values) and \texttt{mark\_julia\_slot} (for pointers to values).



The function \texttt{convert\_julia\_type} can transform between any two types. It returns an R-value with \texttt{cgval.typ} set to \texttt{typ}. It{\textquotesingle}ll cast the object to the requested representation, making heap boxes, allocating stack copies, and computing tagged unions as needed to change the representation.



By contrast \texttt{update\_julia\_type} will change \texttt{cgval.typ} to \texttt{typ}, only if it can be done at zero-cost (i.e. without emitting any code).



\hypertarget{3738811278233217209}{}


\subsection{Union representation}



Inferred union types may be stack allocated via a tagged type representation.



The primitive routines that need to be able to handle tagged unions are:



\begin{itemize}
\item mark-type


\item load-local


\item store-local


\item isa


\item is


\item emit\_typeof


\item emit\_sizeof


\item boxed


\item unbox


\item specialized cc-ret

\end{itemize}


Everything else should be possible to handle in inference by using these primitives to implement union-splitting.



The representation of the tagged-union is as a pair of \texttt{< void* union, byte selector >}. The selector is fixed-size as \texttt{byte \& 0x7f}, and will union-tag the first 126 isbits. It records the one-based depth-first count into the type-union of the isbits objects inside. An index of zero indicates that the \texttt{union*} is actually a tagged heap-allocated \texttt{jl\_value\_t*}, and needs to be treated as normal for a boxed object rather than as a tagged union.



The high bit of the selector (\texttt{byte \& 0x80}) can be tested to determine if the \texttt{void*} is actually a heap-allocated (\texttt{jl\_value\_t*}) box, thus avoiding the cost of re-allocating a box, while maintaining the ability to efficiently handle union-splitting based on the low bits.



It is guaranteed that \texttt{byte \& 0x7f} is an exact test for the type, if the value can be represented by a tag – it will never be marked \texttt{byte = 0x80}. It is not necessary to also test the type-tag when testing \texttt{isa}.



The \texttt{union*} memory region may be allocated at \emph{any} size. The only constraint is that it is big enough to contain the data currently specified by \texttt{selector}. It might not be big enough to contain the union of all types that could be stored there according to the associated Union type field. Use appropriate care when copying.



\hypertarget{5801902447580234191}{}


\subsection{Specialized Calling Convention Signature Representation}



A \texttt{jl\_returninfo\_t} object describes the calling convention details of any callable.



If any of the arguments or return type of a method can be represented unboxed, and the method is not varargs, it{\textquotesingle}ll be given an optimized calling convention signature based on its \texttt{specTypes} and \texttt{rettype} fields.



The general principles are that:



\begin{itemize}
\item Primitive types get passed in int/float registers.


\item Tuples of VecElement types get passed in vector registers.


\item Structs get passed on the stack.


\item Return values are handle similarly to arguments, with a size-cutoff at which they will instead be returned via a hidden sret argument.

\end{itemize}


The total logic for this is implemented by \texttt{get\_specsig\_function} and \texttt{deserves\_sret}.



Additionally, if the return type is a union, it may be returned as a pair of values (a pointer and a tag). If the union values can be stack-allocated, then sufficient space to store them will also be passed as a hidden first argument. It is up to the callee whether the returned pointer will point to this space, a boxed object, or even other constant memory.



\hypertarget{16329287024252296077}{}


\section{Julia Functions}



This document will explain how functions, method definitions, and method tables work.



\hypertarget{791619394702824269}{}


\subsection{Method Tables}



Every function in Julia is a generic function. A generic function is conceptually a single function, but consists of many definitions, or methods. The methods of a generic function are stored in a method table. Method tables (type \texttt{MethodTable}) are associated with \texttt{TypeName}s. A \texttt{TypeName} describes a family of parameterized types. For example \texttt{Complex\{Float32\}} and \texttt{Complex\{Float64\}} share the same \texttt{Complex} type name object.



All objects in Julia are potentially callable, because every object has a type, which in turn has a \texttt{TypeName}.



\hypertarget{5677321158644388515}{}


\subsection{Function calls}



Given the call \texttt{f(x,y)}, the following steps are performed: first, the method table to use is accessed as \texttt{typeof(f).name.mt}. Second, an argument tuple type is formed, \texttt{Tuple\{typeof(f), typeof(x), typeof(y)\}}. Note that the type of the function itself is the first element. This is because the type might have parameters, and so needs to take part in dispatch. This tuple type is looked up in the method table.



This dispatch process is performed by \texttt{jl\_apply\_generic}, which takes two arguments: a pointer to an array of the values f, x, and y, and the number of values (in this case 3).



Throughout the system, there are two kinds of APIs that handle functions and argument lists: those that accept the function and arguments separately, and those that accept a single argument structure. In the first kind of API, the {\textquotedbl}arguments{\textquotedbl} part does \emph{not} contain information about the function, since that is passed separately. In the second kind of API, the function is the first element of the argument structure.



For example, the following function for performing a call accepts just an \texttt{args} pointer, so the first element of the args array will be the function to call:




\begin{lstlisting}
jl_value_t *jl_apply(jl_value_t **args, uint32_t nargs)
\end{lstlisting}



This entry point for the same functionality accepts the function separately, so the \texttt{args} array does not contain the function:




\begin{lstlisting}
jl_value_t *jl_call(jl_function_t *f, jl_value_t **args, int32_t nargs);
\end{lstlisting}



\hypertarget{17618724806250544535}{}


\subsection{Adding methods}



Given the above dispatch process, conceptually all that is needed to add a new method is (1) a tuple type, and (2) code for the body of the method. \texttt{jl\_method\_def} implements this operation. \texttt{jl\_first\_argument\_datatype} is called to extract the relevant method table from what would be the type of the first argument. This is much more complicated than the corresponding procedure during dispatch, since the argument tuple type might be abstract. For example, we can define:




\begin{minted}{julia}
(::Union{Foo{Int},Foo{Int8}})(x) = 0
\end{minted}



which works since all possible matching methods would belong to the same method table.



\hypertarget{16014622082824674579}{}


\subsection{Creating generic functions}



Since every object is callable, nothing special is needed to create a generic function. Therefore \texttt{jl\_new\_generic\_function} simply creates a new singleton (0 size) subtype of \texttt{Function} and returns its instance. A function can have a mnemonic {\textquotedbl}display name{\textquotedbl} which is used in debug info and when printing objects. For example the name of \texttt{Base.sin} is \texttt{sin}. By convention, the name of the created \emph{type} is the same as the function name, with a \texttt{\#} prepended. So \texttt{typeof(sin)} is \texttt{Base.\#sin}.



\hypertarget{5087486620251640638}{}


\subsection{Closures}



A closure is simply a callable object with field names corresponding to captured variables. For example, the following code:




\begin{minted}{julia}
function adder(x)
    return y->x+y
end
\end{minted}



is lowered to (roughly):




\begin{minted}{julia}
struct ##1{T}
    x::T
end

(_::##1)(y) = _.x + y

function adder(x)
    return ##1(x)
end
\end{minted}



\hypertarget{3597362352537600414}{}


\subsection{Constructors}



A constructor call is just a call to a type. The method table for \texttt{Type} contains all constructor definitions. All subtypes of \texttt{Type} (\texttt{Type}, \texttt{UnionAll}, \texttt{Union}, and \texttt{DataType}) currently share a method table via special arrangement.



\hypertarget{4994602509934264781}{}


\subsection{Builtins}



The {\textquotedbl}builtin{\textquotedbl} functions, defined in the \texttt{Core} module, are:




\begin{lstlisting}
=== typeof sizeof <: isa typeassert throw tuple getfield setfield! fieldtype
nfields isdefined arrayref arrayset arraysize applicable invoke apply_type _apply
_expr svec
\end{lstlisting}



These are all singleton objects whose types are subtypes of \texttt{Builtin}, which is a subtype of \texttt{Function}. Their purpose is to expose entry points in the run time that use the {\textquotedbl}jlcall{\textquotedbl} calling convention:




\begin{lstlisting}
jl_value_t *(jl_value_t*, jl_value_t**, uint32_t)
\end{lstlisting}



The method tables of builtins are empty. Instead, they have a single catch-all method cache entry (\texttt{Tuple\{Vararg\{Any\}\}}) whose jlcall fptr points to the correct function. This is kind of a hack but works reasonably well.



\hypertarget{51534660369225743}{}


\subsection{Keyword arguments}



Keyword arguments work by associating a special, hidden function object with each method table that has definitions with keyword arguments. This function is called the {\textquotedbl}keyword argument sorter{\textquotedbl} or {\textquotedbl}keyword sorter{\textquotedbl}, or {\textquotedbl}kwsorter{\textquotedbl}, and is stored in the \texttt{kwsorter} field of \texttt{MethodTable} objects. Every definition in the kwsorter function has the same arguments as some definition in the normal method table, except with a single \texttt{NamedTuple} argument prepended, which gives the names and values of passed keyword arguments. The kwsorter{\textquotesingle}s job is to move keyword arguments into their canonical positions based on name, plus evaluate and substitute any needed default value expressions. The result is a normal positional argument list, which is then passed to yet another compiler-generated function.



The easiest way to understand the process is to look at how a keyword argument method definition is lowered. The code:




\begin{minted}{julia}
function circle(center, radius; color = black, fill::Bool = true, options...)
    # draw
end
\end{minted}



actually produces \emph{three} method definitions. The first is a function that accepts all arguments (including keyword arguments) as positional arguments, and includes the code for the method body. It has an auto-generated name:




\begin{minted}{julia}
function #circle#1(color, fill::Bool, options, circle, center, radius)
    # draw
end
\end{minted}



The second method is an ordinary definition for the original \texttt{circle} function, which handles the case where no keyword arguments are passed:




\begin{minted}{julia}
function circle(center, radius)
    #circle#1(black, true, pairs(NamedTuple()), circle, center, radius)
end
\end{minted}



This simply dispatches to the first method, passing along default values. \texttt{pairs} is applied to the named tuple of rest arguments to provide key-value pair iteration. Note that if the method doesn{\textquotesingle}t accept rest keyword arguments then this argument is absent.



Finally there is the kwsorter definition:




\begin{lstlisting}
function (::Core.kwftype(typeof(circle)))(kws, circle, center, radius)
    if haskey(kws, :color)
        color = kws.color
    else
        color = black
    end
    # etc.

    # put remaining kwargs in `options`
    options = structdiff(kws, NamedTuple{(:color, :fill)})

    # if the method doesn't accept rest keywords, throw an error
    # unless `options` is empty

    #circle#1(color, fill, pairs(options), circle, center, radius)
end
\end{lstlisting}



The function \texttt{Core.kwftype(t)} creates the field \texttt{t.name.mt.kwsorter} (if it hasn{\textquotesingle}t been created yet), and returns the type of that function.



This design has the feature that call sites that don{\textquotesingle}t use keyword arguments require no special handling; everything works as if they were not part of the language at all. Call sites that do use keyword arguments are dispatched directly to the called function{\textquotesingle}s kwsorter. For example the call:




\begin{minted}{julia}
circle((0,0), 1.0, color = red; other...)
\end{minted}



is lowered to:




\begin{minted}{julia}
kwfunc(circle)(merge((color = red,), other), circle, (0,0), 1.0)
\end{minted}



\texttt{kwfunc} (also in\texttt{Core}) fetches the kwsorter for the called function. The keyword splatting operation (written as \texttt{other...}) calls the named tuple \texttt{merge} function. This function further unpacks each \emph{element} of \texttt{other}, expecting each one to contain two values (a symbol and a value). Naturally, a more efficient implementation is available if all splatted arguments are named tuples. Notice that the original \texttt{circle} function is passed through, to handle closures.



\hypertarget{1871194914272945034}{}


\subsection{Compiler efficiency issues}



Generating a new type for every function has potentially serious consequences for compiler resource use when combined with Julia{\textquotesingle}s {\textquotedbl}specialize on all arguments by default{\textquotedbl} design. Indeed, the initial implementation of this design suffered from much longer build and test times, higher memory use, and a system image nearly 2x larger than the baseline. In a naive implementation, the problem is bad enough to make the system nearly unusable. Several significant optimizations were needed to make the design practical.



The first issue is excessive specialization of functions for different values of function-valued arguments. Many functions simply {\textquotedbl}pass through{\textquotedbl} an argument to somewhere else, e.g. to another function or to a storage location. Such functions do not need to be specialized for every closure that might be passed in. Fortunately this case is easy to distinguish by simply considering whether a function \emph{calls} one of its arguments (i.e. the argument appears in {\textquotedbl}head position{\textquotedbl} somewhere). Performance-critical higher-order functions like \texttt{map} certainly call their argument function and so will still be specialized as expected. This optimization is implemented by recording which arguments are called during the \texttt{analyze-variables} pass in the front end. When \texttt{cache\_method} sees an argument in the \texttt{Function} type hierarchy passed to a slot declared as \texttt{Any} or \texttt{Function}, it behaves as if the \texttt{@nospecialize} annotation were applied. This heuristic seems to be extremely effective in practice.



The next issue concerns the structure of method cache hash tables. Empirical studies show that the vast majority of dynamically-dispatched calls involve one or two arguments. In turn, many of these cases can be resolved by considering only the first argument. (Aside: proponents of single dispatch would not be surprised by this at all. However, this argument means {\textquotedbl}multiple dispatch is easy to optimize in practice{\textquotedbl}, and that we should therefore use it, \emph{not} {\textquotedbl}we should use single dispatch{\textquotedbl}!) So the method cache uses the type of the first argument as its primary key. Note, however, that this corresponds to the \emph{second} element of the tuple type for a function call (the first element being the type of the function itself). Typically, type variation in head position is extremely low – indeed, the majority of functions belong to singleton types with no parameters. However, this is not the case for constructors, where a single method table holds constructors for every type. Therefore the \texttt{Type} method table is special-cased to use the \emph{first} tuple type element instead of the second.



The front end generates type declarations for all closures. Initially, this was implemented by generating normal type declarations. However, this produced an extremely large number of constructors, all of which were trivial (simply passing all arguments through to \hyperlink{13888762393600028594}{\texttt{new}}). Since methods are partially ordered, inserting all of these methods is O(n{\textasciicircum}2), plus there are just too many of them to keep around. This was optimized by generating \texttt{struct\_type} expressions directly (bypassing default constructor generation), and using \texttt{new} directly to create closure instances. Not the prettiest thing ever, but you do what you gotta do.



The next problem was the \texttt{@test} macro, which generated a 0-argument closure for each test case. This is not really necessary, since each test case is simply run once in place. Therefore, \texttt{@test} was modified to expand to a try-catch block that records the test result (true, false, or exception raised) and calls the test suite handler on it.



\hypertarget{3147327615803034601}{}


\section{Base.Cartesian}



The (non-exported) Cartesian module provides macros that facilitate writing multidimensional algorithms. Most often you can write such algorithms with \href{https://julialang.org/blog/2016/02/iteration}{straightforward techniques}; however, there are a few cases where \texttt{Base.Cartesian} is still useful or necessary.



\hypertarget{10560451956896766301}{}


\subsection{Principles of usage}



A simple example of usage is:




\begin{minted}{julia}
@nloops 3 i A begin
    s += @nref 3 A i
end
\end{minted}



which generates the following code:




\begin{minted}{julia}
for i_3 = axes(A, 3)
    for i_2 = axes(A, 2)
        for i_1 = axes(A, 1)
            s += A[i_1, i_2, i_3]
        end
    end
end
\end{minted}



In general, Cartesian allows you to write generic code that contains repetitive elements, like the nested loops in this example.  Other applications include repeated expressions (e.g., loop unwinding) or creating function calls with variable numbers of arguments without using the {\textquotedbl}splat{\textquotedbl} construct (\texttt{i...}).



\hypertarget{14286156567823237502}{}


\subsection{Basic syntax}



The (basic) syntax of \texttt{@nloops} is as follows:



\begin{itemize}
\item The first argument must be an integer (\emph{not} a variable) specifying the number of loops.


\item The second argument is the symbol-prefix used for the iterator variable. Here we used \texttt{i}, and variables \texttt{i\_1, i\_2, i\_3} were generated.


\item The third argument specifies the range for each iterator variable. If you use a variable (symbol) here, it{\textquotesingle}s taken as \texttt{axes(A, dim)}. More flexibly, you can use the anonymous-function expression syntax described below.


\item The last argument is the body of the loop. Here, that{\textquotesingle}s what appears between the \texttt{begin...end}.

\end{itemize}


There are some additional features of \texttt{@nloops} described in the \hyperlink{6401299442402093832}{reference section}.



\texttt{@nref} follows a similar pattern, generating \texttt{A[i\_1,i\_2,i\_3]} from \texttt{@nref 3 A i}. The general practice is to read from left to right, which is why \texttt{@nloops} is \texttt{@nloops 3 i A expr} (as in \texttt{for i\_2 = axes(A, 2)}, where \texttt{i\_2} is to the left and the range is to the right) whereas \texttt{@nref} is \texttt{@nref 3 A i} (as in \texttt{A[i\_1,i\_2,i\_3]}, where the array comes first).



If you{\textquotesingle}re developing code with Cartesian, you may find that debugging is easier when you examine the generated code, using \texttt{@macroexpand}:






\begin{minted}{jlcon}
julia> @macroexpand @nref 2 A i
:(A[i_1, i_2])
\end{minted}





\hypertarget{13048573846185312344}{}


\subsubsection{Supplying the number of expressions}



The first argument to both of these macros is the number of expressions, which must be an integer. When you{\textquotesingle}re writing a function that you intend to work in multiple dimensions, this may not be something you want to hard-code. The recommended approach is to use a \texttt{@generated function}.  Here{\textquotesingle}s an example:




\begin{minted}{julia}
@generated function mysum(A::Array{T,N}) where {T,N}
    quote
        s = zero(T)
        @nloops $N i A begin
            s += @nref $N A i
        end
        s
    end
end
\end{minted}



Naturally, you can also prepare expressions or perform calculations before the \texttt{quote} block.



\hypertarget{10509900364879728464}{}


\subsubsection{Anonymous-function expressions as macro arguments}



Perhaps the single most powerful feature in \texttt{Cartesian} is the ability to supply anonymous-function expressions that get evaluated at parsing time.  Let{\textquotesingle}s consider a simple example:




\begin{minted}{julia}
@nexprs 2 j->(i_j = 1)
\end{minted}



\texttt{@nexprs} generates \texttt{n} expressions that follow a pattern. This code would generate the following statements:




\begin{minted}{julia}
i_1 = 1
i_2 = 1
\end{minted}



In each generated statement, an {\textquotedbl}isolated{\textquotedbl} \texttt{j} (the variable of the anonymous function) gets replaced by values in the range \texttt{1:2}. Generally speaking, Cartesian employs a LaTeX-like syntax.  This allows you to do math on the index \texttt{j}.  Here{\textquotesingle}s an example computing the strides of an array:




\begin{minted}{julia}
s_1 = 1
@nexprs 3 j->(s_{j+1} = s_j * size(A, j))
\end{minted}



would generate expressions




\begin{minted}{julia}
s_1 = 1
s_2 = s_1 * size(A, 1)
s_3 = s_2 * size(A, 2)
s_4 = s_3 * size(A, 3)
\end{minted}



Anonymous-function expressions have many uses in practice.



\hypertarget{908996570436533510}{}


\paragraph{Macro reference}


\hypertarget{4938945836201444124}{}
\hyperlink{4938945836201444124}{\texttt{Base.Cartesian.@nloops}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nloops N itersym rangeexpr bodyexpr
@nloops N itersym rangeexpr preexpr bodyexpr
@nloops N itersym rangeexpr preexpr postexpr bodyexpr
\end{minted}

Generate \texttt{N} nested loops, using \texttt{itersym} as the prefix for the iteration variables. \texttt{rangeexpr} may be an anonymous-function expression, or a simple symbol \texttt{var} in which case the range is \texttt{axes(var, d)} for dimension \texttt{d}.

Optionally, you can provide {\textquotedbl}pre{\textquotedbl} and {\textquotedbl}post{\textquotedbl} expressions. These get executed first and last, respectively, in the body of each loop. For example:


\begin{lstlisting}
@nloops 2 i A d -> j_d = min(i_d, 5) begin
    s += @nref 2 A j
end
\end{lstlisting}

would generate:


\begin{lstlisting}
for i_2 = axes(A, 2)
    j_2 = min(i_2, 5)
    for i_1 = axes(A, 1)
        j_1 = min(i_1, 5)
        s += A[j_1, j_2]
    end
end
\end{lstlisting}

If you want just a post-expression, supply \hyperlink{9331422207248206047}{\texttt{nothing}} for the pre-expression. Using parentheses and semicolons, you can supply multi-statement expressions.



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L9-L37}{\texttt{source}}


\end{adjustwidth}
\hypertarget{5318613607184308860}{}
\hyperlink{5318613607184308860}{\texttt{Base.Cartesian.@nref}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nref N A indexexpr
\end{minted}

Generate expressions like \texttt{A[i\_1, i\_2, ...]}. \texttt{indexexpr} can either be an iteration-symbol prefix, or an anonymous-function expression.

\textbf{Examples}


\begin{minted}{jlcon}
julia> @macroexpand Base.Cartesian.@nref 3 A i
:(A[i_1, i_2, i_3])
\end{minted}



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L72-L83}{\texttt{source}}


\end{adjustwidth}
\hypertarget{5592996802344748158}{}
\hyperlink{5592996802344748158}{\texttt{Base.Cartesian.@nextract}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nextract N esym isym
\end{minted}

Generate \texttt{N} variables \texttt{esym\_1}, \texttt{esym\_2}, ..., \texttt{esym\_N} to extract values from \texttt{isym}. \texttt{isym} can be either a \texttt{Symbol} or anonymous-function expression.

\texttt{@nextract 2 x y} would generate


\begin{lstlisting}
x_1 = y[1]
x_2 = y[2]
\end{lstlisting}

while \texttt{@nextract 3 x d->y[2d-1]} yields


\begin{lstlisting}
x_1 = y[1]
x_2 = y[3]
x_3 = y[5]
\end{lstlisting}



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L132-L149}{\texttt{source}}


\end{adjustwidth}
\hypertarget{17386123129446980507}{}
\hyperlink{17386123129446980507}{\texttt{Base.Cartesian.@nexprs}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nexprs N expr
\end{minted}

Generate \texttt{N} expressions. \texttt{expr} should be an anonymous-function expression.

\textbf{Examples}


\begin{minted}{jlcon}
julia> @macroexpand Base.Cartesian.@nexprs 4 i -> y[i] = A[i+j]
quote
    y[1] = A[1 + j]
    y[2] = A[2 + j]
    y[3] = A[3 + j]
    y[4] = A[4 + j]
end
\end{minted}



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L111-L126}{\texttt{source}}


\end{adjustwidth}
\hypertarget{16431416314224139891}{}
\hyperlink{16431416314224139891}{\texttt{Base.Cartesian.@ncall}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@ncall N f sym...
\end{minted}

Generate a function call expression. \texttt{sym} represents any number of function arguments, the last of which may be an anonymous-function expression and is expanded into \texttt{N} arguments.

For example, \texttt{@ncall 3 func a} generates


\begin{lstlisting}
func(a_1, a_2, a_3)
\end{lstlisting}

while \texttt{@ncall 2 func a b i->c[i]} yields


\begin{lstlisting}
func(a, b, c[1], c[2])
\end{lstlisting}



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L89-L103}{\texttt{source}}


\end{adjustwidth}
\hypertarget{4425932542618492714}{}
\hyperlink{4425932542618492714}{\texttt{Base.Cartesian.@ntuple}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@ntuple N expr
\end{minted}

Generates an \texttt{N}-tuple. \texttt{@ntuple 2 i} would generate \texttt{(i\_1, i\_2)}, and \texttt{@ntuple 2 k->k+1} would generate \texttt{(2,3)}.



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L193-L198}{\texttt{source}}


\end{adjustwidth}
\hypertarget{5463798602076286002}{}
\hyperlink{5463798602076286002}{\texttt{Base.Cartesian.@nall}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nall N expr
\end{minted}

Check whether all of the expressions generated by the anonymous-function expression \texttt{expr} evaluate to \texttt{true}.

\texttt{@nall 3 d->(i\_d > 1)} would generate the expression \texttt{(i\_1 > 1 \&\& i\_2 > 1 \&\& i\_3 > 1)}. This can be convenient for bounds-checking.



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L160-L168}{\texttt{source}}


\end{adjustwidth}
\hypertarget{11114957141394185901}{}
\hyperlink{11114957141394185901}{\texttt{Base.Cartesian.@nany}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nany N expr
\end{minted}

Check whether any of the expressions generated by the anonymous-function expression \texttt{expr} evaluate to \texttt{true}.

\texttt{@nany 3 d->(i\_d > 1)} would generate the expression \texttt{(i\_1 > 1 || i\_2 > 1 || i\_3 > 1)}.



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L177-L184}{\texttt{source}}


\end{adjustwidth}
\hypertarget{2428714678347040919}{}
\hyperlink{2428714678347040919}{\texttt{Base.Cartesian.@nif}}  -- {Macro.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
@nif N conditionexpr expr
@nif N conditionexpr expr elseexpr
\end{minted}

Generates a sequence of \texttt{if ... elseif ... else ... end} statements. For example:


\begin{lstlisting}
@nif 3 d->(i_d >= size(A,d)) d->(error("Dimension ", d, " too big")) d->println("All OK")
\end{lstlisting}

would generate:


\begin{lstlisting}
if i_1 > size(A, 1)
    error("Dimension ", 1, " too big")
elseif i_2 > size(A, 2)
    error("Dimension ", 2, " too big")
else
    println("All OK")
end
\end{lstlisting}



\href{https://github.com/JuliaLang/julia/blob/2d472c633d66e7697dda5aff75d2367b823048b8/base/cartesian.jl#L204-L221}{\texttt{source}}


\end{adjustwidth}

\hypertarget{8046590092133914049}{}


\section{Talking to the compiler (the \texttt{:meta} mechanism)}



In some circumstances, one might wish to provide hints or instructions that a given block of code has special properties: you might always want to inline it, or you might want to turn on special compiler optimization passes.  Starting with version 0.4, Julia has a convention that these instructions can be placed inside a \texttt{:meta} expression, which is typically (but not necessarily) the first expression in the body of a function.



\texttt{:meta} expressions are created with macros. As an example, consider the implementation of the \texttt{@inline} macro:




\begin{minted}{julia}
macro inline(ex)
    esc(isa(ex, Expr) ? pushmeta!(ex, :inline) : ex)
end
\end{minted}



Here, \texttt{ex} is expected to be an expression defining a function. A statement like this:




\begin{minted}{julia}
@inline function myfunction(x)
    x*(x+3)
end
\end{minted}



gets turned into an expression like this:




\begin{minted}{julia}
quote
    function myfunction(x)
        Expr(:meta, :inline)
        x*(x+3)
    end
end
\end{minted}



\texttt{Base.pushmeta!(ex, :symbol, args...)} appends \texttt{:symbol} to the end of the \texttt{:meta} expression, creating a new \texttt{:meta} expression if necessary. If \texttt{args} is specified, a nested expression containing \texttt{:symbol} and these arguments is appended instead, which can be used to specify additional information.



To use the metadata, you have to parse these \texttt{:meta} expressions. If your implementation can be performed within Julia, \texttt{Base.popmeta!} is very handy: \texttt{Base.popmeta!(body, :symbol)} will scan a function \emph{body} expression (one without the function signature) for the first \texttt{:meta} expression containing \texttt{:symbol}, extract any arguments, and return a tuple \texttt{(found::Bool, args::Array\{Any\})}. If the metadata did not have any arguments, or \texttt{:symbol} was not found, the \texttt{args} array will be empty.



Not yet provided is a convenient infrastructure for parsing \texttt{:meta} expressions from C++.



\hypertarget{16299844357766264666}{}


\section{SubArrays}



Julia{\textquotesingle}s \texttt{SubArray} type is a container encoding a {\textquotedbl}view{\textquotedbl} of a parent \hyperlink{6514416309183787338}{\texttt{AbstractArray}}.  This page documents some of the design principles and implementation of \texttt{SubArray}s.



One of the major design goals is to ensure high performance for views of both \hyperlink{1761039776681330940}{\texttt{IndexLinear}} and \hyperlink{4052302263500310575}{\texttt{IndexCartesian}} arrays. Furthermore, views of \texttt{IndexLinear} arrays should themselves be \texttt{IndexLinear} to the extent that it is possible.



\hypertarget{5581126844733696350}{}


\subsection{Index replacement}



Consider making 2d slices of a 3d array:






\begin{minted}{jlcon}
julia> A = rand(2,3,4);

julia> S1 = view(A, :, 1, 2:3)
2×2 view(::Array{Float64, 3}, :, 1, 2:3) with eltype Float64:
 0.839622  0.711389
 0.967143  0.103929

julia> S2 = view(A, 1, :, 2:3)
3×2 view(::Array{Float64, 3}, 1, :, 2:3) with eltype Float64:
 0.839622  0.711389
 0.789764  0.806704
 0.566704  0.962715
\end{minted}





\texttt{view} drops {\textquotedbl}singleton{\textquotedbl} dimensions (ones that are specified by an \texttt{Int}), so both \texttt{S1} and \texttt{S2} are two-dimensional \texttt{SubArray}s. Consequently, the natural way to index these is with \texttt{S1[i,j]}. To extract the value from the parent array \texttt{A}, the natural approach is to replace \texttt{S1[i,j]} with \texttt{A[i,1,(2:3)[j]]} and \texttt{S2[i,j]} with \texttt{A[1,i,(2:3)[j]]}.



The key feature of the design of SubArrays is that this index replacement can be performed without any runtime overhead.



\hypertarget{2778530023624514912}{}


\subsection{SubArray design}



\hypertarget{13587272900516328040}{}


\subsubsection{Type parameters and fields}



The strategy adopted is first and foremost expressed in the definition of the type:




\begin{minted}{julia}
struct SubArray{T,N,P,I,L} <: AbstractArray{T,N}
    parent::P
    indices::I
    offset1::Int       # for linear indexing and pointer, only valid when L==true
    stride1::Int       # used only for linear indexing
    ...
end
\end{minted}



\texttt{SubArray} has 5 type parameters.  The first two are the standard element type and dimensionality.  The next is the type of the parent \texttt{AbstractArray}.  The most heavily-used is the fourth parameter, a \texttt{Tuple} of the types of the indices for each dimension. The final one, \texttt{L}, is only provided as a convenience for dispatch; it{\textquotesingle}s a boolean that represents whether the index types support fast linear indexing. More on that later.



If in our example above \texttt{A} is a \texttt{Array\{Float64, 3\}}, our \texttt{S1} case above would be a \texttt{SubArray\{Float64,2,Array\{Float64,3\},Tuple\{Base.Slice\{Base.OneTo\{Int64\}\},Int64,UnitRange\{Int64\}\},false\}}. Note in particular the tuple parameter, which stores the types of the indices used to create \texttt{S1}. Likewise,




\begin{minted}{jlcon}
julia> S1.indices
(Base.Slice(Base.OneTo(2)), 1, 2:3)
\end{minted}



Storing these values allows index replacement, and having the types encoded as parameters allows one to dispatch to efficient algorithms.



\hypertarget{7039991064739471489}{}


\subsubsection{Index translation}



Performing index translation requires that you do different things for different concrete \texttt{SubArray} types.  For example, for \texttt{S1}, one needs to apply the \texttt{i,j} indices to the first and third dimensions of the parent array, whereas for \texttt{S2} one needs to apply them to the second and third.  The simplest approach to indexing would be to do the type-analysis at runtime:




\begin{minted}{julia}
parentindices = Vector{Any}()
for thisindex in S.indices
    ...
    if isa(thisindex, Int)
        # Don't consume one of the input indices
        push!(parentindices, thisindex)
    elseif isa(thisindex, AbstractVector)
        # Consume an input index
        push!(parentindices, thisindex[inputindex[j]])
        j += 1
    elseif isa(thisindex, AbstractMatrix)
        # Consume two input indices
        push!(parentindices, thisindex[inputindex[j], inputindex[j+1]])
        j += 2
    elseif ...
end
S.parent[parentindices...]
\end{minted}



Unfortunately, this would be disastrous in terms of performance: each element access would allocate memory, and involves the running of a lot of poorly-typed code.



The better approach is to dispatch to specific methods to handle each type of stored index. That{\textquotesingle}s what \texttt{reindex} does: it dispatches on the type of the first stored index and consumes the appropriate number of input indices, and then it recurses on the remaining indices. In the case of \texttt{S1}, this expands to




\begin{minted}{julia}
Base.reindex(S1, S1.indices, (i, j)) == (i, S1.indices[2], S1.indices[3][j])
\end{minted}



for any pair of indices \texttt{(i,j)} (except \hyperlink{4571802376991525093}{\texttt{CartesianIndex}}s and arrays thereof, see below).



This is the core of a \texttt{SubArray}; indexing methods depend upon \texttt{reindex} to do this index translation. Sometimes, though, we can avoid the indirection and make it even faster.



\hypertarget{18413909182716267462}{}


\subsubsection{Linear indexing}



Linear indexing can be implemented efficiently when the entire array has a single stride that separates successive elements, starting from some offset. This means that we can pre-compute these values and represent linear indexing simply as an addition and multiplication, avoiding the indirection of \texttt{reindex} and (more importantly) the slow computation of the cartesian coordinates entirely.



For \texttt{SubArray} types, the availability of efficient linear indexing is based purely on the types of the indices, and does not depend on values like the size of the parent array. You can ask whether a given set of indices supports fast linear indexing with the internal \texttt{Base.viewindexing} function:




\begin{minted}{jlcon}
julia> Base.viewindexing(S1.indices)
IndexCartesian()

julia> Base.viewindexing(S2.indices)
IndexLinear()
\end{minted}



This is computed during construction of the \texttt{SubArray} and stored in the \texttt{L} type parameter as a boolean that encodes fast linear indexing support. While not strictly necessary, it means that we can define dispatch directly on \texttt{SubArray\{T,N,A,I,true\}} without any intermediaries.



Since this computation doesn{\textquotesingle}t depend on runtime values, it can miss some cases in which the stride happens to be uniform:




\begin{minted}{jlcon}
julia> A = reshape(1:4*2, 4, 2)
4×2 reshape(::UnitRange{Int64}, 4, 2) with eltype Int64:
 1  5
 2  6
 3  7
 4  8

julia> diff(A[2:2:4,:][:])
3-element Vector{Int64}:
 2
 2
 2
\end{minted}



A view constructed as \texttt{view(A, 2:2:4, :)} happens to have uniform stride, and therefore linear indexing indeed could be performed efficiently.  However, success in this case depends on the size of the array: if the first dimension instead were odd,




\begin{minted}{jlcon}
julia> A = reshape(1:5*2, 5, 2)
5×2 reshape(::UnitRange{Int64}, 5, 2) with eltype Int64:
 1   6
 2   7
 3   8
 4   9
 5  10

julia> diff(A[2:2:4,:][:])
3-element Vector{Int64}:
 2
 3
 2
\end{minted}



then \texttt{A[2:2:4,:]} does not have uniform stride, so we cannot guarantee efficient linear indexing.  Since we have to base this decision based purely on types encoded in the parameters of the \texttt{SubArray}, \texttt{S = view(A, 2:2:4, :)} cannot implement efficient linear indexing.



\hypertarget{6217885754113158897}{}


\subsubsection{A few details}



\begin{itemize}
\item Note that the \texttt{Base.reindex} function is agnostic to the types of the input indices; it simply determines how and where the stored indices should be reindexed. It not only supports integer indices, but it supports non-scalar indexing, too. This means that views of views don{\textquotesingle}t need two levels of indirection; they can simply re-compute the indices into the original parent array!


\item Hopefully by now it{\textquotesingle}s fairly clear that supporting slices means that the dimensionality, given by the parameter \texttt{N}, is not necessarily equal to the dimensionality of the parent array or the length of the \texttt{indices} tuple.  Neither do user-supplied indices necessarily line up with entries in the \texttt{indices} tuple (e.g., the second user-supplied index might correspond to the third dimension of the parent array, and the third element in the \texttt{indices} tuple).

What might be less obvious is that the dimensionality of the stored parent array must be equal to the number of effective indices in the \texttt{indices} tuple. Some examples:


\begin{minted}{julia}
A = reshape(1:35, 5, 7) # A 2d parent Array
S = view(A, 2:7)         # A 1d view created by linear indexing
S = view(A, :, :, 1:1)   # Appending extra indices is supported
\end{minted}

Naively, you{\textquotesingle}d think you could just set \texttt{S.parent = A} and \texttt{S.indices = (:,:,1:1)}, but supporting this dramatically complicates the reindexing process, especially for views of views. Not only do you need to dispatch on the types of the stored indices, but you need to examine whether a given index is the final one and {\textquotedbl}merge{\textquotedbl} any remaining stored indices together. This is not an easy task, and even worse: it{\textquotesingle}s slow since it implicitly depends upon linear indexing.

Fortunately, this is precisely the computation that \texttt{ReshapedArray} performs, and it does so linearly if possible. Consequently, \texttt{view} ensures that the parent array is the appropriate dimensionality for the given indices by reshaping it if needed. The inner \texttt{SubArray} constructor ensures that this invariant is satisfied.


\item \hyperlink{4571802376991525093}{\texttt{CartesianIndex}} and arrays thereof throw a nasty wrench into the \texttt{reindex} scheme. Recall that \texttt{reindex} simply dispatches on the type of the stored indices in order to determine how many passed indices should be used and where they should go. But with \texttt{CartesianIndex}, there{\textquotesingle}s no longer a one-to-one correspondence between the number of passed arguments and the number of dimensions that they index into. If we return to the above example of \texttt{Base.reindex(S1, S1.indices, (i, j))}, you can see that the expansion is incorrect for \texttt{i, j = CartesianIndex(), CartesianIndex(2,1)}. It should \emph{skip} the \texttt{CartesianIndex()} entirely and return:


\begin{minted}{julia}
(CartesianIndex(2,1)[1], S1.indices[2], S1.indices[3][CartesianIndex(2,1)[2]])
\end{minted}

Instead, though, we get:


\begin{minted}{julia}
(CartesianIndex(), S1.indices[2], S1.indices[3][CartesianIndex(2,1)])
\end{minted}

Doing this correctly would require \emph{combined} dispatch on both the stored and passed indices across all combinations of dimensionalities in an intractable manner. As such, \texttt{reindex} must never be called with \texttt{CartesianIndex} indices. Fortunately, the scalar case is easily handled by first flattening the \texttt{CartesianIndex} arguments to plain integers. Arrays of \texttt{CartesianIndex}, however, cannot be split apart into orthogonal pieces so easily. Before attempting to use \texttt{reindex}, \texttt{view} must ensure that there are no arrays of \texttt{CartesianIndex} in the argument list. If there are, it can simply {\textquotedbl}punt{\textquotedbl} by avoiding the \texttt{reindex} calculation entirely, constructing a nested \texttt{SubArray} with two levels of indirection instead.

\end{itemize}


\hypertarget{9194487374122863540}{}


\section{isbits Union Optimizations}



In Julia, the \texttt{Array} type holds both {\textquotedbl}bits{\textquotedbl} values as well as heap-allocated {\textquotedbl}boxed{\textquotedbl} values. The distinction is whether the value itself is stored inline (in the direct allocated memory of the array), or if the memory of the array is simply a collection of pointers to objects allocated elsewhere. In terms of performance, accessing values inline is clearly an advantage over having to follow a pointer to the actual value. The definition of {\textquotedbl}isbits{\textquotedbl} generally means any Julia type with a fixed, determinate size, meaning no {\textquotedbl}pointer{\textquotedbl} fields, see \texttt{?isbitstype}.



Julia also supports Union types, quite literally the union of a set of types. Custom Union type definitions can be extremely handy for applications wishing to {\textquotedbl}cut across{\textquotedbl} the nominal type system (i.e. explicit subtype relationships) and define methods or functionality on these, otherwise unrelated, set of types. A compiler challenge, however, is in determining how to treat these Union types. The naive approach (and indeed, what Julia itself did pre-0.7), is to simply make a {\textquotedbl}box{\textquotedbl} and then a pointer in the box to the actual value, similar to the previously mentioned {\textquotedbl}boxed{\textquotedbl} values. This is unfortunate, however, because of the number of small, primitive {\textquotedbl}bits{\textquotedbl} types (think \texttt{UInt8}, \texttt{Int32}, \texttt{Float64}, etc.) that would easily fit themselves inline in this {\textquotedbl}box{\textquotedbl} without needing any indirection for value access. There are two main ways Julia can take advantage of this optimization as of 0.7: isbits Union fields in types, and isbits Union Arrays.



\hypertarget{4239563333738868441}{}


\subsection{isbits Union Structs}



Julia now includes an optimization wherein {\textquotedbl}isbits Union{\textquotedbl} fields in types (\texttt{mutable struct}, \texttt{struct}, etc.) will be stored inline. This is accomplished by determining the {\textquotedbl}inline size{\textquotedbl} of the Union type (e.g. \texttt{Union\{UInt8, Int16\}} will have a size of two bytes, which represents the size needed of the largest Union type \texttt{Int16}), and in addition, allocating an extra {\textquotedbl}type tag byte{\textquotedbl} (\texttt{UInt8}), whose value signals the type of the actual value stored inline of the {\textquotedbl}Union bytes{\textquotedbl}. The type tag byte value is the index of the actual value{\textquotesingle}s type in the Union type{\textquotesingle}s order of types. For example, a type tag value of \texttt{0x02} for a field with type \texttt{Union\{Nothing, UInt8, Int16\}} would indicate that an \texttt{Int16} value is stored in the 16 bits of the field in the structure{\textquotesingle}s memory; a \texttt{0x01} value would indicate that a \texttt{UInt8} value was stored in the first 8 bits of the 16 bits of the field{\textquotesingle}s memory. Lastly, a value of \texttt{0x00} signals that the \texttt{nothing} value will be returned for this field, even though, as a singleton type with a single type instance, it technically has a size of 0. The type tag byte for a type{\textquotesingle}s Union field is stored directly after the field{\textquotesingle}s computed Union memory.



\hypertarget{3119783438553475290}{}


\subsection{isbits Union Arrays}



Julia can now also store {\textquotedbl}isbits Union{\textquotedbl} values inline in an Array, as opposed to requiring an indirection box. The optimization is accomplished by storing an extra {\textquotedbl}type tag array{\textquotedbl} of bytes, one byte per array element, alongside the bytes of the actual array data. This type tag array serves the same function as the type field case: its value signals the type of the actual stored Union value in the array. In terms of layout, a Julia Array can include extra {\textquotedbl}buffer{\textquotedbl} space before and after its actual data values, which are tracked in the \texttt{a->offset} and \texttt{a->maxsize} fields of the \texttt{jl\_array\_t*} type. The {\textquotedbl}type tag array{\textquotedbl} is treated exactly as another \texttt{jl\_array\_t*}, but which shares the same \texttt{a->offset}, \texttt{a->maxsize}, and \texttt{a->len} fields. So the formula to access an isbits Union Array{\textquotesingle}s type tag bytes is \texttt{a->data + (a->maxsize - a->offset) * a->elsize + a->offset}; i.e. the Array{\textquotesingle}s \texttt{a->data} pointer is already shifted by \texttt{a->offset}, so correcting for that, we follow the data all the way to the max of what it can hold \texttt{a->maxsize}, then adjust by \texttt{a->offset} more bytes to account for any present {\textquotedbl}front buffering{\textquotedbl} the array might be doing. This layout in particular allows for very efficient resizing operations as the type tag data only ever has to move when the actual array{\textquotesingle}s data has to move.



\hypertarget{6450179845418792741}{}


\section{System Image Building}



\hypertarget{2889722918811470983}{}


\subsection{Building the Julia system image}



Julia ships with a preparsed system image containing the contents of the \texttt{Base} module, named \texttt{sys.ji}.  This file is also precompiled into a shared library called \texttt{sys.\{so,dll,dylib\}} on as many platforms as possible, so as to give vastly improved startup times.  On systems that do not ship with a precompiled system image file, one can be generated from the source files shipped in Julia{\textquotesingle}s \texttt{DATAROOTDIR/julia/base} folder.



This operation is useful for multiple reasons.  A user may:



\begin{itemize}
\item Build a precompiled shared library system image on a platform that did not ship with one, thereby improving startup times.


\item Modify \texttt{Base}, rebuild the system image and use the new \texttt{Base} next time Julia is started.


\item Include a \texttt{userimg.jl} file that includes packages into the system image, thereby creating a system image that has packages embedded into the startup environment.

\end{itemize}


The \href{https://github.com/JuliaLang/PackageCompiler.jl}{\texttt{PackageCompiler.jl} package} contains convenient wrapper functions to automate this process.



\hypertarget{6216163822526201376}{}


\subsection{System image optimized for multiple microarchitectures}



The system image can be compiled simultaneously for multiple CPU microarchitectures under the same instruction set architecture (ISA). Multiple versions of the same function may be created with minimum dispatch point inserted into shared functions in order to take advantage of different ISA extensions or other microarchitecture features. The version that offers the best performance will be selected automatically at runtime based on available CPU features.



\hypertarget{7911922725601251657}{}


\subsubsection{Specifying multiple system image targets}



A multi-microarchitecture system image can be enabled by passing multiple targets during system image compilation. This can be done either with the \texttt{JULIA\_CPU\_TARGET} make option or with the \texttt{-C} command line option when running the compilation command manually. Multiple targets are separated by \texttt{;} in the option string. The syntax for each target is a CPU name followed by multiple features separated by \texttt{,}. All features supported by LLVM are supported and a feature can be disabled with a \texttt{-} prefix. (\texttt{+} prefix is also allowed and ignored to be consistent with LLVM syntax). Additionally, a few special features are supported to control the function cloning behavior.



\begin{itemize}
\item[1. ] \texttt{clone\_all}

By default, only functions that are the most likely to benefit from  the microarchitecture features will be cloned.  When \texttt{clone\_all} is specified for a target, however,  \textbf{all} functions in the system image will be cloned for the target.  The negative form \texttt{-clone\_all} can be used to prevent the built-in  heuristic from cloning all functions.


\item[2. ] \texttt{base(<n>)}

Where \texttt{<n>} is a placeholder for a non-negative number (e.g. \texttt{base(0)}, \texttt{base(1)}).  By default, a partially cloned (i.e. not \texttt{clone\_all}) target will use functions  from the default target (first one specified) if a function is not cloned.  This behavior can be changed by specifying a different base with the \texttt{base(<n>)} option.  The \texttt{n}th target (0-based) will be used as the base target instead of the default (\texttt{0}th) one.  The base target has to be either \texttt{0} or another \texttt{clone\_all} target.  Specifying a non-\texttt{clone\_all} target as the base target will cause an error.


\item[3. ] \texttt{opt\_size}

This causes the function for the target to be optimized for size when there isn{\textquotesingle}t a significant  runtime performance impact. This corresponds to \texttt{-Os} GCC and Clang option.


\item[4. ] \texttt{min\_size}

This causes the function for the target to be optimized for size that might have  a significant runtime performance impact. This corresponds to \texttt{-Oz} Clang option.

\end{itemize}


As an example, at the time of this writing, the following string is used in the creation of the official \texttt{x86\_64} Julia binaries downloadable from julialang.org:




\begin{lstlisting}
generic;sandybridge,-xsaveopt,clone_all;haswell,-rdrnd,base(1)
\end{lstlisting}



This creates a system image with three separate targets; one for a generic \texttt{x86\_64} processor, one with a \texttt{sandybridge} ISA (explicitly excluding \texttt{xsaveopt}) that explicitly clones all functions, and one targeting the \texttt{haswell} ISA, based off of the \texttt{sandybridge} sysimg version, and also excluding \texttt{rdrnd}.  When a Julia implementation loads the generated sysimg, it will check the host processor for matching CPU capability flags, enabling the highest ISA level possible.  Note that the base level (\texttt{generic}) requires the \texttt{cx16} instruction, which is disabled in some virtualization software and must be enabled for the \texttt{generic} target to be loaded.  Alternatively, a sysimg could be generated with the target \texttt{generic,-cx16} for greater compatibility, however note that this may cause performance and stability problems in some code.



\hypertarget{6161913736473072394}{}


\subsubsection{Implementation overview}



This is a brief overview of different part involved in the implementation. See code comments for each components for more implementation details.



\begin{itemize}
\item[1. ] System image compilation

The parsing and cloning decision are done in \texttt{src/processor*}.  We currently support cloning of function based on the present of loops, simd instructions,  or other math operations (e.g. fastmath, fma, muladd).  This information is passed on to \texttt{src/llvm-multiversioning.cpp} which does the actual cloning.  In addition to doing the cloning and insert dispatch slots  (see comments in \texttt{MultiVersioning::runOnModule} for how this is done),  the pass also generates metadata so that the runtime can load and initialize the  system image correctly.  A detail description of the metadata is available in \texttt{src/processor.h}.


\item[2. ] System image loading

The loading and initialization of the system image is done in \texttt{src/processor*} by  parsing the metadata saved during system image generation.  Host feature detection and selection decision are done in \texttt{src/processor\_*.cpp}  depending on the ISA. The target selection will prefer exact CPU name match,  larger vector register size, and larger number of features.  An overview of this process is in \texttt{src/processor.cpp}.

\end{itemize}


\hypertarget{5125896206531208336}{}


\section{Working with LLVM}



This is not a replacement for the LLVM documentation, but a collection of tips for working on LLVM for Julia.



\hypertarget{8956364453365125987}{}


\subsection{Overview of Julia to LLVM Interface}



Julia dynamically links against LLVM by default. Build with \texttt{USE\_LLVM\_SHLIB=0} to link statically.



The code for lowering Julia AST to LLVM IR or interpreting it directly is in directory \texttt{src/}.




\begin{table}[h]

\begin{tabulary}{\linewidth}{|L|L|}
\hline
File & Description \\
\hline
\texttt{builtins.c} & Builtin functions \\
\hline
\texttt{ccall.cpp} & Lowering \hyperlink{14245046751182637566}{\texttt{ccall}} \\
\hline
\texttt{cgutils.cpp} & Lowering utilities, notably for array and tuple accesses \\
\hline
\texttt{codegen.cpp} & Top-level of code generation, pass list, lowering builtins \\
\hline
\texttt{debuginfo.cpp} & Tracks debug information for JIT code \\
\hline
\texttt{disasm.cpp} & Handles native object file and JIT code diassembly \\
\hline
\texttt{gf.c} & Generic functions \\
\hline
\texttt{intrinsics.cpp} & Lowering intrinsics \\
\hline
\texttt{llvm-simdloop.cpp} & Custom LLVM pass for \hyperlink{8155428559748374852}{\texttt{@simd}} \\
\hline
\texttt{sys.c} & I/O and operating system utility functions \\
\hline
\end{tabulary}

\end{table}



Some of the \texttt{.cpp} files form a group that compile to a single object.



The difference between an intrinsic and a builtin is that a builtin is a first class function that can be used like any other Julia function.  An intrinsic can operate only on unboxed data, and therefore its arguments must be statically typed.



\hypertarget{8366324567299313031}{}


\subsubsection{Alias Analysis}



Julia currently uses LLVM{\textquotesingle}s \href{https://llvm.org/docs/LangRef.html\#tbaa-metadata}{Type Based Alias Analysis}. To find the comments that document the inclusion relationships, look for \texttt{static MDNode*} in \texttt{src/codegen.cpp}.



The \texttt{-O} option enables LLVM{\textquotesingle}s \href{https://llvm.org/docs/AliasAnalysis.html\#the-basic-aa-pass}{Basic Alias Analysis}.



\hypertarget{13120458447023898074}{}


\subsection{Building Julia with a different version of LLVM}



The default version of LLVM is specified in \texttt{deps/Versions.make}. You can override it by creating a file called \texttt{Make.user} in the top-level directory and adding a line to it such as:




\begin{lstlisting}
LLVM_VER = 12.0.1
\end{lstlisting}



Besides the LLVM release numerals, you can also use \texttt{DEPS\_GIT = llvm} in combination with \texttt{USE\_BINARYBUILDER\_LLVM = 0} to build against the latest development version of LLVM.



You can also specify to build a debug version of LLVM, by setting either \texttt{LLVM\_DEBUG = 1} or \texttt{LLVM\_DEBUG = Release} in your \texttt{Make.user} file. The former will be a fully unoptimized build of LLVM and the latter will produce an optimized build of LLVM. Depending on your needs the latter will suffice and it quite a bit faster. If you use \texttt{LLVM\_DEBUG = Release} you will also want to set \texttt{LLVM\_ASSERTIONS = 1} to enable diagnostics for different passes. Only \texttt{LLVM\_DEBUG = 1} implies that option by default.



\hypertarget{3429896864083237221}{}


\subsection{Passing options to LLVM}



You can pass options to LLVM via the environment variable \texttt{JULIA\_LLVM\_ARGS}. Here are example settings using \texttt{bash} syntax:



\begin{itemize}
\item \texttt{export JULIA\_LLVM\_ARGS=-print-after-all} dumps IR after each pass.


\item \texttt{export JULIA\_LLVM\_ARGS=-debug-only=loop-vectorize} dumps LLVM \texttt{DEBUG(...)} diagnostics for loop vectorizer. If you get warnings about {\textquotedbl}Unknown command line argument{\textquotedbl}, rebuild LLVM with \texttt{LLVM\_ASSERTIONS = 1}.

\end{itemize}


\hypertarget{7270369944062365114}{}


\subsection{Debugging LLVM transformations in isolation}



On occasion, it can be useful to debug LLVM{\textquotesingle}s transformations in isolation from the rest of the Julia system, e.g. because reproducing the issue inside \texttt{julia} would take too long, or because one wants to take advantage of LLVM{\textquotesingle}s tooling (e.g. bugpoint). To get unoptimized IR for the entire system image, pass the \texttt{--output-unopt-bc unopt.bc} option to the system image build process, which will output the unoptimized IR to an \texttt{unopt.bc} file. This file can then be passed to LLVM tools as usual. \texttt{libjulia} can function as an LLVM pass plugin and can be loaded into LLVM tools, to make julia-specific passes available in this environment. In addition, it exposes the \texttt{-julia} meta-pass, which runs the entire Julia pass-pipeline over the IR. As an example, to generate a system image, one could do:




\begin{lstlisting}
opt -enable-new-pm=0 -load libjulia-internal.so -julia -o opt.bc unopt.bc
llc -o sys.o opt.bc
cc -shared -o sys.so sys.o
\end{lstlisting}



This system image can then be loaded by \texttt{julia} as usual.



It is also possible to dump an LLVM IR module for just one Julia function, using:




\begin{minted}{julia}
fun, T = +, Tuple{Int,Int} # Substitute your function of interest here
optimize = false
open("plus.ll", "w") do file
    println(file, InteractiveUtils._dump_function(fun, T, false, false, false, true, :att, optimize, :default))
end
\end{minted}



These files can be processed the same way as the unoptimized sysimg IR shown above.



\hypertarget{18346154193244616171}{}


\subsection{Improving LLVM optimizations for Julia}



Improving LLVM code generation usually involves either changing Julia lowering to be more friendly to LLVM{\textquotesingle}s passes, or improving a pass.



If you are planning to improve a pass, be sure to read the \href{https://llvm.org/docs/DeveloperPolicy.html}{LLVM developer policy}. The best strategy is to create a code example in a form where you can use LLVM{\textquotesingle}s \texttt{opt} tool to study it and the pass of interest in isolation.



\begin{itemize}
\item[1. ] Create an example Julia code of interest.


\item[2. ] Use \texttt{JULIA\_LLVM\_ARGS=-print-after-all} to dump the IR.


\item[3. ] Pick out the IR at the point just before the pass of interest runs.


\item[4. ] Strip the debug metadata and fix up the TBAA metadata by hand.

\end{itemize}


The last step is labor intensive.  Suggestions on a better way would be appreciated.



\hypertarget{9068676689189894192}{}


\subsection{The jlcall calling convention}



Julia has a generic calling convention for unoptimized code, which looks somewhat as follows:




\begin{lstlisting}
jl_value_t *any_unoptimized_call(jl_value_t *, jl_value_t **, int);
\end{lstlisting}



where the first argument is the boxed function object, the second argument is an on-stack array of arguments and the third is the number of arguments. Now, we could perform a straightforward lowering and emit an alloca for the argument array. However, this would betray the SSA nature of the uses at the call site, making optimizations (including GC root placement), significantly harder. Instead, we emit it as follows:




\begin{lstlisting}
%bitcast = bitcast @any_unoptimized_call to %jl_value_t *(*)(%jl_value_t *, %jl_value_t *)
call cc 37 %jl_value_t *%bitcast(%jl_value_t *%arg1, %jl_value_t *%arg2)
\end{lstlisting}



The special \texttt{cc 37} annotation marks the fact that this call site is really using the jlcall calling convention. This allows us to retain the SSA-ness of the uses throughout the optimizer. GC root placement will later lower this call to the original C ABI. In the code the calling convention number is represented by the \texttt{JLCALL\_F\_CC} constant. In addition, there is the \texttt{JLCALL\_CC} calling convention which functions similarly, but omits the first argument.



\hypertarget{1193214752065867769}{}


\subsection{GC root placement}



GC root placement is done by an LLVM pass late in the pass pipeline. Doing GC root placement this late enables LLVM to make more aggressive optimizations around code that requires GC roots, as well as allowing us to reduce the number of required GC roots and GC root store operations (since LLVM doesn{\textquotesingle}t understand our GC, it wouldn{\textquotesingle}t otherwise know what it is and is not allowed to do with values stored to the GC frame, so it{\textquotesingle}ll conservatively do very little). As an example, consider an error path




\begin{minted}{julia}
if some_condition()
    #= Use some variables maybe =#
    error("An error occurred")
end
\end{minted}



During constant folding, LLVM may discover that the condition is always false, and can remove the basic block. However, if GC root lowering is done early, the GC root slots used in the deleted block, as well as any values kept alive in those slots only because they were used in the error path, would be kept alive by LLVM. By doing GC root lowering late, we give LLVM the license to do any of its usual optimizations (constant folding, dead code elimination, etc.), without having to worry (too much) about which values may or may not be GC tracked.



However, in order to be able to do late GC root placement, we need to be able to identify a) which pointers are GC tracked and b) all uses of such pointers. The goal of the GC placement pass is thus simple:



Minimize the number of needed GC roots/stores to them subject to the constraint that at every safepoint, any live GC-tracked pointer (i.e. for which there is a path after this point that contains a use of this pointer) is in some GC slot.



\hypertarget{8769102480019606347}{}


\subsubsection{Representation}



The primary difficulty is thus choosing an IR representation that allows us to identify GC-tracked pointers and their uses, even after the program has been run through the optimizer. Our design makes use of three LLVM features to achieve this:



\begin{itemize}
\item Custom address spaces


\item Operand Bundles


\item Non-integral pointers

\end{itemize}


Custom address spaces allow us to tag every point with an integer that needs to be preserved through optimizations. The compiler may not insert casts between address spaces that did not exist in the original program and it must never change the address space of a pointer on a load/store/etc operation. This allows us to annotate which pointers are GC-tracked in an optimizer-resistant way. Note that metadata would not be able to achieve the same purpose. Metadata is supposed to always be discardable without altering the semantics of the program. However, failing to identify a GC-tracked pointer alters the resulting program behavior dramatically - it{\textquotesingle}ll probably crash or return wrong results. We currently use three different address spaces (their numbers are defined in \texttt{src/codegen\_shared.cpp}):



\begin{itemize}
\item GC Tracked Pointers (currently 10): These are pointers to boxed values that may be put into a GC frame. It is loosely equivalent to a \texttt{jl\_value\_t*} pointer on the C side. N.B. It is illegal to ever have a pointer in this address space that may not be stored to a GC slot.


\item Derived Pointers (currently 11): These are pointers that are derived from some GC tracked pointer. Uses of these pointers generate uses of the original pointer. However, they need not themselves be known to the GC. The GC root placement pass MUST always find the GC tracked pointer from which this pointer is derived and use that as the pointer to root.


\item Callee Rooted Pointers (currently 12): This is a utility address space to express the notion of a callee rooted value. All values of this address space MUST be storable to a GC root (though it is possible to relax this condition in the future), but unlike the other pointers need not be rooted if passed to a call (they do still need to be rooted if they are live across another safepoint between the definition and the call).


\item Pointers loaded from tracked object (currently 13): This is used by arrays, which themselves contain a pointer to the managed data. This data area is owned by the array, but is not a GC-tracked object by itself. The compiler guarantees that as long as this pointer is live, the object that this pointer was loaded from will keep being live.

\end{itemize}


\hypertarget{10709955215160021117}{}


\subsubsection{Invariants}



The GC root placement pass makes use of several invariants, which need to be observed by the frontend and are preserved by the optimizer.



First, only the following address space casts are allowed:



\begin{itemize}
\item 0->\{Tracked,Derived,CalleeRooted\}: It is allowable to decay an untracked pointer to any of the others. However, do note that the optimizer has broad license to not root such a value. It is never safe to have a value in address space 0 in any part of the program if it is (or is derived from) a value that requires a GC root.


\item Tracked->Derived: This is the standard decay route for interior values. The placement pass will look for these to identify the base pointer for any use.


\item Tracked->CalleeRooted: Addrspace CalleeRooted serves merely as a hint that a GC root is not required. However, do note that the Derived->CalleeRooted decay is prohibited, since pointers should generally be storable to a GC slot, even in this address space.

\end{itemize}


Now let us consider what constitutes a use:



\begin{itemize}
\item Loads whose loaded values is in one of the address spaces


\item Stores of a value in one of the address spaces to a location


\item Stores to a pointer in one of the address spaces


\item Calls for which a value in one of the address spaces is an operand


\item Calls in jlcall ABI, for which the argument array contains a value


\item Return instructions.

\end{itemize}


We explicitly allow load/stores and simple calls in address spaces Tracked/Derived. Elements of jlcall argument arrays must always be in address space Tracked (it is required by the ABI that they are valid \texttt{jl\_value\_t*} pointers). The same is true for return instructions (though note that struct return arguments are allowed to have any of the address spaces). The only allowable use of an address space CalleeRooted pointer is to pass it to a call (which must have an appropriately typed operand).



Further, we disallow \texttt{getelementptr} in addrspace Tracked. This is because unless the operation is a noop, the resulting pointer will not be validly storable to a GC slot and may thus not be in this address space. If such a pointer is required, it should be decayed to addrspace Derived first.



Lastly, we disallow \texttt{inttoptr}/\texttt{ptrtoint} instructions in these address spaces. Having these instructions would mean that some \texttt{i64} values are really GC tracked. This is problematic, because it breaks that stated requirement that we{\textquotesingle}re able to identify GC-relevant pointers. This invariant is accomplished using the LLVM {\textquotedbl}non-integral pointers{\textquotedbl} feature, which is new in LLVM 5.0. It prohibits the optimizer from making optimizations that would introduce these operations. Note we can still insert static constants at JIT time by using \texttt{inttoptr} in address space 0 and then decaying to the appropriate address space afterwards.



\hypertarget{5598613901277707990}{}


\subsubsection{Supporting \texttt{ccall}}



One important aspect missing from the discussion so far is the handling of \hyperlink{14245046751182637566}{\texttt{ccall}}. \hyperlink{14245046751182637566}{\texttt{ccall}} has the peculiar feature that the location and scope of a use do not coincide. As an example consider:




\begin{minted}{julia}
A = randn(1024)
ccall(:foo, Cvoid, (Ptr{Float64},), A)
\end{minted}



In lowering, the compiler will insert a conversion from the array to the pointer which drops the reference to the array value. However, we of course need to make sure that the array does stay alive while we{\textquotesingle}re doing the \hyperlink{14245046751182637566}{\texttt{ccall}}. To understand how this is done, first recall the lowering of the above code:




\begin{minted}{julia}
return $(Expr(:foreigncall, :(:foo), Cvoid, svec(Ptr{Float64}), 0, :(:ccall), Expr(:foreigncall, :(:jl_array_ptr), Ptr{Float64}, svec(Any), 0, :(:ccall), :(A)), :(A)))
\end{minted}



The last \texttt{:(A)}, is an extra argument list inserted during lowering that informs the code generator which Julia level values need to be kept alive for the duration of this \hyperlink{14245046751182637566}{\texttt{ccall}}. We then take this information and represent it in an {\textquotedbl}operand bundle{\textquotedbl} at the IR level. An operand bundle is essentially a fake use that is attached to the call site. At the IR level, this looks like so:




\begin{lstlisting}
call void inttoptr (i64 ... to void (double*)*)(double* %5) [ "jl_roots"(%jl_value_t addrspace(10)* %A) ]
\end{lstlisting}



The GC root placement pass will treat the \texttt{jl\_roots} operand bundle as if it were a regular operand. However, as a final step, after the GC roots are inserted, it will drop the operand bundle to avoid confusing instruction selection.



\hypertarget{9100387688336588039}{}


\subsubsection{Supporting \texttt{pointer\_from\_objref}}



\hyperlink{9366554937543398846}{\texttt{pointer\_from\_objref}} is special because it requires the user to take explicit control of GC rooting. By our above invariants, this function is illegal, because it performs an address space cast from 10 to 0. However, it can be useful, in certain situations, so we provide a special intrinsic:




\begin{lstlisting}
declared %jl_value_t *julia.pointer_from_objref(%jl_value_t addrspace(10)*)
\end{lstlisting}



which is lowered to the corresponding address space cast after GC root lowering. Do note however that by using this intrinsic, the caller assumes all responsibility for making sure that the value in question is rooted. Further this intrinsic is not considered a use, so the GC root placement pass will not provide a GC root for the function. As a result, the external rooting must be arranged while the value is still tracked by the system. I.e. it is not valid to attempt to use the result of this operation to establish a global root - the optimizer may have already dropped the value.



\hypertarget{13949345143972752854}{}


\subsubsection{Keeping values alive in the absence of uses}



In certain cases it is necessary to keep an object alive, even though there is no compiler-visible use of said object. This may be case for low level code that operates on the memory-representation of an object directly or code that needs to interface with C code. In order to allow this, we provide the following intrinsics at the LLVM level:




\begin{lstlisting}
token @llvm.julia.gc_preserve_begin(...)
void @llvm.julia.gc_preserve_end(token)
\end{lstlisting}



(The \texttt{llvm.} in the name is required in order to be able to use the \texttt{token} type). The semantics of these intrinsics are as follows: At any safepoint that is dominated by a \texttt{gc\_preserve\_begin} call, but that is not not dominated by a corresponding \texttt{gc\_preserve\_end} call (i.e. a call whose argument is the token returned by a \texttt{gc\_preserve\_begin} call), the values passed as arguments to that \texttt{gc\_preserve\_begin} will be kept live. Note that the \texttt{gc\_preserve\_begin} still counts as a regular use of those values, so the standard lifetime semantics will ensure that the values will be kept alive before entering the preserve region.



\hypertarget{12315144918899798229}{}


\section{printf() and stdio in the Julia runtime}



\hypertarget{12808189198177172525}{}


\subsection{Libuv wrappers for stdio}



\texttt{julia.h} defines \href{https://docs.libuv.org}{libuv} wrappers for the \texttt{stdio.h} streams:




\begin{lstlisting}
uv_stream_t *JL_STDIN;
uv_stream_t *JL_STDOUT;
uv_stream_t *JL_STDERR;
\end{lstlisting}



... and corresponding output functions:




\begin{lstlisting}
int jl_printf(uv_stream_t *s, const char *format, ...);
int jl_vprintf(uv_stream_t *s, const char *format, va_list args);
\end{lstlisting}



These \texttt{printf} functions are used by the \texttt{.c} files in the \texttt{src/} and \texttt{cli/} directories wherever stdio is needed to ensure that output buffering is handled in a unified way.



In special cases, like signal handlers, where the full libuv infrastructure is too heavy, \texttt{jl\_safe\_printf()} can be used to \hyperlink{16947913578760238729}{\texttt{write(2)}} directly to \texttt{STDERR\_FILENO}:




\begin{lstlisting}
void jl_safe_printf(const char *str, ...);
\end{lstlisting}



\hypertarget{9919396910097555458}{}


\subsection{Interface between JL\_STD* and Julia code}



\hyperlink{3330957653919693521}{\texttt{Base.stdin}}, \hyperlink{18181294266083891471}{\texttt{Base.stdout}} and \hyperlink{6150355911915549172}{\texttt{Base.stderr}} are bound to the \texttt{JL\_STD*} libuv streams defined in the runtime.



Julia{\textquotesingle}s \texttt{\_\_init\_\_()} function (in \texttt{base/sysimg.jl}) calls \texttt{reinit\_stdio()} (in \texttt{base/stream.jl}) to create Julia objects for \hyperlink{3330957653919693521}{\texttt{Base.stdin}}, \hyperlink{18181294266083891471}{\texttt{Base.stdout}} and \hyperlink{6150355911915549172}{\texttt{Base.stderr}}.



\texttt{reinit\_stdio()} uses \hyperlink{14245046751182637566}{\texttt{ccall}} to retrieve pointers to \texttt{JL\_STD*} and calls \texttt{jl\_uv\_handle\_type()} to inspect the type of each stream.  It then creates a Julia \texttt{Base.IOStream}, \texttt{Base.TTY} or \texttt{Base.PipeEndpoint} object to represent each stream, e.g.:




\begin{lstlisting}
$ julia -e 'println(typeof((stdin, stdout, stderr)))'
Tuple{Base.TTY,Base.TTY,Base.TTY}

$ julia -e 'println(typeof((stdin, stdout, stderr)))' < /dev/null 2>/dev/null
Tuple{IOStream,Base.TTY,IOStream}

$ echo hello | julia -e 'println(typeof((stdin, stdout, stderr)))' | cat
Tuple{Base.PipeEndpoint,Base.PipeEndpoint,Base.TTY}
\end{lstlisting}



The \hyperlink{8104134490906192097}{\texttt{Base.read}} and \hyperlink{16947913578760238729}{\texttt{Base.write}} methods for these streams use \hyperlink{14245046751182637566}{\texttt{ccall}} to call libuv wrappers in \texttt{src/jl\_uv.c}, e.g.:




\begin{lstlisting}
stream.jl: function write(s::IO, p::Ptr, nb::Integer)
               -> ccall(:jl_uv_write, ...)
  jl_uv.c:          -> int jl_uv_write(uv_stream_t *stream, ...)
                        -> uv_write(uvw, stream, buf, ...)
\end{lstlisting}



\hypertarget{10878739879441287330}{}


\subsection{printf() during initialization}



The libuv streams relied upon by \texttt{jl\_printf()} etc., are not available until midway through initialization of the runtime (see \texttt{init.c}, \texttt{init\_stdio()}).  Error messages or warnings that need to be printed before this are routed to the standard C library \texttt{fwrite()} function by the following mechanism:



In \texttt{sys.c}, the \texttt{JL\_STD*} stream pointers are statically initialized to integer constants: \texttt{STD*\_FILENO (0, 1 and 2)}. In \texttt{jl\_uv.c} the \texttt{jl\_uv\_puts()} function checks its \texttt{uv\_stream\_t* stream} argument and calls \texttt{fwrite()} if stream is set to \texttt{STDOUT\_FILENO} or \texttt{STDERR\_FILENO}.



This allows for uniform use of \texttt{jl\_printf()} throughout the runtime regardless of whether or not any particular piece of code is reachable before initialization is complete.



\hypertarget{15654976823361224335}{}


\subsection{Legacy \texttt{ios.c} library}



The \texttt{src/support/ios.c} library is inherited from \href{https://github.com/JeffBezanson/femtolisp}{femtolisp}. It provides cross-platform buffered file IO and in-memory temporary buffers.



\texttt{ios.c} is still used by:



\begin{itemize}
\item \texttt{src/flisp/*.c}


\item \texttt{src/dump.c} – for serialization file IO and for memory buffers.


\item \texttt{src/staticdata.c} – for serialization file IO and for memory buffers.


\item \texttt{base/iostream.jl} – for file IO (see \texttt{base/fs.jl} for libuv equivalent).

\end{itemize}


Use of \texttt{ios.c} in these modules is mostly self-contained and separated from the libuv I/O system. However, there is \href{https://github.com/JuliaLang/julia/blob/master/src/flisp/print.c\#L654}{one place} where femtolisp calls through to \texttt{jl\_printf()} with a legacy \texttt{ios\_t} stream.



There is a hack in \texttt{ios.h} that makes the \texttt{ios\_t.bm} field line up with the \texttt{uv\_stream\_t.type} and ensures that the values used for \texttt{ios\_t.bm} to not overlap with valid \texttt{UV\_HANDLE\_TYPE} values.  This allows \texttt{uv\_stream\_t} pointers to point to \texttt{ios\_t} streams.



This is needed because \texttt{jl\_printf()} caller \texttt{jl\_static\_show()} is passed an \texttt{ios\_t} stream by femtolisp{\textquotesingle}s \texttt{fl\_print()} function. Julia{\textquotesingle}s \texttt{jl\_uv\_puts()} function has special handling for this:




\begin{lstlisting}
if (stream->type > UV_HANDLE_TYPE_MAX) {
    return ios_write((ios_t*)stream, str, n);
}
\end{lstlisting}



\hypertarget{1119183925675693738}{}


\section{Bounds checking}



Like many modern programming languages, Julia uses bounds checking to ensure program safety when accessing arrays. In tight inner loops or other performance critical situations, you may wish to skip these bounds checks to improve runtime performance. For instance, in order to emit vectorized (SIMD) instructions, your loop body cannot contain branches, and thus cannot contain bounds checks. Consequently, Julia includes an \texttt{@inbounds(...)} macro to tell the compiler to skip such bounds checks within the given block. User-defined array types can use the \texttt{@boundscheck(...)} macro to achieve context-sensitive code selection.



\hypertarget{15977328606651213476}{}


\subsection{Eliding bounds checks}



The \texttt{@boundscheck(...)} macro marks blocks of code that perform bounds checking. When such blocks are inlined into an \texttt{@inbounds(...)} block, the compiler may remove these blocks. The compiler removes the \texttt{@boundscheck} block \emph{only if it is inlined} into the calling function. For example, you might write the method \texttt{sum} as:




\begin{minted}{julia}
function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in eachindex(A)
        @inbounds r += A[i]
    end
    return r
end
\end{minted}



With a custom array-like type \texttt{MyArray} having:




\begin{minted}{julia}
@inline getindex(A::MyArray, i::Real) = (@boundscheck checkbounds(A,i); A.data[to_index(i)])
\end{minted}



Then when \texttt{getindex} is inlined into \texttt{sum}, the call to \texttt{checkbounds(A,i)} will be elided. If your function contains multiple layers of inlining, only \texttt{@boundscheck} blocks at most one level of inlining deeper are eliminated. The rule prevents unintended changes in program behavior from code further up the stack.



\hypertarget{13791305586943178951}{}


\subsubsection{Caution!}



It is easy to accidentally expose unsafe operations with \texttt{@inbounds}. You might be tempted to write the above example as




\begin{minted}{julia}
function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i]
    end
	return r
end
\end{minted}



Which quietly assumes 1-based indexing and therefore exposes unsafe memory access when used with \hyperlink{6526948166346290584}{\texttt{OffsetArrays}}:




\begin{minted}{jlcon}
julia> using OffsetArrays

julia> sum(OffsetArray([1,2,3], -10))
9164911648 # inconsistent results or segfault
\end{minted}



While the original source of the error here is \texttt{1:length(A)}, the use of \texttt{@inbounds} increases the consequences from a bounds error to a less easily caught and debugged unsafe memory access. It is often difficult or impossible to prove that a method which uses \texttt{@inbounds} is safe, so one must weigh the benefits of performance improvements against the risk of segfaults and silent misbehavior, especially in public facing APIs.



\hypertarget{10208484018202603417}{}


\subsection{Propagating inbounds}



There may be certain scenarios where for code-organization reasons you want more than one layer between the \texttt{@inbounds} and \texttt{@boundscheck} declarations. For instance, the default \texttt{getindex} methods have the chain \texttt{getindex(A::AbstractArray, i::Real)} calls \texttt{getindex(IndexStyle(A), A, i)} calls \texttt{\_getindex(::IndexLinear, A, i)}.



To override the {\textquotedbl}one layer of inlining{\textquotedbl} rule, a function may be marked with \hyperlink{4942611866585954207}{\texttt{Base.@propagate\_inbounds}} to propagate an inbounds context (or out of bounds context) through one additional layer of inlining.



\hypertarget{17261866997775737461}{}


\subsection{The bounds checking call hierarchy}



The overall hierarchy is:



\begin{itemize}
\item \texttt{checkbounds(A, I...)} which calls

\begin{itemize}
\item \texttt{checkbounds(Bool, A, I...)} which calls

\begin{itemize}
\item \texttt{checkbounds\_indices(Bool, axes(A), I)} which recursively calls

\begin{itemize}
\item \texttt{checkindex} for each dimension

\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}


Here \texttt{A} is the array, and \texttt{I} contains the {\textquotedbl}requested{\textquotedbl} indices. \texttt{axes(A)} returns a tuple of {\textquotedbl}permitted{\textquotedbl} indices of \texttt{A}.



\texttt{checkbounds(A, I...)} throws an error if the indices are invalid, whereas \texttt{checkbounds(Bool, A, I...)} returns \texttt{false} in that circumstance.  \texttt{checkbounds\_indices} discards any information about the array other than its \texttt{axes} tuple, and performs a pure indices-vs-indices comparison: this allows relatively few compiled methods to serve a huge variety of array types. Indices are specified as tuples, and are usually compared in a 1-1 fashion with individual dimensions handled by calling another important function, \texttt{checkindex}: typically,




\begin{minted}{julia}
checkbounds_indices(Bool, (IA1, IA...), (I1, I...)) = checkindex(Bool, IA1, I1) &
                                                      checkbounds_indices(Bool, IA, I)
\end{minted}



so \texttt{checkindex} checks a single dimension.  All of these functions, including the unexported \texttt{checkbounds\_indices} have docstrings accessible with \texttt{?} .



If you have to customize bounds checking for a specific array type, you should specialize \texttt{checkbounds(Bool, A, I...)}. However, in most cases you should be able to rely on \texttt{checkbounds\_indices} as long as you supply useful \texttt{axes} for your array type.



If you have novel index types, first consider specializing \texttt{checkindex}, which handles a single index for a particular dimension of an array.  If you have a custom multidimensional index type (similar to \texttt{CartesianIndex}), then you may have to consider specializing \texttt{checkbounds\_indices}.



Note this hierarchy has been designed to reduce the likelihood of method ambiguities.  We try to make \texttt{checkbounds} the place to specialize on array type, and try to avoid specializations on index types; conversely, \texttt{checkindex} is intended to be specialized only on index type (especially, the last argument).



\hypertarget{15511139749343586557}{}


\subsection{Emit bounds checks}



Julia can be launched with \texttt{--check-bounds=\{yes|no|auto\}} to emit bounds checks always, never, or respect @inbounds declarations.



\hypertarget{17038639605096915302}{}


\section{Proper maintenance and care of multi-threading locks}



The following strategies are used to ensure that the code is dead-lock free (generally by addressing the 4th Coffman condition: circular wait).



\begin{quote}
\begin{itemize}
\item[1. ] structure code such that only one lock will need to be acquired at a time


\item[2. ] always acquire shared locks in the same order, as given by the table below


\item[3. ] avoid constructs that expect to need unrestricted recursion

\end{itemize}
\end{quote}


\hypertarget{13071695811965191352}{}


\subsection{Locks}



Below are all of the locks that exist in the system and the mechanisms for using them that avoid the potential for deadlocks (no Ostrich algorithm allowed here):



The following are definitely leaf locks (level 1), and must not try to acquire any other lock:



\begin{quote}
\begin{itemize}
\item safepoint

\begin{quote}
Note that this lock is acquired implicitly by \texttt{JL\_LOCK} and \texttt{JL\_UNLOCK}. use the \texttt{\_NOGC} variants to avoid that for level 1 locks.

While holding this lock, the code must not do any allocation or hit any safepoints. Note that there are safepoints when doing allocation, enabling / disabling GC, entering / restoring exception frames, and taking / releasing locks.

\end{quote}

\item shared\_map


\item finalizers


\item pagealloc


\item gc\emph{perm}lock


\item flisp


\item jl\emph{in}stackwalk (Win32)

\begin{quote}
flisp itself is already threadsafe, this lock only protects the \texttt{jl\_ast\_context\_list\_t} pool

\end{quote}
\end{itemize}
\end{quote}


The following is a leaf lock (level 2), and only acquires level 1 locks (safepoint) internally:



\begin{quote}
\begin{itemize}
\item typecache


\item Module->lock

\end{itemize}
\end{quote}


The following is a level 3 lock, which can only acquire level 1 or level 2 locks internally:



\begin{quote}
\begin{itemize}
\item Method->writelock

\end{itemize}
\end{quote}


The following is a level 4 lock, which can only recurse to acquire level 1, 2, or 3 locks:



\begin{quote}
\begin{itemize}
\item MethodTable->writelock

\end{itemize}
\end{quote}


No Julia code may be called while holding a lock above this point.



The following are a level 6 lock, which can only recurse to acquire locks at lower levels:



\begin{quote}
\begin{itemize}
\item codegen


\item jl\emph{modules}mutex

\end{itemize}
\end{quote}


The following is an almost root lock (level end-1), meaning only the root look may be held when trying to acquire it:



\begin{quote}
\begin{itemize}
\item typeinf

\begin{quote}
this one is perhaps one of the most tricky ones, since type-inference can be invoked from many points

currently the lock is merged with the codegen lock, since they call each other recursively

\end{quote}
\end{itemize}
\end{quote}


The following lock synchronizes IO operation. Be aware that doing any I/O (for example, printing warning messages or debug information) while holding any other lock listed above may result in pernicious and hard-to-find deadlocks. BE VERY CAREFUL!



\begin{quote}
\begin{itemize}
\item iolock


\item Individual ThreadSynchronizers locks

\begin{quote}
this may continue to be held after releasing the iolock, or acquired without it, but be very careful to never attempt to acquire the iolock while holding it

\end{quote}
\end{itemize}
\end{quote}


The following is the root lock, meaning no other lock shall be held when trying to acquire it:



\begin{quote}
\begin{itemize}
\item toplevel

\begin{quote}
this should be held while attempting a top-level action (such as making a new type or defining a new method): trying to obtain this lock inside a staged function will cause a deadlock condition!

additionally, it{\textquotesingle}s unclear if \emph{any} code can safely run in parallel with an arbitrary toplevel expression, so it may require all threads to get to a safepoint first

\end{quote}
\end{itemize}
\end{quote}


\hypertarget{9065842407229995149}{}


\subsection{Broken Locks}



The following locks are broken:



\begin{itemize}
\item toplevel

\begin{quote}
doesn{\textquotesingle}t exist right now

fix: create it

\end{quote}

\item Module->lock

\begin{quote}
This is vulnerable to deadlocks since it can{\textquotesingle}t be certain it is acquired in sequence. Some operations (such as \texttt{import\_module}) are missing a lock.

fix: replace with \texttt{jl\_modules\_mutex}?

\end{quote}

\item loading.jl: \texttt{require} and \texttt{register\_root\_module}

\begin{quote}
This file potentially has numerous problems.

fix: needs locks

\end{quote}
\end{itemize}


\hypertarget{1067922422509615580}{}


\subsection{Shared Global Data Structures}



These data structures each need locks due to being shared mutable global state. It is the inverse list for the above lock priority list. This list does not include level 1 leaf resources due to their simplicity.



MethodTable modifications (def, cache, kwsorter type) : MethodTable->writelock



Type declarations : toplevel lock



Type application : typecache lock



Global variable tables : Module->lock



Module serializer : toplevel lock



JIT \& type-inference : codegen lock



MethodInstance/CodeInstance updates : Method->writelock, codegen lock



\begin{quote}
\begin{itemize}
\item These are set at construction and immutable:

\begin{itemize}
\item specTypes


\item sparam\_vals


\item def

\end{itemize}
\end{itemize}
\end{quote}


\begin{quote}
\begin{itemize}
\item These are set by \texttt{jl\_type\_infer} (while holding codegen lock):

\begin{itemize}
\item cache


\item rettype


\item inferred

\end{itemize}
\end{itemize}
\end{quote}



\begin{lstlisting}
    * valid ages
\end{lstlisting}



\begin{quote}
\begin{itemize}
\item \texttt{inInference} flag:

\begin{itemize}
\item optimization to quickly avoid recurring into \texttt{jl\_type\_infer} while it is already running


\item actual state (of setting \texttt{inferred}, then \texttt{fptr}) is protected by codegen lock

\end{itemize}
\end{itemize}
\end{quote}


\begin{quote}
\begin{itemize}
\item Function pointers:

\begin{itemize}
\item these transition once, from \texttt{NULL} to a value, while the codegen lock is held

\end{itemize}

\item Code-generator cache (the contents of \texttt{functionObjectsDecls}):

\begin{itemize}
\item these can transition multiple times, but only while the codegen lock is held


\item it is valid to use old version of this, or block for new versions of this, so races are benign, as long as the code is careful not to reference other data in the method instance (such as \texttt{rettype}) and assume it is coordinated, unless also holding the codegen lock

\end{itemize}
\end{itemize}
\end{quote}


LLVMContext : codegen lock



Method : Method->writelock



\begin{itemize}
\item roots array (serializer and codegen)


\item invoke / specializations / tfunc modifications

\end{itemize}


\hypertarget{10976543622473505620}{}


\section{Arrays with custom indices}



Conventionally, Julia{\textquotesingle}s arrays are indexed starting at 1, whereas some other languages start numbering at 0, and yet others (e.g., Fortran) allow you to specify arbitrary starting indices.  While there is much merit in picking a standard (i.e., 1 for Julia), there are some algorithms which simplify considerably if you can index outside the range \texttt{1:size(A,d)} (and not just \texttt{0:size(A,d)-1}, either). To facilitate such computations, Julia supports arrays with arbitrary indices.



The purpose of this page is to address the question, {\textquotedbl}what do I have to do to support such arrays in my own code?{\textquotedbl}  First, let{\textquotesingle}s address the simplest case: if you know that your code will never need to handle arrays with unconventional indexing, hopefully the answer is {\textquotedbl}nothing.{\textquotedbl} Old code, on conventional arrays, should function essentially without alteration as long as it was using the exported interfaces of Julia. If you find it more convenient to just force your users to supply traditional arrays where indexing starts at one, you can add




\begin{minted}{julia}
Base.require_one_based_indexing(arrays...)
\end{minted}



where \texttt{arrays...} is a list of the array objects that you wish to check for anything that violates 1-based indexing.



\hypertarget{12133789893433293548}{}


\subsection{Generalizing existing code}



As an overview, the steps are:



\begin{itemize}
\item replace many uses of \texttt{size} with \texttt{axes}


\item replace \texttt{1:length(A)} with \texttt{eachindex(A)}, or in some cases \texttt{LinearIndices(A)}


\item replace explicit allocations like \texttt{Array\{Int\}(undef, size(B))} with \texttt{similar(Array\{Int\}, axes(B))}

\end{itemize}


These are described in more detail below.



\hypertarget{3112549925833933267}{}


\subsubsection{Things to watch out for}



Because unconventional indexing breaks many people{\textquotesingle}s assumptions that all arrays start indexing with 1, there is always the chance that using such arrays will trigger errors. The most frustrating bugs would be incorrect results or segfaults (total crashes of Julia). For example, consider the following function:




\begin{minted}{julia}
function mycopy!(dest::AbstractVector, src::AbstractVector)
    length(dest) == length(src) || throw(DimensionMismatch("vectors must match"))
    # OK, now we're safe to use @inbounds, right? (not anymore!)
    for i = 1:length(src)
        @inbounds dest[i] = src[i]
    end
    dest
end
\end{minted}



This code implicitly assumes that vectors are indexed from 1; if \texttt{dest} starts at a different index than \texttt{src}, there is a chance that this code would trigger a segfault. (If you do get segfaults, to help locate the cause try running julia with the option \texttt{--check-bounds=yes}.)



\hypertarget{1628519832551402952}{}


\subsubsection{Using \texttt{axes} for bounds checks and loop iteration}



\texttt{axes(A)} (reminiscent of \texttt{size(A)}) returns a tuple of \texttt{AbstractUnitRange\{<:Integer\}} objects, specifying the range of valid indices along each dimension of \texttt{A}.  When \texttt{A} has unconventional indexing, the ranges may not start at 1.  If you just want the range for a particular dimension \texttt{d}, there is \texttt{axes(A, d)}.



Base implements a custom range type, \texttt{OneTo}, where \texttt{OneTo(n)} means the same thing as \texttt{1:n} but in a form that guarantees (via the type system) that the lower index is 1. For any new \hyperlink{6514416309183787338}{\texttt{AbstractArray}} type, this is the default returned by \texttt{axes}, and it indicates that this array type uses {\textquotedbl}conventional{\textquotedbl} 1-based indexing.



For bounds checking, note that there are dedicated functions \texttt{checkbounds} and \texttt{checkindex} which can sometimes simplify such tests.



\hypertarget{15690038127583202025}{}


\subsubsection{Linear indexing (\texttt{LinearIndices})}



Some algorithms are most conveniently (or efficiently) written in terms of a single linear index, \texttt{A[i]} even if \texttt{A} is multi-dimensional. Regardless of the array{\textquotesingle}s native indices, linear indices always range from \texttt{1:length(A)}. However, this raises an ambiguity for one-dimensional arrays (a.k.a., \hyperlink{12517057979818647811}{\texttt{AbstractVector}}): does \texttt{v[i]} mean linear indexing , or Cartesian indexing with the array{\textquotesingle}s native indices?



For this reason, your best option may be to iterate over the array with \texttt{eachindex(A)}, or, if you require the indices to be sequential integers, to get the index range by calling \texttt{LinearIndices(A)}. This will return \texttt{axes(A, 1)} if A is an AbstractVector, and the equivalent of \texttt{1:length(A)} otherwise.



By this definition, 1-dimensional arrays always use Cartesian indexing with the array{\textquotesingle}s native indices. To help enforce this, it{\textquotesingle}s worth noting that the index conversion functions will throw an error if shape indicates a 1-dimensional array with unconventional indexing (i.e., is a \texttt{Tuple\{UnitRange\}} rather than a tuple of \texttt{OneTo}). For arrays with conventional indexing, these functions continue to work the same as always.



Using \texttt{axes} and \texttt{LinearIndices}, here is one way you could rewrite \texttt{mycopy!}:




\begin{minted}{julia}
function mycopy!(dest::AbstractVector, src::AbstractVector)
    axes(dest) == axes(src) || throw(DimensionMismatch("vectors must match"))
    for i in LinearIndices(src)
        @inbounds dest[i] = src[i]
    end
    dest
end
\end{minted}



\hypertarget{8664586686165592487}{}


\subsubsection{Allocating storage using generalizations of \texttt{similar}}



Storage is often allocated with \texttt{Array\{Int\}(undef, dims)} or \texttt{similar(A, args...)}. When the result needs to match the indices of some other array, this may not always suffice. The generic replacement for such patterns is to use \texttt{similar(storagetype, shape)}.  \texttt{storagetype} indicates the kind of underlying {\textquotedbl}conventional{\textquotedbl} behavior you{\textquotesingle}d like, e.g., \texttt{Array\{Int\}} or \texttt{BitArray} or even \texttt{dims->zeros(Float32, dims)} (which would allocate an all-zeros array). \texttt{shape} is a tuple of \hyperlink{8469131683393450448}{\texttt{Integer}} or \texttt{AbstractUnitRange} values, specifying the indices that you want the result to use. Note that a convenient way of producing an all-zeros array that matches the indices of A is simply \texttt{zeros(A)}.



Let{\textquotesingle}s walk through a couple of explicit examples. First, if \texttt{A} has conventional indices, then \texttt{similar(Array\{Int\}, axes(A))} would end up calling \texttt{Array\{Int\}(undef, size(A))}, and thus return an array.  If \texttt{A} is an \texttt{AbstractArray} type with unconventional indexing, then \texttt{similar(Array\{Int\}, axes(A))} should return something that {\textquotedbl}behaves like{\textquotedbl} an \texttt{Array\{Int\}} but with a shape (including indices) that matches \texttt{A}.  (The most obvious implementation is to allocate an \texttt{Array\{Int\}(undef, size(A))} and then {\textquotedbl}wrap{\textquotedbl} it in a type that shifts the indices.)



Note also that \texttt{similar(Array\{Int\}, (axes(A, 2),))} would allocate an \texttt{AbstractVector\{Int\}} (i.e., 1-dimensional array) that matches the indices of the columns of \texttt{A}.



\hypertarget{4699181205936675892}{}


\subsection{Writing custom array types with non-1 indexing}



Most of the methods you{\textquotesingle}ll need to define are standard for any \texttt{AbstractArray} type, see \hyperlink{9718377734213742156}{Abstract Arrays}. This page focuses on the steps needed to define unconventional indexing.



\hypertarget{5924242856598341681}{}


\subsubsection{Custom \texttt{AbstractUnitRange} types}



If you{\textquotesingle}re writing a non-1 indexed array type, you will want to specialize \texttt{axes} so it returns a \texttt{UnitRange}, or (perhaps better) a custom \texttt{AbstractUnitRange}.  The advantage of a custom type is that it {\textquotedbl}signals{\textquotedbl} the allocation type for functions like \texttt{similar}. If we{\textquotesingle}re writing an array type for which indexing will start at 0, we likely want to begin by creating a new \texttt{AbstractUnitRange}, \texttt{ZeroRange}, where \texttt{ZeroRange(n)} is equivalent to \texttt{0:n-1}.



In general, you should probably \emph{not} export \texttt{ZeroRange} from your package: there may be other packages that implement their own \texttt{ZeroRange}, and having multiple distinct \texttt{ZeroRange} types is (perhaps counterintuitively) an advantage: \texttt{ModuleA.ZeroRange} indicates that \texttt{similar} should create a \texttt{ModuleA.ZeroArray}, whereas \texttt{ModuleB.ZeroRange} indicates a \texttt{ModuleB.ZeroArray} type.  This design allows peaceful coexistence among many different custom array types.



Note that the Julia package \href{https://github.com/JuliaArrays/CustomUnitRanges.jl}{CustomUnitRanges.jl} can sometimes be used to avoid the need to write your own \texttt{ZeroRange} type.



\hypertarget{5545377887498946318}{}


\subsubsection{Specializing \texttt{axes}}



Once you have your \texttt{AbstractUnitRange} type, then specialize \texttt{axes}:




\begin{minted}{julia}
Base.axes(A::ZeroArray) = map(n->ZeroRange(n), A.size)
\end{minted}



where here we imagine that \texttt{ZeroArray} has a field called \texttt{size} (there would be other ways to implement this).



In some cases, the fallback definition for \texttt{axes(A, d)}:




\begin{minted}{julia}
axes(A::AbstractArray{T,N}, d) where {T,N} = d <= N ? axes(A)[d] : OneTo(1)
\end{minted}



may not be what you want: you may need to specialize it to return something other than \texttt{OneTo(1)} when \texttt{d > ndims(A)}.  Likewise, in \texttt{Base} there is a dedicated function \texttt{axes1} which is equivalent to \texttt{axes(A, 1)} but which avoids checking (at runtime) whether \texttt{ndims(A) > 0}. (This is purely a performance optimization.)  It is defined as:




\begin{minted}{julia}
axes1(A::AbstractArray{T,0}) where {T} = OneTo(1)
axes1(A::AbstractArray) = axes(A)[1]
\end{minted}



If the first of these (the zero-dimensional case) is problematic for your custom array type, be sure to specialize it appropriately.



\hypertarget{5556461946249659648}{}


\subsubsection{Specializing \texttt{similar}}



Given your custom \texttt{ZeroRange} type, then you should also add the following two specializations for \texttt{similar}:




\begin{minted}{julia}
function Base.similar(A::AbstractArray, T::Type, shape::Tuple{ZeroRange,Vararg{ZeroRange}})
    # body
end

function Base.similar(f::Union{Function,DataType}, shape::Tuple{ZeroRange,Vararg{ZeroRange}})
    # body
end
\end{minted}



Both of these should allocate your custom array type.



\hypertarget{16440701718956142132}{}


\subsubsection{Specializing \texttt{reshape}}



Optionally, define a method




\begin{lstlisting}
Base.reshape(A::AbstractArray, shape::Tuple{ZeroRange,Vararg{ZeroRange}}) = ...
\end{lstlisting}



and you can \texttt{reshape} an array so that the result has custom indices.



\hypertarget{18101383726856776769}{}


\subsubsection{For objects that mimic AbstractArray but are not subtypes}



\texttt{has\_offset\_axes} depends on having \texttt{axes} defined for the objects you call it on. If there is some reason you don{\textquotesingle}t have an \texttt{axes} method defined for your object, consider defining a method




\begin{minted}{julia}
Base.has_offset_axes(obj::MyNon1IndexedArraylikeObject) = true
\end{minted}



This will allow code that assumes 1-based indexing to detect a problem and throw a helpful error, rather than returning incorrect results or segfaulting julia.



\hypertarget{12667413576127681892}{}


\subsubsection{Catching errors}



If your new array type triggers errors in other code, one helpful debugging step can be to comment out \texttt{@boundscheck} in your \texttt{getindex} and \texttt{setindex!} implementation. This will ensure that every element access checks bounds. Or, restart julia with \texttt{--check-bounds=yes}.



In some cases it may also be helpful to temporarily disable \texttt{size} and \texttt{length} for your new array type, since code that makes incorrect assumptions frequently uses these functions.



\hypertarget{860188194179028487}{}


\section{Module loading}



\hyperlink{16690217505788642360}{\texttt{Base.require}} is responsible for loading modules and it also manages the precompilation cache. It is the implementation of the \texttt{import} statement.



\hypertarget{8895570116735182580}{}


\subsection{Experimental features}



The features below are experimental and not part of the stable Julia API. Before building upon them inform yourself about the current thinking and whether they might change soon.



\hypertarget{14149276766401458683}{}


\subsubsection{Module loading callbacks}



It is possible to listen to the modules loaded by \texttt{Base.require}, by registering a callback.




\begin{minted}{julia}
loaded_packages = Channel{Symbol}()
callback = (mod::Symbol) -> put!(loaded_packages, mod)
push!(Base.package_callbacks, callback)
\end{minted}



Please note that the symbol given to the callback is a non-unique identifier and it is the responsibility of the callback provider to walk the module chain to determine the fully qualified name of the loaded binding.



The callback below is an example of how to do that:




\begin{minted}{julia}
# Get the fully-qualified name of a module.
function module_fqn(name::Symbol)
    fqn = fullname(Base.root_module(name))
    return join(fqn, '.')
end
\end{minted}



\hypertarget{582051353723326392}{}


\section{Inference}



\hypertarget{7885214660101366687}{}


\subsection{How inference works}



\href{https://en.wikipedia.org/wiki/Type\_inference}{Type inference} refers to the process of deducing the types of later values from the types of input values. Julia{\textquotesingle}s approach to inference has been described in blog posts (\href{https://juliacomputing.com/blog/2016/04/inference-convergence/}{1}, \href{https://juliacomputing.com/blog/2017/05/inference-converage2/}{2}).



\hypertarget{5858996762602267783}{}


\subsection{Debugging compiler.jl}



You can start a Julia session, edit \texttt{compiler/*.jl} (for example to insert \texttt{print} statements), and then replace \texttt{Core.Compiler} in your running session by navigating to \texttt{base} and executing \texttt{include({\textquotedbl}compiler/compiler.jl{\textquotedbl})}. This trick typically leads to much faster development than if you rebuild Julia for each change.



Alternatively, you can use the \href{https://github.com/timholy/Revise.jl}{Revise.jl} package to track the compiler changes by using the command \texttt{Revise.track(Core.Compiler)} at the beginning of your Julia session. As explained in the \href{https://timholy.github.io/Revise.jl/stable/}{Revise documentation}, the modifications to the compiler will be reflected when the modified files are saved.



A convenient entry point into inference is \texttt{typeinf\_code}. Here{\textquotesingle}s a demo running inference on \texttt{convert(Int, UInt(1))}:




\begin{minted}{julia}
# Get the method
atypes = Tuple{Type{Int}, UInt}  # argument types
mths = methods(convert, atypes)  # worth checking that there is only one
m = first(mths)

# Create variables needed to call `typeinf_code`
interp = Core.Compiler.NativeInterpreter()
sparams = Core.svec()      # this particular method doesn't have type-parameters
optimize = true            # run all inference optimizations
types = Tuple{typeof(convert), atypes.parameters...} # Tuple{typeof(convert), Type{Int}, UInt}
Core.Compiler.typeinf_code(interp, m, types, sparams, optimize)
\end{minted}



If your debugging adventures require a \texttt{MethodInstance}, you can look it up by calling \texttt{Core.Compiler.specialize\_method} using many of the variables above. A \texttt{CodeInfo} object may be obtained with




\begin{minted}{julia}
# Returns the CodeInfo object for `convert(Int, ::UInt)`:
ci = (@code_typed convert(Int, UInt(1)))[1]
\end{minted}



\hypertarget{3650644351684738912}{}


\subsection{The inlining algorithm (\texttt{inline\_worthy})}



Much of the hardest work for inlining runs in \texttt{ssa\_inlining\_pass!}. However, if your question is {\textquotedbl}why didn{\textquotesingle}t my function inline?{\textquotedbl} then you will most likely be interested in \texttt{inline\_worthy}, which makes a decision to inline the function call or not.



\texttt{inline\_worthy} implements a cost-model, where {\textquotedbl}cheap{\textquotedbl} functions get inlined; more specifically, we inline functions if their anticipated run-time is not large compared to the time it would take to \href{https://en.wikipedia.org/wiki/Calling\_convention}{issue a call} to them if they were not inlined. The cost-model is extremely simple and ignores many important details: for example, all \texttt{for} loops are analyzed as if they will be executed once, and the cost of an \texttt{if...else...end} includes the summed cost of all branches. It{\textquotesingle}s also worth acknowledging that we currently lack a suite of functions suitable for testing how well the cost model predicts the actual run-time cost, although \href{https://github.com/JuliaCI/BaseBenchmarks.jl}{BaseBenchmarks} provides a great deal of indirect information about the successes and failures of any modification to the inlining algorithm.



The foundation of the cost-model is a lookup table, implemented in \texttt{add\_tfunc} and its callers, that assigns an estimated cost (measured in CPU cycles) to each of Julia{\textquotesingle}s intrinsic functions. These costs are based on \href{http://ithare.com/wp-content/uploads/part101\_infographics\_v08.png}{standard ranges for common architectures} (see \href{https://www.agner.org/optimize/instruction\_tables.pdf}{Agner Fog{\textquotesingle}s analysis} for more detail).



We supplement this low-level lookup table with a number of special cases. For example, an \texttt{:invoke} expression (a call for which all input and output types were inferred in advance) is assigned a fixed cost (currently 20 cycles). In contrast, a \texttt{:call} expression, for functions other than intrinsics/builtins, indicates that the call will require dynamic dispatch, in which case we assign a cost set by \texttt{Params.inline\_nonleaf\_penalty} (currently set at \texttt{1000}). Note that this is not a {\textquotedbl}first-principles{\textquotedbl} estimate of the raw cost of dynamic dispatch, but a mere heuristic indicating that dynamic dispatch is extremely expensive.



Each statement gets analyzed for its total cost in a function called \texttt{statement\_cost}. You can display the cost associated with each statement as follows:




\begin{minted}{jlcon}
julia> Base.print_statement_costs(stdout, map, (typeof(sqrt), Tuple{Int},)) # map(sqrt, (2,))
map(f, t::Tuple{Any}) in Base at tuple.jl:179
  0 1 ─ %1  = Base.getfield(_3, 1, true)::Int64
  1 │   %2  = Base.sitofp(Float64, %1)::Float64
  2 │   %3  = Base.lt_float(%2, 0.0)::Bool
  0 └──       goto #3 if not %3
  0 2 ─       invoke Base.Math.throw_complex_domainerror(:sqrt::Symbol, %2::Float64)::Union{}
  0 └──       unreachable
 20 3 ─ %7  = Base.Math.sqrt_llvm(%2)::Float64
  0 └──       goto #4
  0 4 ─       goto #5
  0 5 ─ %10 = Core.tuple(%7)::Tuple{Float64}
  0 └──       return %10
\end{minted}



The line costs are in the left column. This includes the consequences of inlining and other forms of optimization.



\hypertarget{16468279497102584880}{}


\section{Julia SSA-form IR}



\hypertarget{5370755282098388224}{}


\subsection{Background}



Beginning in Julia 0.7, parts of the compiler use a new \href{https://en.wikipedia.org/wiki/Static\_single\_assignment\_form}{SSA-form} intermediate representation. Historically, the compiler would directly generate LLVM IR from a lowered form of the Julia AST. This form had most syntactic abstractions removed, but still looked a lot like an abstract syntax tree. Over time, in order to facilitate optimizations, SSA values were introduced to this IR and the IR was linearized (i.e. turned into a form where function arguments could only be SSA values or constants). However, non-SSA values (slots) remained in the IR due to the lack of Phi nodes in the IR (necessary for back-edges and re-merging of conditional control flow). This negated much of the usefulness of SSA form representation when performing middle end optimizations. Some heroic effort was put into making these optimizations work without a complete SSA form representation, but the lack of such a representation ultimately proved prohibitive.



\hypertarget{14402415240566144810}{}


\subsection{New IR nodes}



With the new IR representation, the compiler learned to handle four new IR nodes, Phi nodes, Pi nodes as well as PhiC nodes and Upsilon nodes (the latter two are only used for exception handling).



\hypertarget{919068206461821671}{}


\subsubsection{Phi nodes and Pi nodes}



Phi nodes are part of generic SSA abstraction (see the link above if you{\textquotesingle}re not familiar with the concept). In the Julia IR, these nodes are represented as:




\begin{lstlisting}
struct PhiNode
    edges::Vector{Int}
    values::Vector{Any}
end
\end{lstlisting}



where we ensure that both vectors always have the same length. In the canonical representation (the one handled by codegen and the interpreter), the edge values indicate come-from statement numbers (i.e. if edge has an entry of \texttt{15}, there must be a \texttt{goto}, \texttt{gotoifnot} or implicit fall through from statement \texttt{15} that targets this phi node). Values are either SSA values or constants. It is also possible for a value to be unassigned if the variable was not defined on this path. However, undefinedness checks get explicitly inserted and represented as booleans after middle end optimizations, so code generators may assume that any use of a Phi node will have an assigned value in the corresponding slot. It is also legal for the mapping to be incomplete, i.e. for a Phi node to have missing incoming edges. In that case, it must be dynamically guaranteed that the corresponding value will not be used.



PiNodes encode statically proven information that may be implicitly assumed in basic blocks dominated by a given pi node. They are conceptually equivalent to the technique introduced in the paper \href{https://dl.acm.org/citation.cfm?id=358438.349342}{ABCD: Eliminating Array Bounds Checks on Demand} or the predicate info nodes in LLVM. To see how they work, consider, e.g.




\begin{minted}{julia}
%x::Union{Int, Float64} # %x is some Union{Int, Float64} typed ssa value
if isa(x, Int)
    # use x
else
    # use x
end
\end{minted}



We can perform predicate insertion and turn this into:




\begin{minted}{julia}
%x::Union{Int, Float64} # %x is some Union{Int, Float64} typed ssa value
if isa(x, Int)
    %x_int = PiNode(x, Int)
    # use %x_int
else
    %x_float = PiNode(x, Float64)
    # use %x_float
end
\end{minted}



Pi nodes are generally ignored in the interpreter, since they don{\textquotesingle}t have any effect on the values, but they may sometimes lead to code generation in the compiler (e.g. to change from an implicitly union split representation to a plain unboxed representation). The main usefulness of PiNodes stems from the fact that path conditions of the values can be accumulated simply by def-use chain walking that is generally done for most optimizations that care about these conditions anyway.



\hypertarget{1498305755051877944}{}


\subsubsection{PhiC nodes and Upsilon nodes}



Exception handling complicates the SSA story moderately, because exception handling introduces additional control flow edges into the IR across which values must be tracked. One approach to do so, which is followed by LLVM, is to make calls which may throw exceptions into basic block terminators and add an explicit control flow edge to the catch handler:




\begin{lstlisting}
invoke @function_that_may_throw() to label %regular unwind to %catch

regular:
# Control flow continues here

catch:
# Exceptions go here
\end{lstlisting}



However, this is problematic in a language like Julia, where at the start of the optimization pipeline, we do not know which calls throw. We would have to conservatively assume that every call (which in Julia is every statement) throws. This would have several negative effects. On the one hand, it would essentially reduce the scope of every basic block to a single call, defeating the purpose of having operations be performed at the basic block level. On the other hand, every catch basic block would have \texttt{n*m} phi node arguments (\texttt{n}, the number of statements in the critical region, \texttt{m} the number of live values through the catch block).



To work around this, we use a combination of \texttt{Upsilon} and \texttt{PhiC} nodes (the C standing for \texttt{catch}, written \texttt{φᶜ} in the IR pretty printer, because unicode subscript c is not available). There are several ways to think of these nodes, but perhaps the easiest is to think of each \texttt{PhiC} as a load from a unique store-many, read-once slot, with \texttt{Upsilon} being the corresponding store operation. The \texttt{PhiC} has an operand list of all the upsilon nodes that store to its implicit slot. The \texttt{Upsilon} nodes however, do not record which \texttt{PhiC} node they store to. This is done for more natural integration with the rest of the SSA IR. E.g. if there are no more uses of a \texttt{PhiC} node, it is safe to delete it, and the same is true of an \texttt{Upsilon} node. In most IR passes, \texttt{PhiC} nodes can be treated like \texttt{Phi} nodes. One can follow use-def chains through them, and they can be lifted to new \texttt{PhiC} nodes and new \texttt{Upsilon} nodes (in the same places as the original \texttt{Upsilon} nodes). The result of this scheme is that the number of \texttt{Upsilon} nodes (and \texttt{PhiC} arguments) is proportional to the number of assigned values to a particular variable (before SSA conversion), rather than the number of statements in the critical region.



To see this scheme in action, consider the function




\begin{minted}{julia}
@noinline opaque() = invokelatest(identity, nothing) # Something opaque
function foo()
    local y
    x = 1
    try
        y = 2
        opaque()
        y = 3
        error()
    catch
    end
    (x, y)
end
\end{minted}



The corresponding IR (with irrelevant types stripped) is:




\begin{lstlisting}
1 ─       nothing::Nothing
2 ─ %2  = $(Expr(:enter, #4))
3 ─ %3  = ϒ (false)
│   %4  = ϒ (#undef)
│   %5  = ϒ (1)
│   %6  = ϒ (true)
│   %7  = ϒ (2)
│         invoke Main.opaque()::Any
│   %9  = ϒ (true)
│   %10 = ϒ (3)
│         invoke Main.error()::Union{}
└──       $(Expr(:unreachable))::Union{}
4 ┄ %13 = φᶜ (%3, %6, %9)::Bool
│   %14 = φᶜ (%4, %7, %10)::Core.Compiler.MaybeUndef(Int64)
│   %15 = φᶜ (%5)::Core.Const(1)
└──       $(Expr(:leave, 1))
5 ─       $(Expr(:pop_exception, :(%2)))::Any
│         $(Expr(:throw_undef_if_not, :y, :(%13)))::Any
│   %19 = Core.tuple(%15, %14)
└──       return %19
\end{lstlisting}



Note in particular that every value live into the critical region gets an upsilon node at the top of the critical region. This is because catch blocks are considered to have an invisible control flow edge from outside the function. As a result, no SSA value dominates the catch blocks, and all incoming values have to come through a \texttt{φᶜ} node.



\hypertarget{13190851783504287053}{}


\subsection{Main SSA data structure}



The main \texttt{SSAIR} data structure is worthy of discussion. It draws inspiration from LLVM and Webkit{\textquotesingle}s B3 IR. The core of the data structure is a flat vector of statements. Each statement is implicitly assigned an SSA value based on its position in the vector (i.e. the result of the statement at idx 1 can be accessed using \texttt{SSAValue(1)} etc). For each SSA value, we additionally maintain its type. Since, SSA values are definitionally assigned only once, this type is also the result type of the expression at the corresponding index. However, while this representation is rather efficient (since the assignments don{\textquotesingle}t need to be explicitly encoded), it of course carries the drawback that order is semantically significant, so reorderings and insertions change statement numbers. Additionally, we do not keep use lists (i.e. it is impossible to walk from a def to all its uses without explicitly computing this map–def lists however are trivial since you can look up the corresponding statement from the index), so the LLVM-style RAUW (replace-all-uses-with) operation is unavailable.



Instead, we do the following:



\begin{itemize}
\item We keep a separate buffer of nodes to insert (including the position to insert them at, the type of the corresponding value and the node itself). These nodes are numbered by their occurrence in the insertion buffer, allowing their values to be immediately used elsewhere in the IR (i.e. if there are 12 statements in the original statement list, the first new statement will be accessible as \texttt{SSAValue(13)}).


\item RAUW style operations are performed by setting the corresponding statement index to the replacement value.


\item Statements are erased by setting the corresponding statement to \texttt{nothing} (this is essentially just a special-case convention of the above.


\item If there are any uses of the statement being erased, they will be set to \texttt{nothing}.

\end{itemize}


There is a \texttt{compact!} function that compacts the above data structure by performing the insertion of nodes in the appropriate place, trivial copy propagation, and renaming of uses to any changed SSA values. However, the clever part of this scheme is that this compaction can be done lazily as part of the subsequent pass. Most optimization passes need to walk over the entire list of statements, performing analysis or modifications along the way. We provide an \texttt{IncrementalCompact} iterator that can be used to iterate over the statement list. It will perform any necessary compaction and return the new index of the node, as well as the node itself. It is legal at this point to walk def-use chains, as well as make any modifications or deletions to the IR (insertions are disallowed however).



The idea behind this arrangement is that, since the optimization passes need to touch the corresponding memory anyway and incur the corresponding memory access penalty, performing the extra housekeeping should have comparatively little overhead (and save the overhead of maintaining these data structures during IR modification).



\hypertarget{3374914057216712271}{}


\section{Static analyzer annotations for GC correctness in C code}



\hypertarget{10926948174874297988}{}


\subsection{Running the analysis}



The analyzer plugin that drives the anlysis ships with julia. Its source code can be found in \texttt{src/clangsa}. Running it requires the clang dependency to be build. Set the \texttt{BUILD\_LLVM\_CLANG} variable in your Make.user in order to build an appropriate version of clang. You may also want to use the prebuilt binaries using the \texttt{USE\_BINARYBUILDER\_LLVM} options. Afterwards, running the analysis over the source tree is as simple as running \texttt{make -C src analyzegc}.



\hypertarget{9968293705752668266}{}


\subsection{General Overview}



Since Julia{\textquotesingle}s GC is precise, it needs to maintain correct rooting information for any value that may be referenced at any time GC may occur. These places are known as \texttt{safepoints} and in the function local context, we extend this designation to any function call that may recursively end up at a safepoint.



In generated code, this is taken care of automatically by the GC root placement pass (see the chapter on GC rooting in the LLVM codegen devdocs). However, in C code, we need to inform the runtime of any GC roots manually. This is done using the following macros:




\begin{lstlisting}
// The value assigned to any slot passed as an argument to these
// is rooted for the duration of this GC frame.
JL_GC_PUSH{1,...,6}(args...)
// The values assigned into the size `n` array `rts` are rooted
// for the duration of this GC frame.
JL_GC_PUSHARGS(rts, n)
// Pop a GC frame
JL_GC_POP
\end{lstlisting}



If these macros are not used where they need to be, or they are used incorrectly, the result is silent memory corruption. As such it is very important that they are placed correctly in all applicable code.



As such, we employ static analysis (and in particular the clang static analyzer) to help ensure that these macros are used correctly. The remainder of this document gives an overview of this static analysis and describes the support needed in the julia code base to make things work.



\hypertarget{15061338110065038518}{}


\subsection{GC Invariants}



There is two simple invariants correctness:



\begin{itemize}
\item All \texttt{GC\_PUSH} calls need to be followed by an appropriate \texttt{GC\_POP} (in practice we enforce this at the function level)


\item If a value was previously not rooted at any safepoint, it may no longer be referenced afterwards

\end{itemize}


Of course the devil is in the details here. In particular to satisfy the second of the above conditions, we need to know:



\begin{itemize}
\item Which calls are safepoints and which are not


\item Which values are rooted at any given safepoint and which are not


\item When is a value referenced

\end{itemize}


For the second point in particular, we need to know which memory locations will be considered rooting at runtime (i.e. values assigned to such locations are rooted). This includes locations explicitly designated as such by passing them to one of the \texttt{GC\_PUSH} macros, globally rooted locations and values, as well as any location recursively reachable from one of those locations.



\hypertarget{7498210263575228495}{}


\subsection{Static Analysis Algorithm}



The idea itself is very simple, although the implementation is quite a bit more complicated (mainly due to a large number of special cases and intricacies of C and C++). In essence, we keep track of all locations that are rooting, all values that are rootable and any expression (assignments, allocations, etc) affect the rootedness of any rootable values. Then, at any safepoint, we perform a {\textquotedbl}symbolic GC{\textquotedbl} and poison any values that are not rooted at said location. If these values are later referenced, we emit an error.



The clang static analyzer works by constructing a graph of states and exploring this graph for sources of errors. Several nodes in this graph are generated by the analyzer itself (e.g. for control flow), but the definitions above augment this graph with our own state.



The static analyzer is interprocedural and can analyze control flow across function boundaries. However, the static analyzer is not fully recursive and makes heuristic decisions about which calls to explore (additionally some calls are cross-translation unit and invisible to the analyzer). In our case, our definition of correctness requires total information. As such, we need to annotate the prototypes of all function calls with whatever information the analysis required, even if that information would otherwise be available by interprocedural static analysis.



Luckily however, we can still use this interprocedural analysis to ensure that the annotations we place on a given function are indeed correct given the implementation of said function.



\hypertarget{5382015011410468170}{}


\subsection{The analyzer annotations}



These annotations are found in src/support/analyzer\_annotations.h. The are only active when the analyzer is being used and expand either to nothing (for prototype annotations) or to no-ops (for function like annotations).



\hypertarget{4716575215472782777}{}


\subsubsection{\texttt{JL\_NOTSAFEPOINT}}



This is perhaps the most common annotation, and should be placed on any function that is known not to possibly lead to reaching a GC safepoint. In general, it is only safe for such a function to perform arithmetic, memory accesses and calls to functions either annotated \texttt{JL\_NOTSAFEPOINT} or otherwise known not to be safepoints (e.g. function in the C standard library, which are hardcoded as such in the analyzer)



It is valid to keep values unrooted across calls to any function annotated with this attribute:



Usage Example:




\begin{lstlisting}
void jl_get_one() JL_NOTSAFEPOINT {
  return 1;
}

jl_value_t *example() {
  jl_value_t *val = jl_alloc_whatever();
  // This is valid, even though `val` is unrooted, because
  // jl_get_one is not a safepoint
  jl_get_one();
  return val;
}
\end{lstlisting}



\hypertarget{2060328680206426649}{}


\subsubsection{\texttt{JL\_MAYBE\_UNROOTED}/\texttt{JL\_ROOTS\_TEMPORARILY}}



When \texttt{JL\_MAYBE\_UNROOTED} is annotated as an argument on a function, indicates that said argument may be passed, even if it is not rooted. In the ordinary course of events, the julia ABI guarantees that callers root values before passing them to callees. However, some functions do not follow this ABI and allow values to be passed to them even though they are not rooted. Note however, that this does not automatically imply that said argument will be preserved. The \texttt{ROOTS\_TEMPORARILY} annotation provides the stronger guarantee that, not only may the value be unrooted when passed, it will also be preserved across any internal safepoints by the callee.



Note that \texttt{JL\_NOTSAFEPOINT} essentially implies \texttt{JL\_MAYBE\_UNROOTED}/\texttt{JL\_ROOTS\_TEMPORARILY}, because the rootedness of an argument is irrelevant if the function contains no safepoints.



One additional point to note is that these annotations apply on both the caller and the callee side. On the caller side, they lift rootedness restrictions that are normally required for julia ABI functions. On the callee side, they have the reverse effect of preventing these arguments from being considered implicitly rooted.



If either of these annotations is applied to the function as a whole, it applies to all arguments of the function. This should generally only be necessary for varargs functions.



Usage example:




\begin{lstlisting}
JL_DLLEXPORT void JL_NORETURN jl_throw(jl_value_t *e JL_MAYBE_UNROOTED);
jl_value_t *jl_alloc_error();

void example() {
  // The return value of the allocation is unrooted. This would normally
  // be an error, but is allowed because of the above annotation.
  jl_throw(jl_alloc_error());
}
\end{lstlisting}



\hypertarget{8714501739065873603}{}


\subsubsection{\texttt{JL\_PROPAGATES\_ROOT}}



This annotation is commonly found on accessor functions that return one rootable object stored within another. When annotated on a function argument, it tells the analyzer that the root for that argument also applies to the value returned by the function.



Usage Example:




\begin{lstlisting}
jl_value_t *jl_svecref(jl_svec_t *t JL_PROPAGATES_ROOT, size_t i) JL_NOTSAFEPOINT;

size_t example(jl_svec_t *svec) {
  jl_value_t *val = jl_svecref(svec, 1)
  // This is valid, because, as annotated by the PROPAGATES_ROOT annotation,
  // jl_svecref propagates the rooted-ness from `svec` to `val`
  jl_gc_safepoint();
  return jl_unbox_long(val);
}
\end{lstlisting}



\hypertarget{78198301944071640}{}


\subsubsection{\texttt{JL\_ROOTING\_ARGUMENT}/\texttt{JL\_ROOTED\_ARGUMENT}}



This is essentially the assignment counterpart to \texttt{JL\_PROPAGATES\_ROOT}. When assigning a value to a field of another value that is already rooted, the assigned value will inherit the root of the value it is assigned into.



Usage Example:




\begin{lstlisting}
void jl_svecset(void *t JL_ROOTING_ARGUMENT, size_t i, void *x JL_ROOTED_ARGUMENT) JL_NOTSAFEPOINT


size_t example(jl_svec_t *svec) {
  jl_value_t *val = jl_box_long(10000);
  jl_svecset(svec, val);
  // This is valid, because the annotations imply that the
  // jl_svecset propagates the rooted-ness from `svec` to `val`
  jl_gc_safepoint();
  return jl_unbox_long(val);
}
\end{lstlisting}



\hypertarget{9078058200292228414}{}


\subsubsection{\texttt{JL\_GC\_DISABLED}}



This annotation implies that this function is only called with the GC runtime-disabled. Functions of this kind are most often encountered during startup and in the GC code itself. Note that this annotation is checked against the runtime enable/disable calls, so clang will know if you lie. This is not a good way to disable processing of a given function if the GC is not actually disabled (use \texttt{ifdef \_\_clang\_analyzer\_\_} for that if you must).



Usage example:




\begin{lstlisting}
void jl_do_magic() JL_GC_DISABLED {
  // Wildly allocate here with no regard for roots
}

void example() {
  int en = jl_gc_enable(0);
  jl_do_magic();
  jl_gc_enable(en);
}
\end{lstlisting}



\hypertarget{14120380498342272328}{}


\subsubsection{\texttt{JL\_REQUIRE\_ROOTED\_SLOT}}



This annotation requires the caller to pass in a slot that is rooted (i.e. values assigned to this slot will be rooted).



Usage example:




\begin{lstlisting}
void jl_do_processing(jl_value_t **slot JL_REQUIRE_ROOTED_SLOT) {
  *slot = jl_box_long(1);
  // Ok, only, because the slot was annotated as rooting
  jl_gc_safepoint();
}

void example() {
  jl_value_t *slot = NULL;
  JL_GC_PUSH1(&slot);
  jl_do_processing(&slot);
  JL_GC_POP();
}
\end{lstlisting}



\hypertarget{11329720779284449718}{}


\subsubsection{\texttt{JL\_GLOBALLY\_ROOTED}}



This annotation implies that a given value is always globally rooted. It can be applied to global variable declarations, in which case it will apply to the value of those variables (or values if the declaration if for an array), or to functions, in which case it will apply to the return value of such functions (e.g. for functions that always return some private, globally rooted value).



Usage example:




\begin{lstlisting}
extern JL_DLLEXPORT jl_datatype_t *jl_any_type JL_GLOBALLY_ROOTED;
jl_ast_context_t *jl_ast_ctx(fl_context_t *fl) JL_GLOBALLY_ROOTED;
\end{lstlisting}



\hypertarget{503057295712361449}{}


\subsubsection{\texttt{JL\_ALWAYS\_LEAFTYPE}}



This annotations is essentially equivalent to \texttt{JL\_GLOBALLY\_ROOTED}, except that is should only be used if those values are globally rooted by virtue of being a leaftype. The rooting of leaftypes is a bit complicated. They are generally rooted through \texttt{cache} field of the corresponding \texttt{TypeName}, which itself is rooted by the containing module (so they{\textquotesingle}re rooted as long as the containing module is ok) and we can generally assume that leaftypes are rooted where they are used, but we may refine this property in the future, so the separate annotation helps split out the reason for being globally rooted.



The analyzer also automatically detects checks for leaftype-ness and will not complain about missing GC roots on these paths.




\begin{lstlisting}
JL_DLLEXPORT jl_value_t *jl_apply_array_type(jl_value_t *type, size_t dim) JL_ALWAYS_LEAFTYPE;
\end{lstlisting}



\hypertarget{15063650179417682020}{}


\subsubsection{\texttt{JL\_GC\_PROMISE\_ROOTED}}



This is a function-like annotation. Any value passed to this annotation will be considered rooted for the scope of the current function. It is designed as an escape hatch for analyzer inadequacy or complicated situations. However, it should be used sparingly, in favor of improving the analyzer itself.




\begin{lstlisting}
void example() {
  jl_value_t *val = jl_alloc_something();
  if (some_condition) {
    // We happen to know for complicated external reasons
    // that val is rooted under these conditions
    JL_GC_PROMISE_ROOTED(val);
  }
}
\end{lstlisting}



\hypertarget{18107261293830917313}{}


\subsection{Completeness of analysis}



The analyzer only looks at local information. In particular, e.g. in the \texttt{PROPAGATES\_ROOT} case above, it assumes that such memory is only modified in ways it can see, not in any called functions (unless it happens to decide to consider them in its analysis) and not in any concurrently running threads. As such, it may miss a few problematic cases, though in practice such concurrent modification is fairly rare. Improving the analyzer to handle more such cases may be an interesting topic for future work.



\chapter{Developing/debugging Julia's C code}


\hypertarget{11376306494480499528}{}


\section{Reporting and analyzing crashes (segfaults)}



So you managed to break Julia.  Congratulations!  Collected here are some general procedures you can undergo for common symptoms encountered when something goes awry.  Including the information from these debugging steps can greatly help the maintainers when tracking down a segfault or trying to figure out why your script is running slower than expected.



If you{\textquotesingle}ve been directed to this page, find the symptom that best matches what you{\textquotesingle}re experiencing and follow the instructions to generate the debugging information requested.  Table of symptoms:



\begin{itemize}
\item \hyperlink{13671941627037387928}{Segfaults during bootstrap (\texttt{sysimg.jl})}


\item \hyperlink{17238593239204343556}{Segfaults when running a script}


\item \hyperlink{15906783386188107842}{Errors during Julia startup}


\item \hyperlink{9648352711765890842}{Other generic segfaults or unreachables reached}

\end{itemize}


\hypertarget{13046485394122703550}{}


\subsection{Version/Environment info}



No matter the error, we will always need to know what version of Julia you are running. When Julia first starts up, a header is printed out with a version number and date. Please also include the output of \texttt{versioninfo()} (exported from the \hyperlink{11698106121547091928}{\texttt{InteractiveUtils}} standard library) in any report you create:




\begin{minted}{jlcon}
julia> using InteractiveUtils


julia> versioninfo()
Julia Version 1.8.0-DEV.1223
Commit 2d472c633d* (2022-01-06 07:43 UTC)
Platform Info:
  OS: Windows (x86_64-w64-mingw32)
  CPU: Intel(R) Core(TM) i5-9400F CPU @ 2.90GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-12.0.1 (ORCJIT, skylake)
\end{minted}



\hypertarget{3553581061539675492}{}


\subsection{Segfaults during bootstrap (\texttt{sysimg.jl})}



Segfaults toward the end of the \texttt{make} process of building Julia are a common symptom of something going wrong while Julia is preparsing the corpus of code in the \texttt{base/} folder.  Many factors can contribute toward this process dying unexpectedly, however it is as often as not due to an error in the C-code portion of Julia, and as such must typically be debugged with a debug build inside of \texttt{gdb}.  Explicitly:



Create a debug build of Julia:




\begin{lstlisting}
$ cd <julia_root>
$ make debug
\end{lstlisting}



Note that this process will likely fail with the same error as a normal \texttt{make} incantation, however this will create a debug executable that will offer \texttt{gdb} the debugging symbols needed to get accurate backtraces.  Next, manually run the bootstrap process inside of \texttt{gdb}:




\begin{lstlisting}
$ cd base/
$ gdb -x ../contrib/debug_bootstrap.gdb
\end{lstlisting}



This will start \texttt{gdb}, attempt to run the bootstrap process using the debug build of Julia, and print out a backtrace if (when) it segfaults.  You may need to hit \texttt{<enter>} a few times to get the full backtrace.  Create a \href{https://gist.github.com}{gist} with the backtrace, the \hyperlink{4601189142062189569}{version info}, and any other pertinent information you can think of and open a new \href{https://github.com/JuliaLang/julia/issues?q=is\%3Aopen}{issue} on Github with a link to the gist.



\hypertarget{17337880191991868567}{}


\subsection{Segfaults when running a script}



The procedure is very similar to \hyperlink{13671941627037387928}{Segfaults during bootstrap (\texttt{sysimg.jl})}.  Create a debug build of Julia, and run your script inside of a debugged Julia process:




\begin{lstlisting}
$ cd <julia_root>
$ make debug
$ gdb --args usr/bin/julia-debug <path_to_your_script>
\end{lstlisting}



Note that \texttt{gdb} will sit there, waiting for instructions.  Type \texttt{r} to run the process, and \texttt{bt} to generate a backtrace once it segfaults:




\begin{lstlisting}
(gdb) r
Starting program: /home/sabae/src/julia/usr/bin/julia-debug ./test.jl
...
(gdb) bt
\end{lstlisting}



Create a \href{https://gist.github.com}{gist} with the backtrace, the \hyperlink{4601189142062189569}{version info}, and any other pertinent information you can think of and open a new \href{https://github.com/JuliaLang/julia/issues?q=is\%3Aopen}{issue} on Github with a link to the gist.



\hypertarget{8599391806074935874}{}


\subsection{Errors during Julia startup}



Occasionally errors occur during Julia{\textquotesingle}s startup process (especially when using binary distributions, as opposed to compiling from source) such as the following:




\begin{minted}{julia}
$ julia
exec: error -5
\end{minted}



These errors typically indicate something is not getting loaded properly very early on in the bootup phase, and our best bet in determining what{\textquotesingle}s going wrong is to use external tools to audit the disk activity of the \texttt{julia} process:



\begin{itemize}
\item On Linux, use \texttt{strace}:


\begin{lstlisting}
$ strace julia
\end{lstlisting}


\item On OSX, use \texttt{dtruss}:


\begin{lstlisting}
$ dtruss -f julia
\end{lstlisting}

\end{itemize}


Create a \href{https://gist.github.com}{gist} with the \texttt{strace}/ \texttt{dtruss} output, the \hyperlink{4601189142062189569}{version info}, and any other pertinent information and open a new \href{https://github.com/JuliaLang/julia/issues?q=is\%3Aopen}{issue} on Github with a link to the gist.



\hypertarget{15486738137712275027}{}


\subsection{Other generic segfaults or unreachables reached}



As mentioned elsewhere, \texttt{julia} has good integration with \texttt{rr} for generating traces; this includes, on Linux, the ability to automatically run \texttt{julia} under \texttt{rr} and share the trace after a crash. This can be immensely helpful when debugging such crashes and is strongly encouraged when reporting crash issues to the JuliaLang/julia repo. To run \texttt{julia} under \texttt{rr} automatically, do:




\begin{minted}{julia}
julia --bug-report=rr
\end{minted}



To generate the \texttt{rr} trace locally, but not share, you can do:




\begin{minted}{julia}
julia --bug-report=rr-local
\end{minted}



Note that this is only works on Linux. The blog post on \href{https://julialang.org/blog/2020/05/rr/}{Time Travelling Bug Reporting} has many more details.



\hypertarget{15861385441225766089}{}


\subsection{Glossary}



A few terms have been used as shorthand in this guide:



\begin{itemize}
\item \texttt{<julia\_root>} refers to the root directory of the Julia source tree; e.g. it should contain folders such as \texttt{base}, \texttt{deps}, \texttt{src}, \texttt{test}, etc.....

\end{itemize}


\hypertarget{9363494011247086183}{}


\section{gdb debugging tips}



\hypertarget{7470105576182008459}{}


\subsection{Displaying Julia variables}



Within \texttt{gdb}, any \texttt{jl\_value\_t*} object \texttt{obj} can be displayed using




\begin{lstlisting}
(gdb) call jl_(obj)
\end{lstlisting}



The object will be displayed in the \texttt{julia} session, not in the gdb session. This is a useful way to discover the types and values of objects being manipulated by Julia{\textquotesingle}s C code.



Similarly, if you{\textquotesingle}re debugging some of Julia{\textquotesingle}s internals (e.g., \texttt{compiler.jl}), you can print \texttt{obj} using




\begin{minted}{julia}
ccall(:jl_, Cvoid, (Any,), obj)
\end{minted}



This is a good way to circumvent problems that arise from the order in which julia{\textquotesingle}s output streams are initialized.



Julia{\textquotesingle}s flisp interpreter uses \texttt{value\_t} objects; these can be displayed with \texttt{call fl\_print(fl\_ctx, ios\_stdout, obj)}.



\hypertarget{13194055767125353293}{}


\subsection{Useful Julia variables for Inspecting}



While the addresses of many variables, like singletons, can be useful to print for many failures, there are a number of additional variables (see \texttt{julia.h} for a complete list) that are even more useful.



\begin{itemize}
\item (when in \texttt{jl\_apply\_generic}) \texttt{mfunc} and \texttt{jl\_uncompress\_ast(mfunc->def, mfunc->code)} :: for figuring out a bit about the call-stack


\item \texttt{jl\_lineno} and \texttt{jl\_filename} :: for figuring out what line in a test to go start debugging from (or figure out how far into a file has been parsed)


\item \texttt{\$1} :: not really a variable, but still a useful shorthand for referring to the result of the last gdb command (such as \texttt{print})


\item \texttt{jl\_options} :: sometimes useful, since it lists all of the command line options that were successfully parsed


\item \texttt{jl\_uv\_stderr} :: because who doesn{\textquotesingle}t like to be able to interact with stdio

\end{itemize}


\hypertarget{7178558395386377758}{}


\subsection{Useful Julia functions for Inspecting those variables}



\begin{itemize}
\item \texttt{jl\_gdblookup(\$rip)} :: For looking up the current function and line. (use \texttt{\$eip} on i686 platforms)


\item \texttt{jlbacktrace()} :: For dumping the current Julia backtrace stack to stderr. Only usable after \texttt{record\_backtrace()} has been called.


\item \texttt{jl\_dump\_llvm\_value(Value*)} :: For invoking \texttt{Value->dump()} in gdb, where it doesn{\textquotesingle}t work natively. For example, \texttt{f->linfo->functionObject}, \texttt{f->linfo->specFunctionObject}, and \texttt{to\_function(f->linfo)}.


\item \texttt{Type->dump()} :: only works in lldb. Note: add something like \texttt{;1} to prevent lldb from printing its prompt over the output


\item \texttt{jl\_eval\_string({\textquotedbl}expr{\textquotedbl})} :: for invoking side-effects to modify the current state or to lookup symbols


\item \texttt{jl\_typeof(jl\_value\_t*)} :: for extracting the type tag of a Julia value (in gdb, call \texttt{macro define jl\_typeof jl\_typeof} first, or pick something short like \texttt{ty} for the first arg to define a shorthand)

\end{itemize}


\hypertarget{1392415358123037898}{}


\subsection{Inserting breakpoints for inspection from gdb}



In your \texttt{gdb} session, set a breakpoint in \texttt{jl\_breakpoint} like so:




\begin{lstlisting}
(gdb) break jl_breakpoint
\end{lstlisting}



Then within your Julia code, insert a call to \texttt{jl\_breakpoint} by adding




\begin{minted}{julia}
ccall(:jl_breakpoint, Cvoid, (Any,), obj)
\end{minted}



where \texttt{obj} can be any variable or tuple you want to be accessible in the breakpoint.



It{\textquotesingle}s particularly helpful to back up to the \texttt{jl\_apply} frame, from which you can display the arguments to a function using, e.g.,




\begin{lstlisting}
(gdb) call jl_(args[0])
\end{lstlisting}



Another useful frame is \texttt{to\_function(jl\_method\_instance\_t *li, bool cstyle)}. The \texttt{jl\_method\_instance\_t*} argument is a struct with a reference to the final AST sent into the compiler. However, the AST at this point will usually be compressed; to view the AST, call \texttt{jl\_uncompress\_ast} and then pass the result to \texttt{jl\_}:




\begin{lstlisting}
#2  0x00007ffff7928bf7 in to_function (li=0x2812060, cstyle=false) at codegen.cpp:584
584          abort();
(gdb) p jl_(jl_uncompress_ast(li, li->ast))
\end{lstlisting}



\hypertarget{10952602490249170772}{}


\subsection{Inserting breakpoints upon certain conditions}



\hypertarget{796097711199942153}{}


\subsubsection{Loading a particular file}



Let{\textquotesingle}s say the file is \texttt{sysimg.jl}:




\begin{lstlisting}
(gdb) break jl_load if strcmp(fname, "sysimg.jl")==0
\end{lstlisting}



\hypertarget{11262037379695434792}{}


\subsubsection{Calling a particular method}




\begin{lstlisting}
(gdb) break jl_apply_generic if strcmp((char*)(jl_symbol_name)(jl_gf_mtable(F)->name), "method_to_break")==0
\end{lstlisting}



Since this function is used for every call, you will make everything 1000x slower if you do this.



\hypertarget{12553217263049394878}{}


\subsection{Dealing with signals}



Julia requires a few signals to function properly. The profiler uses \texttt{SIGUSR2} for sampling and the garbage collector uses \texttt{SIGSEGV} for threads synchronization. If you are debugging some code that uses the profiler or multiple threads, you may want to let the debugger ignore these signals since they can be triggered very often during normal operations. The command to do this in GDB is (replace \texttt{SIGSEGV} with \texttt{SIGUSR2} or other signals you want to ignore):




\begin{lstlisting}
(gdb) handle SIGSEGV noprint nostop pass
\end{lstlisting}



The corresponding LLDB command is (after the process is started):




\begin{lstlisting}
(lldb) pro hand -p true -s false -n false SIGSEGV
\end{lstlisting}



If you are debugging a segfault with threaded code, you can set a breakpoint on \texttt{jl\_critical\_error} (\texttt{sigdie\_handler} should also work on Linux and BSD) in order to only catch the actual segfault rather than the GC synchronization points.



\hypertarget{8144212274800572894}{}


\subsection{Debugging during Julia{\textquotesingle}s build process (bootstrap)}



Errors that occur during \texttt{make} need special handling. Julia is built in two stages, constructing \texttt{sys0} and \texttt{sys.ji}. To see what commands are running at the time of failure, use \texttt{make VERBOSE=1}.



At the time of this writing, you can debug build errors during the \texttt{sys0} phase from the \texttt{base} directory using:




\begin{lstlisting}
julia/base$ gdb --args ../usr/bin/julia-debug -C native --build ../usr/lib/julia/sys0 sysimg.jl
\end{lstlisting}



You might need to delete all the files in \texttt{usr/lib/julia/} to get this to work.



You can debug the \texttt{sys.ji} phase using:




\begin{lstlisting}
julia/base$ gdb --args ../usr/bin/julia-debug -C native --build ../usr/lib/julia/sys -J ../usr/lib/julia/sys0.ji sysimg.jl
\end{lstlisting}



By default, any errors will cause Julia to exit, even under gdb. To catch an error {\textquotedbl}in the act{\textquotedbl}, set a breakpoint in \texttt{jl\_error} (there are several other useful spots, for specific kinds of failures, including: \texttt{jl\_too\_few\_args}, \texttt{jl\_too\_many\_args}, and \texttt{jl\_throw}).



Once an error is caught, a useful technique is to walk up the stack and examine the function by inspecting the related call to \texttt{jl\_apply}. To take a real-world example:




\begin{lstlisting}
Breakpoint 1, jl_throw (e=0x7ffdf42de400) at task.c:802
802 {
(gdb) p jl_(e)
ErrorException("auto_unbox: unable to determine argument type")
$2 = void
(gdb) bt 10
#0  jl_throw (e=0x7ffdf42de400) at task.c:802
#1  0x00007ffff65412fe in jl_error (str=0x7ffde56be000 <_j_str267> "auto_unbox:
   unable to determine argument type")
   at builtins.c:39
#2  0x00007ffde56bd01a in julia_convert_16886 ()
#3  0x00007ffff6541154 in jl_apply (f=0x7ffdf367f630, args=0x7fffffffc2b0, nargs=2) at julia.h:1281
...
\end{lstlisting}



The most recent \texttt{jl\_apply} is at frame \#3, so we can go back there and look at the AST for the function \texttt{julia\_convert\_16886}. This is the uniqued name for some method of \texttt{convert}. \texttt{f} in this frame is a \texttt{jl\_function\_t*}, so we can look at the type signature, if any, from the \texttt{specTypes} field:




\begin{lstlisting}
(gdb) f 3
#3  0x00007ffff6541154 in jl_apply (f=0x7ffdf367f630, args=0x7fffffffc2b0, nargs=2) at julia.h:1281
1281            return f->fptr((jl_value_t*)f, args, nargs);
(gdb) p f->linfo->specTypes
$4 = (jl_tupletype_t *) 0x7ffdf39b1030
(gdb) p jl_( f->linfo->specTypes )
Tuple{Type{Float32}, Float64}           # <-- type signature for julia_convert_16886
\end{lstlisting}



Then, we can look at the AST for this function:




\begin{lstlisting}
(gdb) p jl_( jl_uncompress_ast(f->linfo, f->linfo->ast) )
Expr(:lambda, Array{Any, 1}[:#s29, :x], Array{Any, 1}[Array{Any, 1}[], Array{Any, 1}[Array{Any, 1}[:#s29, :Any, 0], Array{Any, 1}[:x, :Any, 0]], Array{Any, 1}[], 0], Expr(:body,
Expr(:line, 90, :float.jl)::Any,
Expr(:return, Expr(:call, :box, :Float32, Expr(:call, :fptrunc, :Float32, :x)::Any)::Any)::Any)::Any)::Any
\end{lstlisting}



Finally, and perhaps most usefully, we can force the function to be recompiled in order to step through the codegen process. To do this, clear the cached \texttt{functionObject} from the \texttt{jl\_lamdbda\_info\_t*}:




\begin{lstlisting}
(gdb) p f->linfo->functionObject
$8 = (void *) 0x1289d070
(gdb) set f->linfo->functionObject = NULL
\end{lstlisting}



Then, set a breakpoint somewhere useful (e.g. \texttt{emit\_function}, \texttt{emit\_expr}, \texttt{emit\_call}, etc.), and run codegen:




\begin{lstlisting}
(gdb) p jl_compile(f)
... # your breakpoint here
\end{lstlisting}



\hypertarget{15283936980874101721}{}


\subsection{Debugging precompilation errors}



Module precompilation spawns a separate Julia process to precompile each module. Setting a breakpoint or catching failures in a precompile worker requires attaching a debugger to the worker. The easiest approach is to set the debugger watch for new process launches matching a given name. For example:




\begin{lstlisting}
(gdb) attach -w -n julia-debug
\end{lstlisting}



or:




\begin{lstlisting}
(lldb) process attach -w -n julia-debug
\end{lstlisting}



Then run a script/command to start precompilation. As described earlier, use conditional breakpoints in the parent process to catch specific file-loading events and narrow the debugging window. (some operating systems may require alternative approaches, such as following each \texttt{fork} from the parent process)



\hypertarget{2874239207045478266}{}


\subsection{Mozilla{\textquotesingle}s Record and Replay Framework (rr)}



Julia now works out of the box with \href{https://rr-project.org/}{rr}, the lightweight recording and deterministic debugging framework from Mozilla. This allows you to replay the trace of an execution deterministically.  The replayed execution{\textquotesingle}s address spaces, register contents, syscall data etc are exactly the same in every run.



A recent version of rr (3.1.0 or higher) is required.



\hypertarget{8199500901675456324}{}


\subsubsection{Reproducing concurrency bugs with rr}



rr simulates a single-threaded machine by default. In order to debug concurrent code you can use \texttt{rr record --chaos} which will cause rr to simulate between one to eight cores, chosen randomly. You might therefore want to set \texttt{JULIA\_NUM\_THREADS=8} and rerun your code under rr until you have caught your bug.



\hypertarget{3182145831135367960}{}


\section{Using Valgrind with Julia}



\href{https://valgrind.org/}{Valgrind} is a tool for memory debugging, memory leak detection, and profiling.  This section describes things to keep in mind when using Valgrind to debug memory issues with Julia.



\hypertarget{527857279218691176}{}


\subsection{General considerations}



By default, Valgrind assumes that there is no self modifying code in the programs it runs.  This assumption works fine in most instances but fails miserably for a just-in-time compiler like \texttt{julia}.  For this reason it is crucial to pass \texttt{--smc-check=all-non-file} to \texttt{valgrind}, else code may crash or behave unexpectedly (often in subtle ways).



In some cases, to better detect memory errors using Valgrind it can help to compile \texttt{julia} with memory pools disabled.  The compile-time flag \texttt{MEMDEBUG} disables memory pools in Julia, and \texttt{MEMDEBUG2} disables memory pools in FemtoLisp.  To build \texttt{julia} with both flags, add the following line to \texttt{Make.user}:




\begin{minted}{julia}
CFLAGS = -DMEMDEBUG -DMEMDEBUG2
\end{minted}



Another thing to note: if your program uses multiple workers processes, it is likely that you want all such worker processes to run under Valgrind, not just the parent process.  To do this, pass \texttt{--trace-children=yes} to \texttt{valgrind}.



Yet another thing to note: if using \texttt{valgrind} errors with \texttt{Unable to find compatible target in system image}, try rebuilding the sysimage with target \texttt{generic} or julia with \texttt{JULIA\_CPU\_TARGET=generic}.



\hypertarget{9183907630008953484}{}


\subsection{Suppressions}



Valgrind will typically display spurious warnings as it runs.  To reduce the number of such warnings, it helps to provide a \href{https://valgrind.org/docs/manual/manual-core.html\#manual-core.suppress}{suppressions file} to Valgrind.  A sample suppressions file is included in the Julia source distribution at \texttt{contrib/valgrind-julia.supp}.



The suppressions file can be used from the \texttt{julia/} source directory as follows:




\begin{lstlisting}
$ valgrind --smc-check=all-non-file --suppressions=contrib/valgrind-julia.supp ./julia progname.jl
\end{lstlisting}



Any memory errors that are displayed should either be reported as bugs or contributed as additional suppressions.  Note that some versions of Valgrind are \href{https://github.com/JuliaLang/julia/issues/8314\#issuecomment-55766210}{shipped with insufficient default suppressions}, so that may be one thing to consider before submitting any bugs.



\hypertarget{10173787738831416739}{}


\subsection{Running the Julia test suite under Valgrind}



It is possible to run the entire Julia test suite under Valgrind, but it does take quite some time (typically several hours).  To do so, run the following command from the \texttt{julia/test/} directory:




\begin{lstlisting}
valgrind --smc-check=all-non-file --trace-children=yes --suppressions=$PWD/../contrib/valgrind-julia.supp ../julia runtests.jl all
\end{lstlisting}



If you would like to see a report of {\textquotedbl}definite{\textquotedbl} memory leaks, pass the flags \texttt{--leak-check=full --show-leak-kinds=definite} to \texttt{valgrind} as well.



\hypertarget{11463604234155946056}{}


\subsection{Caveats}



Valgrind currently \href{https://bugs.kde.org/show\_bug.cgi?id=136779}{does not support multiple rounding modes}, so code that adjusts the rounding mode will behave differently when run under Valgrind.



In general, if after setting \texttt{--smc-check=all-non-file} you find that your program behaves differently when run under Valgrind, it may help to pass \texttt{--tool=none} to \texttt{valgrind} as you investigate further.  This will enable the minimal Valgrind machinery but will also run much faster than when the full memory checker is enabled.



\hypertarget{7868060637958278195}{}


\section{Sanitizer support}



\hypertarget{527857279218691176}{}


\subsection{General considerations}



Using Clang{\textquotesingle}s sanitizers obviously requires you to use Clang (\texttt{USECLANG=1}), but there{\textquotesingle}s another catch: most sanitizers require a run-time library, provided by the host compiler, while the instrumented code generated by Julia{\textquotesingle}s JIT relies on functionality from that library. This implies that the LLVM version of your host compiler must match that of the LLVM library used within Julia.



An easy solution is to have a dedicated build folder for providing a matching toolchain, by building with \texttt{BUILD\_LLVM\_CLANG=1}. You can then refer to this toolchain from another build folder by specifying \texttt{USECLANG=1} while overriding the \texttt{CC} and \texttt{CXX} variables.



The sanitizers error out when they detect a shared library being opened using \texttt{RTLD\_DEEPBIND} (ref: \href{https://github.com/google/sanitizers/issues/611}{google/sanitizers\#611}). Since \href{https://github.com/staticfloat/libblastrampoline}{libblastrampoline} by default uses \texttt{RTLD\_DEEPBIND}, we need to set the environment variable \texttt{LBT\_USE\_RTLD\_DEEPBIND=0} when using a sanitizer.



To use one of of the sanitizers set \texttt{SANITIZE=1} and then the appropriate flag for the sanitizer you want to use.



On macOS, this might need some extra flags also to work. Altogether, it might look like this, plus one or more of the \texttt{SANITIZE\_*} flags listed below:




\begin{lstlisting}
make -C deps USE_BINARYBUILDER_LLVM=0 LLVM_VER=svn stage-llvm

make -C src SANITIZE=1 USECLANG=1 \
    CC=~+/deps/scratch/llvm-svn/build_Release/bin/clang \
    CXX=~+/deps/scratch/llvm-svn/build_Release/bin/clang++ \
    CPPFLAGS="-isysroot $(xcode-select -p)/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk" \
    CXXFLAGS="-isystem $(xcode-select -p)/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1"
\end{lstlisting}



(or put these into your \texttt{Make.user}, so you don{\textquotesingle}t need to remember them every time).



\hypertarget{1061650975116506198}{}


\subsection{Address Sanitizer (ASAN)}



For detecting or debugging memory bugs, you can use Clang{\textquotesingle}s \href{https://clang.llvm.org/docs/AddressSanitizer.html}{address sanitizer (ASAN)}. By compiling with \texttt{SANITIZE\_ADDRESS=1} you enable ASAN for the Julia compiler and its generated code. In addition, you can specify \texttt{LLVM\_SANITIZE=1} to sanitize the LLVM library as well. Note that these options incur a high performance and memory cost. For example, using ASAN for Julia and LLVM makes \texttt{testall1} take 8-10 times as long while using 20 times as much memory (this can be reduced to respectively a factor of 3 and 4 by using the options described below).



By default, Julia sets the \texttt{allow\_user\_segv\_handler=1} ASAN flag, which is required for signal delivery to work properly. You can define other options using the \texttt{ASAN\_OPTIONS} environment flag, in which case you{\textquotesingle}ll need to repeat the default option mentioned before. For example, memory usage can be reduced by specifying \texttt{fast\_unwind\_on\_malloc=0} and \texttt{malloc\_context\_size=2}, at the cost of backtrace accuracy. For now, Julia also sets \texttt{detect\_leaks=0}, but this should be removed in the future.



\hypertarget{3406286911383043950}{}


\subsubsection{Example setup}



\hypertarget{442822451198415909}{}


\paragraph{Step 1: Install toolchain}



Checkout a Git worktree (or create out-of-tree build directory) at \texttt{\$TOOLCHAIN\_WORKTREE} and create a config file \texttt{\$TOOLCHAIN\_WORKTREE/Make.user} with




\begin{lstlisting}
USE_BINARYBUILDER_LLVM=1
BUILD_LLVM_CLANG=1
\end{lstlisting}



Run:




\begin{lstlisting}
cd $TOOLCHAIN_WORKTREE
make -C deps install-llvm install-clang install-llvm-tools
\end{lstlisting}



to install toolchain binaries in \texttt{\$TOOLCHAIN\_WORKTREE/usr/tools}



\hypertarget{11712997947190658584}{}


\paragraph{Step 2: Build Julia with ASAN}



Checkout a Git worktree (or create out-of-tree build directory) at \texttt{\$BUILD\_WORKTREE} and create a config file \texttt{\$BUILD\_WORKTREE/Make.user} with




\begin{lstlisting}
TOOLCHAIN=$(TOOLCHAIN_WORKTREE)/usr/tools

# use our new toolchain
USECLANG=1
override CC=$(TOOLCHAIN)/clang
override CXX=$(TOOLCHAIN)/clang++
export ASAN_SYMBOLIZER_PATH=$(TOOLCHAIN)/llvm-symbolizer

USE_BINARYBUILDER_LLVM=1

override SANITIZE=1
override SANITIZE_ADDRESS=1

# make the GC use regular malloc/frees, which are hooked by ASAN
override WITH_GC_DEBUG_ENV=1

# default to a debug build for better line number reporting
override JULIA_BUILD_MODE=debug

# make ASAN consume less memory
export ASAN_OPTIONS=detect_leaks=0:fast_unwind_on_malloc=0:allow_user_segv_handler=1:malloc_context_size=2

JULIA_PRECOMPILE=1

# tell libblastrampoline to not use RTLD_DEEPBIND
export LBT_USE_RTLD_DEEPBIND=0
\end{lstlisting}



Run:




\begin{lstlisting}
cd $BUILD_WORKTREE
make debug
\end{lstlisting}



to build \texttt{julia-debug} with ASAN.



\hypertarget{11752764117060042950}{}


\subsection{Memory Sanitizer (MSAN)}



For detecting use of uninitialized memory, you can use Clang{\textquotesingle}s \href{https://clang.llvm.org/docs/MemorySanitizer.html}{memory sanitizer (MSAN)} by compiling with \texttt{SANITIZE\_MEMORY=1}.



\hypertarget{14311970646292028418}{}


\subsection{Thread Sanitizer (TSAN)}



For debugging data-races and other threading related issues you can use Clang{\textquotesingle}s \href{https://clang.llvm.org/docs/ThreadSanitizer.html}{thread sanitizer (TSAN)} by compiling with \texttt{SANITIZE\_THREAD=1}.



\hypertarget{4644242585410635176}{}


\section{Instrumenting Julia with DTrace, and bpftrace}



DTrace and bpftrace are tools that enable lightweight instrumentation of processes. You can turn the instrumentation on and off while the process is running, and with instrumentation off the overhead is minimal.



\begin{quote}
\textbf{Julia 1.8}

Support for probes was added in Julia 1.8

\end{quote}


\begin{quote}
\textbf{Note}

This documentation has been written from a Linux perspective, most of this should hold on Mac OS/Darwin and FreeBSD.

\end{quote}


\hypertarget{10219409411331572644}{}


\subsection{Enabling support}



On Linux install the \texttt{systemtap} package that has a version of \texttt{dtrace}.




\begin{lstlisting}
WITH_DTRACE=1
\end{lstlisting}



\hypertarget{9303790172260158252}{}


\subsubsection{Verifying}




\begin{lstlisting}
> readelf -n usr/lib/libjulia-internal.so.1

Displaying notes found in: .note.gnu.build-id
  Owner                Data size 	Description
  GNU                  0x00000014	NT_GNU_BUILD_ID (unique build ID bitstring)
    Build ID: 57161002f35548772a87418d2385c284ceb3ead8

Displaying notes found in: .note.stapsdt
  Owner                Data size 	Description
  stapsdt              0x00000029	NT_STAPSDT (SystemTap probe descriptors)
    Provider: julia
    Name: gc__begin
    Location: 0x000000000013213e, Base: 0x00000000002bb4da, Semaphore: 0x0000000000346cac
    Arguments:
  stapsdt              0x00000032	NT_STAPSDT (SystemTap probe descriptors)
    Provider: julia
    Name: gc__stop_the_world
    Location: 0x0000000000132144, Base: 0x00000000002bb4da, Semaphore: 0x0000000000346cae
    Arguments:
  stapsdt              0x00000027	NT_STAPSDT (SystemTap probe descriptors)
    Provider: julia
    Name: gc__end
    Location: 0x000000000013214a, Base: 0x00000000002bb4da, Semaphore: 0x0000000000346cb0
    Arguments:
  stapsdt              0x0000002d	NT_STAPSDT (SystemTap probe descriptors)
    Provider: julia
    Name: gc__finalizer
    Location: 0x0000000000132150, Base: 0x00000000002bb4da, Semaphore: 0x0000000000346cb2
    Arguments:
\end{lstlisting}



\hypertarget{11396479853674722935}{}


\subsection{Adding probes in libjulia}



Probes are declared in dtraces format in the file \texttt{src/uprobes.d}. The generated header file is included in \texttt{src/julia\_internal.h} and if you add probes you should provide a noop implementation there.



The header will contain a semaphore \texttt{*\_ENABLED} and the actual call to the probe. If the probe arguments are expensive to compute you should first check if the probe is enabled and then compute the arguments and call the probe.




\begin{lstlisting}
  if (JL_PROBE_{PROBE}_ENABLED())
    auto expensive_arg = ...;
    JL_PROBE_{PROBE}(expensive_arg);
\end{lstlisting}



If your probe has no arguments it is preferred to not include the semaphore check. With USDT probes enabled the cost of a semaphore is a memory load, irrespective of the fact that the probe is enabled or not.




\begin{lstlisting}
#define JL_PROBE_GC_BEGIN_ENABLED() __builtin_expect (julia_gc__begin_semaphore, 0)
__extension__ extern unsigned short julia_gc__begin_semaphore __attribute__ ((unused)) __attribute__ ((section (".probes")));
\end{lstlisting}



Whereas the probe itself is a noop sled that will be patched to a trampoline to the probe handler.



\hypertarget{5986924530124409558}{}


\subsection{Available probes}



\hypertarget{15347650561970213123}{}


\subsubsection{GC probes}



\begin{itemize}
\item[1. ] \texttt{julia:gc\_\_begin}: GC begins running on one thread and triggers stop-the-world.


\item[2. ] \texttt{julia:gc\_\_stop\_the\_world}: All threads have reached a safepoint and GC runs.


\item[3. ] \texttt{julia:gc\_\_mark\_\_begin}: Beginning the mark phase


\item[4. ] \texttt{julia:gc\_\_mark\_end(scanned\_bytes, perm\_scanned)}: Mark phase ended


\item[5. ] \texttt{julia:gc\_\_sweep\_begin(full)}: Starting sweep


\item[6. ] \texttt{julia:gc\_\_sweep\_end()}: Sweep phase finished


\item[7. ] \texttt{julia:gc\_\_end}: GC is finished, other threads continue work


\item[8. ] \texttt{julia:gc\_\_finalizer}: Initial GC thread has finished running finalizers

\end{itemize}


\hypertarget{2953811662478296787}{}


\paragraph{GC stop-the-world latency}



An example \texttt{bpftrace} script is given in \texttt{contrib/gc\_stop\_the\_world\_latency.bt} and it creates a histogram of the latency for all threads to reach a safepoint.



Running this Julia code, with \texttt{julia -t 2}




\begin{lstlisting}
using Base.Threads

fib(x) = x <= 1 ? 1 : fib(x-1) + fib(x-2)

beaver = @spawn begin
    while true
        fib(30)
        # This safepoint is necessary until #41616, since otherwise this
        # loop will never yield to GC.
        GC.safepoint()
    end
end

allocator = @spawn begin
    while true
        zeros(1024)
    end
end

wait(allocator)
\end{lstlisting}



and in a second terminal




\begin{lstlisting}
> sudo contrib/bpftrace/gc_stop_the_world_latency.bt
Attaching 4 probes...
Tracing Julia GC Stop-The-World Latency... Hit Ctrl-C to end.
^C


@usecs[1743412]:
[4, 8)               971 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
[8, 16)              837 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@        |
[16, 32)             129 |@@@@@@                                              |
[32, 64)              10 |                                                    |
[64, 128)              1 |                                                    |
\end{lstlisting}



We can see the latency distribution of the stop-the-world phase in the executed Julia process.



\hypertarget{8894559035613839694}{}


\subsection{Notes on using \texttt{bpftrace}}



An example probe in the bpftrace format looks like:




\begin{lstlisting}
usdt:usr/lib/libjulia-internal.so:julia:gc__begin
{
	@start[pid] = nsecs;
}
\end{lstlisting}



The probe declaration takes the kind \texttt{usdt}, then either the path to the library or the PID, the provider name \texttt{julia} and the probe name \texttt{gc\_\_begin}. Note that I am using a relative path to the \texttt{libjulia-internal.so}, but this might need to be an absolute path on a production system.



\hypertarget{17401464672027654977}{}


\subsection{Useful references:}



\begin{itemize}
\item \href{https://jvns.ca/blog/2017/07/05/linux-tracing-systems}{Julia Evans blog on Linux tracing systems}


\item \href{https://lwn.net/Articles/753601/}{LWN article on USDT and BPF}


\item \href{https://sourceware.org/gdb/onlinedocs/gdb/Static-Probe-Points.html}{GDB support for probes}


\item \href{https://www.brendangregg.com/linuxperf.html}{Brendan Gregg – Linux Performance}

\end{itemize}


\chapter{Building Julia}


\hypertarget{16968142636188277999}{}


\section{Building Julia (Detailed)}



\hypertarget{18173005634014884273}{}


\subsection{Downloading the Julia source code}



If you are behind a firewall, you may need to use the \texttt{https} protocol instead of the \texttt{git} protocol:




\begin{lstlisting}
git config --global url."https://".insteadOf git://
\end{lstlisting}



Be sure to also configure your system to use the appropriate proxy settings, e.g. by setting the \texttt{https\_proxy} and \texttt{http\_proxy} variables.



\hypertarget{9756497403871169155}{}


\subsection{Building Julia}



When compiled the first time, the build will automatically download pre-built \href{\#required-build-tools-and-external-libraries}{external dependencies}. If you prefer to build all the dependencies on your own, or are building on a system that cannot access the network during the build process, add the following in \texttt{Make.user}:




\begin{lstlisting}
USE_BINARYBUILDER=0
\end{lstlisting}



Building Julia requires 5GiB if building all dependencies and approximately 4GiB of virtual memory.



To perform a parallel build, use \texttt{make -j N} and supply the maximum number of concurrent processes. If the defaults in the build do not work for you, and you need to set specific make parameters, you can save them in \texttt{Make.user}, and place the file in the root of your Julia source. The build will automatically check for the existence of \texttt{Make.user} and use it if it exists.



You can create out-of-tree builds of Julia by specifying \texttt{make O=<build-directory> configure} on the command line. This will create a directory mirror, with all of the necessary Makefiles to build Julia, in the specified directory. These builds will share the source files in Julia and \texttt{deps/srccache}. Each out-of-tree build directory can have its own \texttt{Make.user} file to override the global \texttt{Make.user} file in the top-level folder.



If everything works correctly, you will see a Julia banner and an interactive prompt into which you can enter expressions for evaluation. (Errors related to libraries might be caused by old, incompatible libraries sitting around in your PATH. In this case, try moving the \texttt{julia} directory earlier in the PATH). Note that most of the instructions above apply to unix systems.



To run julia from anywhere you can:



\begin{itemize}
\item add an alias (in \texttt{bash}: \texttt{echo {\textquotedbl}alias julia={\textquotesingle}/path/to/install/folder/bin/julia{\textquotesingle}{\textquotedbl} >> {\textasciitilde}/.bashrc \&\& source {\textasciitilde}/.bashrc}), or


\item add a soft link to the \texttt{julia} executable in the \texttt{julia} directory to \texttt{/usr/local/bin} (or any suitable directory already in your path), or


\item add the \texttt{julia} directory to your executable path for this shell session (in \texttt{bash}: \texttt{export PATH={\textquotedbl}\$(pwd):\$PATH{\textquotedbl}} ; in \texttt{csh} or \texttt{tcsh}:

\end{itemize}


\texttt{set path= ( \$path \$cwd )} ), or



\begin{itemize}
\item add the \texttt{julia} directory to your executable path permanently (e.g. in \texttt{.bash\_profile}), or


\item write \texttt{prefix=/path/to/install/folder} into \texttt{Make.user} and then run \texttt{make install}. If there is a version of Julia already installed in this folder, you should delete it before running \texttt{make install}.

\end{itemize}


Now you should be able to run Julia like this:




\begin{lstlisting}
julia
\end{lstlisting}



If you are building a Julia package for distribution on Linux, macOS, or Windows, take a look at the detailed notes in \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/distributing.md}{distributing.md}.



\hypertarget{10318125723095277293}{}


\subsection{Updating an existing source tree}



If you have previously downloaded \texttt{julia} using \texttt{git clone}, you can update the existing source tree using \texttt{git pull} rather than starting anew:




\begin{lstlisting}
cd julia
git pull && make
\end{lstlisting}



Assuming that you had made no changes to the source tree that will conflict with upstream updates, these commands will trigger a build to update to the latest version.



\hypertarget{10270356017395204247}{}


\subsection{General troubleshooting}



\begin{itemize}
\item[1. ] Over time, the base library may accumulate enough changes such that the bootstrapping process in building the system image will fail. If this happens, the build may fail with an error like


\begin{lstlisting}
 *** This error is usually fixed by running 'make clean'. If the error persists, try 'make cleanall' ***
\end{lstlisting}

As described, running \texttt{make clean \&\& make} is usually sufficient. Occasionally, the stronger cleanup done by \texttt{make cleanall} is needed.


\item[2. ] New versions of external dependencies may be introduced which may occasionally cause conflicts with existing builds of older versions.

a. Special \texttt{make} targets exist to help wipe the existing build of a    dependency. For example, \texttt{make -C deps clean-llvm} will clean out the    existing build of \texttt{llvm} so that \texttt{llvm} will be rebuilt from the    downloaded source distribution the next time \texttt{make} is called.    \texttt{make -C deps distclean-llvm} is a stronger wipe which will also delete    the downloaded source distribution, ensuring that a fresh copy of the    source distribution will be downloaded and that any new patches will be    applied the next time \texttt{make} is called.

b. To delete existing binaries of \texttt{julia} and all its dependencies,    delete the \texttt{./usr} directory \emph{in the source tree}.


\item[3. ] If you{\textquotesingle}ve updated macOS recently, be sure to run \texttt{xcode-select --install} to update the command line tools. Otherwise, you could run into errors for missing headers and libraries, such as \texttt{ld: library not found for -lcrt1.10.6.o}.


\item[4. ] If you{\textquotesingle}ve moved the source directory, you might get errors such as  \texttt{CMake Error: The current CMakeCache.txt directory ... is different than the directory ... where     CMakeCache.txt was created.}, in which case you may delete the offending dependency under \texttt{deps}


\item[5. ] In extreme cases, you may wish to reset the source tree to a pristine state. The following git commands may be helpful:


\begin{lstlisting}
 git reset --hard #Forcibly remove any changes to any files under version control
 git clean -x -f -d #Forcibly remove any file or directory not under version control
\end{lstlisting}

\emph{To avoid losing work, make sure you know what these commands do before you run them. \texttt{git} will not be able to undo these changes!}

\end{itemize}


\hypertarget{12302666080168492498}{}


\subsection{Platform-Specific Notes}



Notes for various operating systems:



\begin{itemize}
\item \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/linux.md}{Linux}


\item \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/macos.md}{macOS}


\item \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/windows.md}{Windows}


\item \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/freebsd.md}{FreeBSD}

\end{itemize}


Notes for various architectures:



\begin{itemize}
\item \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/arm.md}{ARM}

\end{itemize}


\hypertarget{13972737225468149107}{}


\subsection{Required Build Tools and External Libraries}



Building Julia requires that the following software be installed:



\begin{itemize}
\item \textbf{[GNU make]}                — building dependencies.


\item \textbf{[gcc \& g++][gcc]} (>= 5.1) or \textbf{[Clang][clang]} (>= 3.5, >= 6.0 for Apple Clang) — compiling and linking C, C++.


\item \textbf{[libatomic][gcc]}          — provided by \textbf{[gcc]} and needed to support atomic operations.


\item \textbf{[python]} (>=2.7)          — needed to build LLVM.


\item \textbf{[gfortran]}                — compiling and linking Fortran libraries.


\item \textbf{[perl]}                    — preprocessing of header files of libraries.


\item \textbf{[wget]}, \textbf{[curl]}, or \textbf{[fetch]} (FreeBSD) — to automatically download external libraries.


\item \textbf{[m4]}                      — needed to build GMP.


\item \textbf{[awk]}                     — helper tool for Makefiles.


\item \textbf{[patch]}                   — for modifying source code.


\item \textbf{[cmake]} (>= 3.4.3)        — needed to build \texttt{libgit2}.


\item \textbf{[pkg-config]}              — needed to build \texttt{libgit2} correctly, especially for proxy support.


\item \textbf{[powershell]} (>= 3.0)     — necessary only on Windows.


\item \textbf{[which]}                   — needed for checking build dependencies.

\end{itemize}


On Debian-based distributions (e.g. Ubuntu), you can easily install them with \texttt{apt-get}:




\begin{lstlisting}
sudo apt-get install build-essential libatomic1 python gfortran perl wget m4 cmake pkg-config curl
\end{lstlisting}



Julia uses the following external libraries, which are automatically downloaded (or in a few cases, included in the Julia source repository) and then compiled from source the first time you run \texttt{make}. The specific version numbers of these libraries that Julia uses are listed in \href{https://github.com/JuliaLang/julia/blob/master/deps/Versions.make}{\texttt{deps/Versions.make}}:



\begin{itemize}
\item \textbf{[LLVM]} (9.0 + \href{https://github.com/JuliaLang/julia/tree/master/deps/patches}{patches}) — compiler infrastructure (see \href{\#llvm}{note below}).


\item \textbf{[FemtoLisp]}            — packaged with Julia source, and used to implement the compiler front-end.


\item \textbf{[libuv]}  (custom fork) — portable, high-performance event-based I/O library.


\item \textbf{[OpenLibm]}             — portable libm library containing elementary math functions.


\item \textbf{[DSFMT]}                — fast Mersenne Twister pseudorandom number generator library.


\item \textbf{[OpenBLAS]}             — fast, open, and maintained [basic linear algebra subprograms (BLAS)]


\item \textbf{[LAPACK]}               — library of linear algebra routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems.


\item \textbf{[MKL]} (optional)       – OpenBLAS and LAPACK may be replaced by Intel{\textquotesingle}s MKL library.


\item \textbf{[SuiteSparse]}          — library of linear algebra routines for sparse matrices.


\item \textbf{[PCRE]}                 — Perl-compatible regular expressions library.


\item \textbf{[GMP]}                  — GNU multiple precision arithmetic library, needed for \texttt{BigInt} support.


\item \textbf{[MPFR]}                 — GNU multiple precision floating point library, needed for arbitrary precision floating point (\texttt{BigFloat}) support.


\item \textbf{[libgit2]}              — Git linkable library, used by Julia{\textquotesingle}s package manager.


\item \textbf{[curl]}                 — libcurl provides download and proxy support.


\item \textbf{[libssh2]}              — library for SSH transport, used by libgit2 for packages with SSH remotes.


\item \textbf{[mbedtls]}              — library used for cryptography and transport layer security, used by libssh2


\item \textbf{[utf8proc]}             — a library for processing UTF-8 encoded Unicode strings.


\item \textbf{[LLVM libunwind]}       — LLVM{\textquotesingle}s fork of [libunwind], a library that determines the call-chain of a program.

\end{itemize}


[GNU make]:     https://www.gnu.org/software/make [patch]:        https://www.gnu.org/software/patch [wget]:         https://www.gnu.org/software/wget [m4]:           https://www.gnu.org/software/m4 [awk]:          https://www.gnu.org/software/gawk [gcc]:          https://gcc.gnu.org [clang]:        https://clang.llvm.org [python]:       https://www.python.org/ [gfortran]:     https://gcc.gnu.org/fortran/ [curl]:         https://curl.haxx.se [fetch]:        https://www.freebsd.org/cgi/man.cgi?fetch(1) [perl]:         https://www.perl.org [cmake]:        https://www.cmake.org [OpenLibm]:     https://github.com/JuliaLang/openlibm [DSFMT]:        https://github.com/MersenneTwister-Lab/dSFMT [OpenBLAS]:     https://github.com/xianyi/OpenBLAS [LAPACK]:       https://www.netlib.org/lapack [MKL]:          https://software.intel.com/en-us/articles/intel-mkl [SuiteSparse]:  https://people.engr.tamu.edu/davis/suitesparse.html [PCRE]:         https://www.pcre.org [LLVM]:         https://www.llvm.org [LLVM libunwind]: https://github.com/llvm/llvm-project/tree/main/libunwind [FemtoLisp]:    https://github.com/JeffBezanson/femtolisp [GMP]:          https://gmplib.org [MPFR]:         https://www.mpfr.org [libuv]:        https://github.com/JuliaLang/libuv [libgit2]:      https://libgit2.org/ [utf8proc]:     https://julialang.org/utf8proc/ [libunwind]:    https://www.nongnu.org/libunwind [libssh2]:      https://www.libssh2.org [mbedtls]:      https://tls.mbed.org/ [pkg-config]:   https://www.freedesktop.org/wiki/Software/pkg-config/ [powershell]:   https://docs.microsoft.com/en-us/powershell/scripting/wmf/overview [which]:        https://carlowood.github.io/which/



\hypertarget{14428860566813867056}{}


\subsection{Build dependencies}



If you already have one or more of these packages installed on your system, you can prevent Julia from compiling duplicates of these libraries by passing \texttt{USE\_SYSTEM\_...=1} to \texttt{make} or adding the line to \texttt{Make.user}. The complete list of possible flags can be found in \texttt{Make.inc}.



Please be aware that this procedure is not officially supported, as it introduces additional variability into the installation and versioning of the dependencies, and is recommended only for system package maintainers. Unexpected compile errors may result, as the build system will do no further checking to ensure the proper packages are installed.



\hypertarget{2251940844893656958}{}


\subsubsection{LLVM}



The most complicated dependency is LLVM, for which we require additional patches from upstream (LLVM is not backward compatible).



For packaging Julia with LLVM, we recommend either:



\begin{itemize}
\item bundling a Julia-only LLVM library inside the Julia package, or


\item adding the patches to the LLVM package of the distribution.

\begin{itemize}
\item A complete list of patches is available in \texttt{deps/llvm.mk}, and the patches themselves are in \texttt{deps/patches/}.


\item The only Julia-specific patch is the lib renaming (\texttt{llvm-symver-jlprefix.patch}), which should \emph{not} be applied to a system LLVM.


\item The remaining patches are all upstream bug fixes, and have been contributed into upstream LLVM.

\end{itemize}
\end{itemize}


Using an unpatched or different version of LLVM will result in errors and/or poor performance. Though Julia can be built with newer LLVM versions, support for this should be regarded as experimental and not suitable for packaging.



\hypertarget{4646697391847551863}{}


\subsubsection{libuv}



Julia uses a custom fork of libuv. It is a small dependency, and can be safely bundled in the same package as Julia, and will not conflict with the system library. Julia builds should \emph{not} try to use the system libuv.



\hypertarget{16914034871400552403}{}


\subsubsection{BLAS and LAPACK}



As a high-performance numerical language, Julia should be linked to a multi-threaded BLAS and LAPACK, such as OpenBLAS or ATLAS, which will provide much better performance than the reference \texttt{libblas} implementations which may be default on some systems.



\hypertarget{14353796440229471494}{}


\subsection{Source distributions of releases}



Each pre-release and release of Julia has a {\textquotedbl}full{\textquotedbl} source distribution and a {\textquotedbl}light{\textquotedbl} source distribution.



The full source distribution contains the source code for Julia and all dependencies so that it can be built from source without an internet connection. The light source distribution does not include the source code of dependencies.



For example, \texttt{julia-1.0.0.tar.gz} is the light source distribution for the \texttt{v1.0.0} release of Julia, while \texttt{julia-1.0.0-full.tar.gz} is the full source distribution.



\hypertarget{12404192935650271562}{}


\subsection{Building Julia from source with a Git checkout of a stdlib}



If you need to build Julia from source with a Git checkout of a stdlib, then use \texttt{make DEPS\_GIT=NAME\_OF\_STDLIB} when building Julia.



For example, if you need to build Julia from source with a Git checkout of Pkg, then use \texttt{make DEPS\_GIT=Pkg} when building Julia. The \texttt{Pkg} repo is in \texttt{stdlib/Pkg}, and created initially with a detached \texttt{HEAD}. If you{\textquotesingle}re doing this from a pre-existing Julia repository, you may need to \texttt{make clean} beforehand.



If you need to build Julia from source with Git checkouts of more than one stdlib, then \texttt{DEPS\_GIT} should be a space-separated list of the stdlib names. For example, if you need to build Julia from source with a Git checkout of Pkg, Tar, and Downloads, then use \texttt{make DEPS\_GIT={\textquotesingle}Pkg Tar Downloads{\textquotesingle}} when building Julia.



\hypertarget{17775253147870272772}{}


\section{Linux}



\begin{itemize}
\item GCC version 4.7 or later is required to build Julia.


\item To use external shared libraries not in the system library search path, set \texttt{USE\_SYSTEM\_XXX=1} and \texttt{LDFLAGS=-Wl,-rpath,/path/to/dir/contains/libXXX.so} in \texttt{Make.user}.


\item Instead of setting \texttt{LDFLAGS}, putting the library directory into the environment variable \texttt{LD\_LIBRARY\_PATH} (at both compile and run time) also works.


\item The \texttt{USE\_SYSTEM\_*} flags should be used with caution. These are meant only for troubleshooting, porting, and packaging, where package maintainers work closely with the Julia developers to make sure that Julia is built correctly. Production use cases should use the officially provided binaries. Issues arising from the use of these flags will generally not be accepted.


\item See also the \hyperlink{4364585969841474105}{external dependencies}.

\end{itemize}


\hypertarget{7684914430795515278}{}


\subsection{Architecture Customization}



Julia can be built for a non-generic architecture by configuring the \texttt{ARCH} Makefile variable in a \texttt{Make.user} file. See the appropriate section of \texttt{Make.inc} for additional customization options, such as \texttt{MARCH} and \texttt{JULIA\_CPU\_TARGET}.



For example, to build for Pentium 4, set \texttt{MARCH=pentium4} and install the necessary system libraries for linking. On Ubuntu, these may include lib32gfortran-6-dev, lib32gcc1, and lib32stdc++6, among others.



You can also set \texttt{MARCH=native} in \texttt{Make.user} for a maximum-performance build customized for the current machine CPU.



\hypertarget{15149557091365000286}{}


\subsection{Linux Build Troubleshooting}




\begin{table}[h]

\begin{tabulary}{\linewidth}{|R|R|}
\hline
Problem & Possible Solution \\
\hline
OpenBLAS build failure & Set one of the following build options in \texttt{Make.user} and build again: <ul><li> \texttt{OPENBLAS\_TARGET\_ARCH=BARCELONA} (AMD CPUs) or \texttt{OPENBLAS\_TARGET\_ARCH=NEHALEM} (Intel CPUs)<ul>Set \texttt{OPENBLAS\_DYNAMIC\_ARCH = 0} to disable compiling multiple architectures in a single binary.</ul></li><li> \texttt{OPENBLAS\_NO\_AVX2 = 1} disables AVX2 instructions, allowing OpenBLAS to compile with \texttt{OPENBLAS\_DYNAMIC\_ARCH = 1} using old versions of binutils </li><li> \texttt{USE\_SYSTEM\_BLAS=1} uses the system provided \texttt{libblas} <ul><li>Set \texttt{LIBBLAS=-lopenblas} and \texttt{LIBBLASNAME=libopenblas} to force the use of the system provided OpenBLAS when multiple BLAS versions are installed. </li></ul></li></ul><p> If you get an error that looks like \texttt{../kernel/x86\_64/dgemm\_kernel\_4x4\_haswell.S:1709: Error: no such instruction: `vpermpd \$ 0xb1,\%ymm0,\%ymm0{\textquotesingle}}, then you need to set \texttt{OPENBLAS\_DYNAMIC\_ARCH = 0} or \texttt{OPENBLAS\_NO\_AVX2 = 1}, or you need a newer version of \texttt{binutils} (2.18 or newer). (\href{https://github.com/JuliaLang/julia/issues/7653}{Issue \#7653})</p><p> If the linker cannot find \texttt{gfortran} and you get an error like \texttt{julia /usr/bin/x86\_64-linux-gnu-ld: cannot find -lgfortran}, check the path with \texttt{gfortran -print-file-name=libgfortran.so} and use the output to export something similar to this: \texttt{export LDFLAGS=-L/usr/lib/gcc/x86\_64-linux-gnu/8/}. See \href{https://github.com/JuliaLang/julia/issues/6150\#issuecomment-37546803}{Issue \#6150}.</p> \\
\hline
Illegal Instruction error & Check if your CPU supports AVX while your OS does not (e.g. through virtualization, as described in \href{https://github.com/JuliaLang/julia/issues/3263}{this issue}). \\
\hline
\end{tabulary}

\end{table}



\hypertarget{12383401899447558259}{}


\section{macOS}



You need to have the current Xcode command line utilities installed: run \texttt{xcode-select --install} in the terminal. You will need to rerun this terminal command after each macOS update, otherwise you may run into errors involving missing libraries or headers.



The dependent libraries are now built with \href{https://binarybuilder.org}{BinaryBuilder} and will be automatically downloaded. This is the preferred way to build Julia source. In case you want to build them all on your own, you will need a 64-bit gfortran to compile Julia dependencies.




\begin{lstlisting}
brew install gcc
\end{lstlisting}



If you have set \texttt{LD\_LIBRARY\_PATH} or \texttt{DYLD\_LIBRARY\_PATH} in your \texttt{.bashrc} or equivalent, Julia may be unable to find various libraries that come bundled with it. These environment variables need to be unset for Julia to work.



\hypertarget{16271249808016261018}{}


\section{Windows}



This file describes how to install, or build, and use Julia on Windows.



For more general information about Julia, please see the \href{https://github.com/JuliaLang/julia/blob/master/README.md}{main README} or the \href{https://docs.julialang.org}{documentation}.



\hypertarget{9379381249768363825}{}


\subsection{General Information for Windows}



We highly recommend running Julia using a modern terminal application, in particular Windows Terminal, which can be installed from the \href{https://aka.ms/terminal}{Microsoft Store}.



\hypertarget{14641683579495570257}{}


\subsubsection{Line endings}



Julia uses binary-mode files exclusively. Unlike many other Windows programs, if you write \texttt{{\textbackslash}n} to a file, you get a \texttt{{\textbackslash}n} in the file, not some other bit pattern. This matches the behavior exhibited by other operating systems. If you have installed Git for Windows, it is suggested, but not required, that you configure your system Git to use the same convention:




\begin{lstlisting}
git config --global core.eol lf
git config --global core.autocrlf input
\end{lstlisting}



or edit \texttt{\%USERPROFILE\%{\textbackslash}.gitconfig} and add/edit the lines:




\begin{lstlisting}
[core]
    eol = lf
    autocrlf = input
\end{lstlisting}



\hypertarget{14077789474605882353}{}


\subsection{Binary distribution}



For the binary distribution installation notes on Windows please see the instructions at \href{https://julialang.org/downloads/platform/\#windows}{https://julialang.org/downloads/platform/\#windows}.



\hypertarget{11651987673758849221}{}


\subsection{Source distribution}



\hypertarget{7881402461160500543}{}


\subsubsection{Cygwin-to-MinGW cross-compiling}



The recommended way of compiling Julia from source on Windows is by cross compiling from \href{https://www.cygwin.com}{Cygwin}, using versions of the MinGW-w64 compilers available through Cygwin{\textquotesingle}s package manager.



\begin{itemize}
\item[1. ] Download and run Cygwin setup for \href{https://cygwin.com/setup-x86.exe}{32 bit} or \href{https://cygwin.com/setup-x86\_64.exe}{64 bit}. Note, that you can compile either 32 or 64 bit Julia from either 32 or 64 bit Cygwin. 64 bit Cygwin has a slightly smaller but often more up-to-date selection of packages.

Advanced: you may skip steps 2-4 by running:


\begin{lstlisting}
setup-x86_64.exe -s <url> -q -P cmake,gcc-g++,git,make,patch,curl,m4,python3,p7zip,mingw64-i686-gcc-g++,mingw64-i686-gcc-fortran,mingw64-x86_64-gcc-g++,mingw64-x86_64-gcc-fortran
:: replace <url> with a site from https://cygwin.com/mirrors.html
:: or run setup manually first and select a mirror
\end{lstlisting}


\item[2. ] Select installation location and download mirror.


\item[3. ] At the {\textquotesingle}\emph{Select Packages{\textquotesingle}} step, select the following:

\begin{itemize}
\item[1. ] From the \emph{Devel} category: \texttt{cmake}, \texttt{gcc-g++}, \texttt{git}, \texttt{make}, \texttt{patch}


\item[2. ] From the \emph{Net} category: \texttt{curl}


\item[3. ] From \emph{Interpreters} (or \emph{Python}) category: \texttt{m4}, \texttt{python3}


\item[4. ] From the \emph{Archive} category: \texttt{p7zip}


\item[5. ] For 32 bit Julia, and also from the \emph{Devel} category:  \texttt{mingw64-i686-gcc-g++} and \texttt{mingw64-i686-gcc-fortran}


\item[6. ] For 64 bit Julia, and also from the \emph{Devel} category:  \texttt{mingw64-x86\_64-gcc-g++} and \texttt{mingw64-x86\_64-gcc-fortran}

\end{itemize}

\item[4. ] At the \emph{{\textquotesingle}Resolving Dependencies{\textquotesingle}} step, be sure to leave \emph{{\textquotesingle}Select required packages (RECOMMENDED){\textquotesingle}} enabled.


\item[5. ] Allow Cygwin installation to finish, then start from the installed shortcut a \emph{{\textquotesingle}Cygwin Terminal{\textquotesingle}}, or \emph{{\textquotesingle}Cygwin64 Terminal{\textquotesingle}}, respectively.


\item[6. ] Build Julia and its dependencies from source:

\begin{itemize}
\item[1. ] Get the Julia sources


\begin{lstlisting}
git clone https://github.com/JuliaLang/julia.git
cd julia
\end{lstlisting}

Tip: If you get an \texttt{error: cannot fork() for fetch-pack: Resource temporarily unavailable} from git, add \texttt{alias git={\textquotedbl}env PATH=/usr/bin git{\textquotedbl}} to \texttt{{\textasciitilde}/.bashrc} and restart Cygwin.


\item[2. ] Set the \texttt{XC\_HOST} variable in \texttt{Make.user} to indicate MinGW-w64 cross compilation


\begin{lstlisting}
echo 'XC_HOST = i686-w64-mingw32' > Make.user     # for 32 bit Julia
# or
echo 'XC_HOST = x86_64-w64-mingw32' > Make.user   # for 64 bit Julia
\end{lstlisting}


\item[3. ] Start the build


\begin{lstlisting}
make -j 4   # Adjust the number of threads (4) to match your build environment.
\end{lstlisting}

\end{itemize}
\end{itemize}



\begin{lstlisting}
> Protip: build both!
> ```sh
> make O=julia-win32 configure
> make O=julia-win64 configure
> echo 'XC_HOST = i686-w64-mingw32' > julia-win32/Make.user
> echo 'XC_HOST = x86_64-w64-mingw32' > julia-win64/Make.user
> echo 'ifeq ($(BUILDROOT),$(JULIAHOME))
>         $(error "in-tree build disabled")
>       endif' >> Make.user
> make -C julia-win32  # build for Windows x86 in julia-win32 folder
> make -C julia-win64  # build for Windows x86-64 in julia-win64 folder
> ```
\end{lstlisting}



\begin{itemize}
\item[7. ] Run Julia using the Julia executables directly


\begin{lstlisting}
usr/bin/julia.exe
usr/bin/julia-debug.exe
\end{lstlisting}

\end{itemize}


\hypertarget{5433981816410773170}{}


\subsubsection{Compiling with MinGW/MSYS2}



Compiling Julia from source using \href{https://msys2.github.io}{MSYS2} has worked in the past but is not actively supported. Pull requests to restore support would be welcome. See a \href{https://github.com/JuliaLang/julia/blob/v0.6.0/README.windows.md}{past version of this file} for the former instructions for compiling using MSYS2.



\hypertarget{16470279549755078624}{}


\subsubsection{Cross-compiling from Unix (Linux/Mac/WSL)}



You can also use MinGW-w64 cross compilers to build a Windows version of Julia from Linux, Mac, or the Windows Subsystem for Linux (WSL).



First, you will need to ensure your system has the required dependencies. We need wine (>=1.7.5), a system compiler, and some downloaders. Note: a cygwin install might interfere with this method if using WSL.



\textbf{On Ubuntu} (on other Linux systems the dependency names are likely to be similar):




\begin{lstlisting}
apt-get install wine-stable gcc wget p7zip-full winbind mingw-w64 gfortran-mingw-w64
dpkg --add-architecture i386 && apt-get update && apt-get install wine32 # add sudo to each if needed
# switch all of the following to their "-posix" variants (interactively):
for pkg in i686-w64-mingw32-g++ i686-w64-mingw32-gcc i686-w64-mingw32-gfortran x86_64-w64-mingw32-g++ x86_64-w64-mingw32-gcc x86_64-w64-mingw32-gfortran; do sudo update-alternatives --config $pkg; done
\end{lstlisting}



\textbf{On Mac}: Install XCode, XCode command line tools, X11 (now \href{https://www.xquartz.org/}{XQuartz}), and \href{https://www.macports.org/install.php}{MacPorts} or \href{https://brew.sh/}{Homebrew}.  Then run \texttt{port install wine wget mingw-w64}, or \texttt{brew install wine wget mingw-w64}, as appropriate.



\textbf{Then run the build:}



\begin{itemize}
\item[1. ] \texttt{git clone https://github.com/JuliaLang/julia.git julia-win32}


\item[2. ] \texttt{cd julia-win32}


\item[3. ] \texttt{echo override XC\_HOST = i686-w64-mingw32 >> Make.user}


\item[4. ] \texttt{make}


\item[5. ] \texttt{make win-extras} (Necessary before running \texttt{make binary-dist})


\item[6. ] \texttt{make binary-dist} then \texttt{make exe} to create the Windows installer.


\item[7. ] move the \texttt{julia-*.exe} installer to the target machine

\end{itemize}


If you are building for 64-bit windows, the steps are essentially the same. Just replace \texttt{i686} in \texttt{XC\_HOST} with \texttt{x86\_64}. (note: on Mac, wine only runs in 32-bit mode).



\hypertarget{18405518002992942783}{}


\subsection{Debugging a cross-compiled build under wine}



The most effective way to debug a cross-compiled version of Julia on the cross-compilation host is to install a windows version of gdb and run it under wine as usual. The pre-built packages available \href{https://sourceforge.net/projects/msys2/files/REPOS/MINGW/}{as part of the MSYS2 project} are known to work. Apart from the GDB package you may also need the python and termcap packages. Finally, GDB{\textquotesingle}s prompt may not work when launch from the command line. This can be worked around by prepending \texttt{wineconsole} to the regular GDB invocation.



\hypertarget{10640396420501361633}{}


\subsection{After compiling}



Compiling using one of the options above creates a basic Julia build, but not some extra components that are included if you run the full Julia binary installer. If you need these components, the easiest way to get them is to build the installer yourself using \texttt{make win-extras} followed by \texttt{make binary-dist} and \texttt{make exe}. Then running the resulting installer.



\hypertarget{10585766524694478249}{}


\subsection{Windows Build Debugging}



\hypertarget{12675452062080563616}{}


\subsubsection{GDB hangs with cygwin mintty}



\begin{itemize}
\item Run gdb under the windows console (cmd) instead. gdb \href{https://www.cygwin.com/ml/cygwin/2009-02/msg00531.html}{may not function properly} under mintty with non- cygwin applications. You can use \texttt{cmd /c start} to start the windows console from mintty if necessary.

\end{itemize}


\hypertarget{8856415863446835392}{}


\subsubsection{GDB not attaching to the right process}



\begin{itemize}
\item Use the PID from the windows task manager or \texttt{WINPID} from the \texttt{ps} command instead of the PID from unix style command line tools (e.g. \texttt{pgrep}).  You may need to add the PID column if it is not shown by default in the windows task manager.

\end{itemize}


\hypertarget{8463982529066496058}{}


\subsubsection{GDB not showing the right backtrace}



\begin{itemize}
\item When attaching to the julia process, GDB may not be attaching to the right thread.  Use \texttt{info threads} command to show all the threads and \texttt{thread <threadno>} to switch threads.


\item Be sure to use a 32 bit version of GDB to debug a 32 bit build of Julia, or a 64 bit version of GDB to debug a 64 bit build of Julia.

\end{itemize}


\hypertarget{8099613672189184632}{}


\subsubsection{Build process is slow/eats memory/hangs my computer}



\begin{itemize}
\item Disable the Windows \href{https://en.wikipedia.org/wiki/Windows\_Vista\_I/O\_technologies\#SuperFetch}{Superfetch} and \href{https://blogs.msdn.com/b/cjacks/archive/2011/11/22/managing-the-windows-7-program-compatibility-assistant-pca.aspx}{Program Compatibility Assistant} services, as they are known to have \href{https://cygwin.com/ml/cygwin/2011-12/msg00058.html}{spurious interactions} with MinGW/Cygwin.

As mentioned in the link above: excessive memory use by \texttt{svchost} specifically may be investigated in the Task Manager by clicking on the high-memory \texttt{svchost.exe} process and selecting \texttt{Go to Services}. Disable child services one-by-one until a culprit is found.


\item Beware of \href{https://cygwin.com/faq/faq.html\#faq.using.bloda}{BLODA}. The \href{https://technet.microsoft.com/en-us/sysinternals/dd535533.aspx}{vmmap} tool is indispensable for identifying such software conflicts. Use vmmap to inspect the list of loaded DLLs for bash, mintty, or another persistent process used to drive the build. Essentially \emph{any} DLL outside of the Windows System directory is potential BLODA.

\end{itemize}


\hypertarget{3581420464307357684}{}


\section{FreeBSD}



Clang is the default compiler on FreeBSD 11.0-RELEASE and above. The remaining build tools are available from the Ports Collection, and can be installed using \texttt{pkg install git gcc gmake cmake pkgconf}. To build Julia, simply run \texttt{gmake}. (Note that \texttt{gmake} must be used rather than \texttt{make}, since \texttt{make} on FreeBSD corresponds to the incompatible BSD Make rather than GNU Make.)



As mentioned above, it is important to note that the \texttt{USE\_SYSTEM\_*} flags should be used with caution on FreeBSD. This is because many system libraries, and even libraries from the Ports Collection, link to the system{\textquotesingle}s \texttt{libgcc\_s.so.1}, or to another library which links to the system \texttt{libgcc\_s}. This library declares its GCC version to be 4.6, which is too old to build Julia, and conflicts with other libraries when linking. Thus it is highly recommended to simply allow Julia to build all of its dependencies. If you do choose to use the \texttt{USE\_SYSTEM\_*} flags, note that \texttt{/usr/local} is not on the compiler path by default, so you may need to add \texttt{LDFLAGS=-L/usr/local/lib} and \texttt{CPPFLAGS=-I/usr/local/include} to your \texttt{Make.user}, though doing so may interfere with other dependencies.



Note that the x86 architecture does not support threading due to lack of compiler runtime library support, so you may need to set \texttt{JULIA\_THREADS=0} in your \texttt{Make.user} if you{\textquotesingle}re on a 32-bit system.



\hypertarget{7093051588243331933}{}


\section{ARM (Linux)}



Julia fully supports ARMv8 (AArch64) processors, and supports ARMv7 and ARMv6 (AArch32) with some caveats. This file provides general guidelines for compilation, in addition to instructions for specific devices.



A list of \href{https://github.com/JuliaLang/julia/labels/arm}{known issues} for ARM is available. If you encounter difficulties, please create an issue including the output from \texttt{cat /proc/cpuinfo}.



\hypertarget{6625901791414385042}{}


\subsection{32-bit (ARMv6, ARMv7)}



Julia has been successfully compiled on several variants of the following ARMv6 \& ARMv7 devices:



\begin{itemize}
\item ARMv7 / Cortex A15 Samsung Chromebooks running Ubuntu Linux under Crouton;


\item \href{https://www.raspberrypi.org}{Raspberry Pi}.


\item \href{https://www.hardkernel.com}{Odroid}.

\end{itemize}


Julia requires at least the \texttt{armv6} and \texttt{vfpv2} instruction sets. It{\textquotesingle}s recommended to use  \texttt{armv7-a}. \texttt{armv5} or soft float are not supported.



\hypertarget{10466379612209245112}{}


\subsubsection{Raspberry Pi 1 / Raspberry Pi Zero}



If the type of ARM CPU used in the Raspberry Pi is not detected by LLVM, then explicitly set the CPU target by adding the following to \texttt{Make.user}:




\begin{lstlisting}
JULIA_CPU_TARGET=arm1176jzf-s
\end{lstlisting}



To complete the build, you may need to increase the swap file size. To do so, edit \texttt{/etc/dphys-swapfile}, changing the line:




\begin{lstlisting}
CONF_SWAPSIZE=100
\end{lstlisting}



to:




\begin{lstlisting}
CONF_SWAPSIZE=512
\end{lstlisting}



before restarting the swapfile service:




\begin{lstlisting}
sudo /etc/init.d/dphys-swapfile stop
sudo /etc/init.d/dphys-swapfile start
\end{lstlisting}



\hypertarget{17980381388168105915}{}


\subsubsection{Raspberry Pi 2}



The type of ARM CPU used in the Raspberry Pi 2 is not detected by LLVM. Explicitly set the CPU target by adding the following to \texttt{Make.user}:



\texttt{JULIA\_CPU\_TARGET=cortex-a7}



Depending on the exact compiler and distribution, there might be a build failure due to unsupported inline assembly. In that case, add \texttt{MCPU=armv7-a} to \texttt{Make.user}.



\hypertarget{17302249718319781638}{}


\subsection{AArch64 (ARMv8)}



Julia has been successfully built on the following ARMv8 devices:



\begin{itemize}
\item \href{https://www.nvidia.com/object/embedded-systems-dev-kits-modules.html}{nVidia Jetson TX1 \& TX2};


\item \href{https://www.apm.com/products/data-center/x-gene-family/x-gene/}{X-Gene 1};


\item \href{https://softiron.com/products/overdrive-3000/}{Overdrive 3000};


\item \href{https://www.cavium.com/ThunderX\_ARM\_Processors.html}{Cavium ThunderX} on \href{https://www.packet.net}{packet.net}.

\end{itemize}


Compilation on \texttt{ARMv8-A} requires that \texttt{Make.user} is configured as follows:




\begin{lstlisting}
MCPU=armv8-a
\end{lstlisting}



\hypertarget{1712927129164502401}{}


\subsubsection{nVidia Jetson TX2}



Julia builds and runs on the \href{https://www.nvidia.com/object/embedded-systems-dev-kits-modules.html}{nVidia Jetson TX2} platform with minimal configuration changes.



After configuring \texttt{Make.user} as per the \texttt{AArch64} instructions in this document, follow the general \href{https://github.com/JuliaLang/julia/blob/master/README.md}{build instructions}. The majority of the build dependencies specified in the instructions are installed by the default configuration flashed by \href{https://developer.nvidia.com/embedded/jetpack}{Jetpack 3.0}. The remaining tools can be installed by issuing the following command:




\begin{lstlisting}
sudo apt-get install gfortran wget cmake
\end{lstlisting}



A full parallel build, including LLVM, will complete in around two hours. All tests pass and CUDA functionality is available through, e.g., \href{https://github.com/JuliaGPU/CUDAdrv.jl}{CUDAdrv}.



\hypertarget{10368093425134318336}{}


\section{Binary distributions}



These notes are for those wishing to compile a binary distribution of Julia for distribution on various platforms.  We love users spreading Julia as far and wide as they can, trying it out on as wide an array of operating systems and hardware configurations as possible.  As each platform has specific gotchas and processes that must be followed in order to create a portable, working Julia distribution, we have separated most of the notes by OS.



Note that while the code for Julia is \href{https://github.com/JuliaLang/julia/blob/master/LICENSE.md}{MIT-licensed, with a few exceptions}, the distribution created by the techniques described herein will be GPL licensed, as various dependent libraries such as \texttt{SuiteSparse} are GPL licensed. We do hope to have a non-GPL distribution of Julia in the future.



\hypertarget{10631287561622970973}{}


\subsection{Versioning and Git}



The Makefile uses both the \texttt{VERSION} file and commit hashes and tags from the git repository to generate the \texttt{base/version\_git.jl} with information we use to fill the splash screen and the \texttt{versioninfo()} output. If you for some reason don{\textquotesingle}t want to have the git repository available when building you should pregenerate the \texttt{base/version\_git.jl} file with:




\begin{lstlisting}
make -C base version_git.jl.phony
\end{lstlisting}



Julia has lots of build dependencies where we use patched versions that has not yet been included by the popular package managers. These dependencies will usually be automatically downloaded when you build, but if you want to be able to build Julia on a computer without internet access you should create a full-source-dist archive with the special make target




\begin{lstlisting}
make full-source-dist
\end{lstlisting}



that creates a julia-version-commit.tar.gz archive with all required dependencies.



When compiling a tagged release in the git repository, we don{\textquotesingle}t display the branch/commit hash info in the splash screen. You can use this line to show a release description of up to 45 characters. To set this line you have to create a Make.user file containing:




\begin{lstlisting}
override TAGGED_RELEASE_BANNER = "my-package-repository build"
\end{lstlisting}



\hypertarget{15292951791170792274}{}


\subsection{Target Architectures}



By default, Julia optimizes its system image to the native architecture of the build machine. This is usually not what you want when building packages, as it will make Julia fail at startup on any machine with incompatible CPUs (in particular older ones with more restricted instruction sets).



We therefore recommend that you pass the \texttt{MARCH} variable when calling \texttt{make}, setting it to the baseline target you intend to support. This will determine the target CPU for both the Julia executable and libraries, and the system image (the latter can also be set using \texttt{JULIA\_CPU\_TARGET}). Typically useful values for x86 CPUs are \texttt{x86-64} and \texttt{core2} (for 64-bit builds) and \texttt{pentium4} (for 32-bit builds). Unfortunately, CPUs older than Pentium 4 are currently not supported (see \href{https://github.com/JuliaLang/julia/issues/7185}{this issue}).



The full list of CPU targets supported by LLVM can be obtained by running \texttt{llc -mattr=help}.



\hypertarget{17775253147870272772}{}


\subsection{Linux}



On Linux, \texttt{make binary-dist} creates a tarball that contains a fully functional Julia installation. If you wish to create a distribution package such as a \texttt{.deb}, or \texttt{.rpm}, some extra effort is needed. See the \href{https://github.com/staticfloat/julia-debian}{julia-debian} repository for an example of what metadata is needed for creating \texttt{.deb} packages for Debian and Ubuntu-based systems. See the \href{https://src.fedoraproject.org/rpms/julia}{Fedora package} for RPM-based distributions. Although we have not yet experimented with it, \href{https://wiki.debian.org/Alien}{Alien} could be used to generate Julia packages for various Linux distributions.



Julia supports overriding standard installation directories via \texttt{prefix} and other environment variables you can pass when calling \texttt{make} and \texttt{make install}. See Make.inc for their list. \texttt{DESTDIR} can also be used to force the installation into a temporary directory.



By default, Julia loads \texttt{\$prefix/etc/julia/startup.jl} as an installation-wide initialization file. This file can be used by distribution managers to set up custom paths or initialization code. For Linux distribution packages, if \texttt{\$prefix} is set to \texttt{/usr}, there is no \texttt{/usr/etc} to look into. This requires the path to Julia{\textquotesingle}s private \texttt{etc} directory to be changed.  This can be done via the \texttt{sysconfdir} make variable when building.  Simply pass \texttt{sysconfdir=/etc} to \texttt{make} when building and Julia will first check \texttt{/etc/julia/startup.jl} before trying \texttt{\$prefix/etc/julia/startup.jl}.



\hypertarget{1900796573592861142}{}


\subsection{OS X}



To create a binary distribution on OSX, build Julia first, then cd to \texttt{contrib/mac/app}, and run \texttt{make} with the same makevars that were used with \texttt{make} when building Julia proper.  This will then create a \texttt{.dmg} file in the \texttt{contrib/mac/app} directory holding a completely self-contained Julia.app.



Alternatively, Julia may be built as a framework by invoking \texttt{make} with the \texttt{darwinframework} target and \texttt{DARWIN\_FRAMEWORK=1} set.  For example, \texttt{make DARWIN\_FRAMEWORK=1 darwinframework}.



\hypertarget{16271249808016261018}{}


\subsection{Windows}



Instructions for reating a Julia distribution on Windows are described in the \href{https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/windows.md}{build devdocs for Windows}.



\hypertarget{12663593244703205925}{}


\subsection{Notes on BLAS and LAPACK}



Julia builds OpenBLAS by default, which includes the BLAS and LAPACK libraries. On 32-bit architectures, Julia builds OpenBLAS to use 32-bit integers, while on 64-bit architectures, Julia builds OpenBLAS to use 64-bit integers (ILP64). It is essential that all Julia functions that call BLAS and LAPACK API routines use integers of the correct width.



Most BLAS and LAPACK distributions provided on linux distributions, and even commercial implementations ship libraries that use 32-bit APIs. In many cases, a 64-bit API is provided as a separate library.



When using vendor provided or OS provided libraries, a \texttt{make} option called \texttt{USE\_BLAS64} is available as part of the Julia build. When doing \texttt{make USE\_BLAS64=0}, Julia will call BLAS and LAPACK assuming a 32-bit API, where all integers are 32-bit wide, even on a 64-bit architecture.



Other libraries that Julia uses, such as SuiteSparse also use BLAS and LAPACK internally. The APIs need to be consistent across all libraries that depend on BLAS and LAPACK. The Julia build process will build all these libraries correctly, but when overriding defaults and using system provided libraries, this consistency must be ensured.



Also note that Linux distributions sometimes ship several versions of OpenBLAS, some of which enable multithreading, and others only working in a serial fashion. For example, in Fedora, \texttt{libopenblasp.so} is threaded, but \texttt{libopenblas.so} is not. We recommend using the former for optimal performance. To choose an OpenBLAS library whose name is different from the default \texttt{libopenblas.so}, pass \texttt{LIBBLAS=-l\$(YOURBLAS)} and \texttt{LIBBLASNAME=lib\$(YOURBLAS)} to \texttt{make}, replacing \texttt{\$(YOURBLAS)} with the name of your library. You can also add \texttt{.so.0} to the name of the library if you want your package to work without requiring the unversioned \texttt{.so} symlink.



Finally, OpenBLAS includes its own optimized version of LAPACK. If you set \texttt{USE\_SYSTEM\_BLAS=1} and \texttt{USE\_SYSTEM\_LAPACK=1}, you should also set \texttt{LIBLAPACK=-l\$(YOURBLAS)} and \texttt{LIBLAPACKNAME=lib\$(YOURBLAS)}. Else, the reference LAPACK will be used and performance will typically be much lower.



Starting with Julia 1.7, Julia uses \href{https://github.com/JuliaLinearAlgebra/libblastrampoline}{libblastrampoline} to pick a different BLAS at runtime.



\hypertarget{13254364787517196998}{}


\section{Point releasing 101}



Creating a point/patch release consists of several distinct steps.



\hypertarget{12357762204955987571}{}


\subsection{Backporting commits}



Some pull requests are labeled {\textquotedbl}backport pending x.y{\textquotedbl}, e.g. {\textquotedbl}backport pending 0.6{\textquotedbl}. This designates that the next subsequent release tagged from the release-x.y branch should include the commit(s) in that pull request. Once the pull request is merged into master, each of the commits should be \href{https://git-scm.com/docs/git-cherry-pick}{cherry picked} to a dedicated branch that will ultimately be merged into release-x.y.



\hypertarget{4966159163932283918}{}


\subsubsection{Creating a backports branch}



First, create a new branch based on release-x.y. The typical convention for Julia branches is to prefix the branch name with your initials if it{\textquotesingle}s intended to be a personal branch. For the sake of example, we{\textquotesingle}ll say that the author of the branch is Jane Smith.




\begin{lstlisting}
git fetch origin
git checkout release-x.y
git rebase origin/release-x.y
git checkout -b js/backport-x.y
\end{lstlisting}



This ensures that your local copy of release-x.y is up to date with origin before you create a new branch from it.



\hypertarget{6792962923353110702}{}


\subsubsection{Cherry picking commits}



Now we do the actual backporting. Find all merged pull requests labeled {\textquotedbl}backport pending x.y{\textquotedbl} in the GitHub web UI. For each of these, scroll to the bottom where it says {\textquotedbl}someperson merged commit \texttt{123abc} into \texttt{master} XX minutes ago{\textquotedbl}. Note that the commit name is a link; if you click it, you{\textquotesingle}ll be shown the contents of the commit. If this page shows that \texttt{123abc} is a merge commit, go back to the PR page–-we don{\textquotesingle}t want merge commits, we want the actual commits. However, if this does not show a merge commit, it means that the PR was squash-merged. In that case, use the git SHA of the commit, listed next to commit on this page.



Once you have the SHA of the commit, cherry-pick it onto the backporting branch:




\begin{lstlisting}
git cherry-pick -x -e <sha>
\end{lstlisting}



There may be conflicts which need to be resolved manually. Once conflicts are resolved (if applicable), add a reference to the GitHub pull request that introduced the commit in the body of the commit message.



After all of the relevant commits are on the backports branch, push the branch to GitHub.



\hypertarget{14599065304395472247}{}


\subsection{Checking for performance regressions}



Point releases should never introduce performance regressions. Luckily the Julia benchmarking bot, Nanosoldier, can run benchmarks against any branch, not just master. In this case we want to check the benchmark results of js/backport-x.y against release-x.y. To do this, awaken the Nanosoldier from his robotic slumber using a comment on your backporting pull request:




\begin{lstlisting}
@nanosoldier `runbenchmarks(ALL, vs=":release-x.y")`
\end{lstlisting}



This will run all registered benchmarks on release-x.y and js/backport-x.y and produce a summary of results, marking all improvements and regressions.



If Nanosoldier finds any regressions, try verifying locally and rerun Nanosoldier if necessary. If the regressions are deemed to be real rather than just noise, you{\textquotesingle}ll have to find a commit on master to backport that fixes it if one exists, otherwise you should determine what caused the regression and submit a patch (or get someone who knows the code to submit a patch) to master, then backport the commit once that{\textquotesingle}s merged. (Or submit a patch directly to the backport branch if appropriate.)



\hypertarget{11019808236351530595}{}


\subsection{Building test binaries}



After the backport PR has been merged into the \texttt{release-x.y} branch, update your local clone of Julia, then get the SHA of the branch using




\begin{lstlisting}
git rev-parse origin/release-x.y
\end{lstlisting}



Keep that handy, as it{\textquotesingle}s what you{\textquotesingle}ll enter in the {\textquotedbl}Revision{\textquotedbl} field in the buildbot UI.



For now, all you need are binaries for Linux x86-64, since this is what{\textquotesingle}s used for running PackageEvaluator. Go to https://buildog.julialang.org, submit a job for \texttt{nuke\_linux64}, then queue up a job for \texttt{package\_linux64}, providing the SHA as the revision. When the packaging job completes, it will upload the binary to the \texttt{julialang2} bucket on AWS. Retrieve the URL, as it will be used for PackageEvaluator.



\hypertarget{18088580952798281058}{}


\subsection{Checking for package breakages}



Point releases should never break packages, with the possible exception of packages that are doing some seriously questionable hacks using Base internals that are not intended to be user-facing. (In those cases, maybe have a word with the package author.)



Checking whether changes made in the forthcoming new version will break packages can be accomplished using \href{https://github.com/JuliaCI/PackageEvaluator.jl}{PackageEvaluator}, often called {\textquotedbl}PkgEval{\textquotedbl} for short. PkgEval is what populates the status badges on GitHub repos and on pkg.julialang.org. It typically runs on one of the non-benchmarking nodes of Nanosoldier and uses Vagrant to perform its duties in separate, parallel VirtualBox virtual machines.



\hypertarget{8830711245206929156}{}


\subsubsection{Setting up PackageEvaluator}



Clone PackageEvaluator and create a branch called \texttt{backport-x.y.z}, and check it out. Note that the required changes are a little hacky and confusing, and hopefully that will be addressed in a future version of PackageEvaluator. The changes to make will be modeled off of \href{https://github.com/JuliaCI/PackageEvaluator.jl/commit/5ba6a3b000e7a3793391d16f695c8704b91d6016}{this commit}.



The setup script takes its first argument as the version of Julia to run and the second as the range of package names (AK for packages named A-K, LZ for L-Z). The basic idea is that we{\textquotesingle}re going to tweak that a bit to run only two versions of Julia, the current x.y release and our backport version, each with three ranges of packages.



In the linked diff, we{\textquotesingle}re saying that if the second argument is LZ, use the binaries built from our backport branch, otherwise (AK) use the release binaries. Then we{\textquotesingle}re using the first argument to run a section of the package list: A-F for input 0.4, G-N for 0.5, and O-Z for 0.6.



\hypertarget{12273793797160936753}{}


\subsubsection{Running PackageEvaluator}



To run PkgEval, find a hefty enough machine (such as Nanosoldier node 1), then run




\begin{lstlisting}
git clone https://github.com/JuliaCI/PackageEvaluator.jl.git
cd PackageEvaluator.jl/scripts
git checkout backport-x.y.z
./runvagrant.sh
\end{lstlisting}



This produces some folders in the scripts/ directory. The folder names and their contents are decoded below:




\begin{table}[h]

\begin{tabulary}{\linewidth}{|C|C|C|}
\hline
Folder name & Julia version & Package range \\
\hline
0.4AK & Release & A-F \\
\hline
0.4LZ & Backport & A-F \\
\hline
0.5AK & Release & G-N \\
\hline
0.5LZ & Backport & G-N \\
\hline
0.6AK & Release & O-Z \\
\hline
0.6LZ & Backport & O-Z \\
\hline
\end{tabulary}

\end{table}



\hypertarget{9508198716944071618}{}


\subsubsection{Investigating results}



Once that{\textquotesingle}s done, you can use \texttt{./summary.sh} from that same directory to produce a summary report of the findings. We{\textquotesingle}ll do so for each of the folders to aggregate overall results by version.




\begin{lstlisting}
./summary.sh 0.4AK/*.json > summary_release.txt
./summary.sh 0.5AK/*.json >> summary_release.txt
./summary.sh 0.6AK/*.json >> summary_release.txt
./summary.sh 0.4LZ/*.json > summary_backport.txt
./summary.sh 0.5LZ/*.json >> summary_backport.txt
./summary.sh 0.6LZ/*.json >> summary_backport.txt
\end{lstlisting}



Now we have two files, \texttt{summary\_release.txt} and \texttt{summary\_backport.txt}, containing the PackageEvaluator test results (pass/fail) for each package for the two versions.



To make these easier to ingest into a Julia, we{\textquotesingle}ll convert them into CSV files then use the DataFrames package to process the results. To convert to CSV, copy each .txt file to a corresponding .csv file, then enter Vim and execute \texttt{ggVGI{\textquotedbl}<esc>} then \texttt{:\%s/{\textbackslash}.json /{\textquotedbl},/g}. (You don{\textquotesingle}t have to use Vim; this just is one way to do it.) Now process the results with Julia code similar to the following.




\begin{minted}{julia}
using DataFrames

release = readtable("summary_release.csv", header=false, names=[:package, :release])
backport = readtable("summary_backport.csv", header=false, names=[:package, :backport])

results = join(release, backport, on=:package, kind=:outer)

for result in eachrow(results)
    a = result[:release]
    b = result[:backport]
    if (isna(a) && !isna(b)) || (isna(b) && !isna(a))
        color = :yellow
    elseif a != b && occursin("pass", b)
        color = :green
    elseif a != b
        color = :red
    else
        continue
    end
    printstyled(result[:package], ": Release ", a, " -> Backport ", b, "\n", color=color)
end
\end{minted}



This will write color-coded lines to \texttt{stdout}. All lines in red must be investigated as they signify potential breakages caused by the backport version. Lines in yellow should be looked into since it means a package ran on one version but not on the other for some reason. If you find that your backported branch is causing breakages, use \texttt{git bisect} to identify the problematic commits, \texttt{git revert} those commits, and repeat the process.



\hypertarget{10606148317076151231}{}


\subsection{Merging backports into the release branch}



After you have ensured that



\begin{itemize}
\item the backported commits pass all of Julia{\textquotesingle}s unit tests,


\item there are no performance regressions introduced by the backported commits as compared to the release branch, and


\item the backported commits do not break any registered packages,

\end{itemize}


then the backport branch is ready to be merged into release-x.y. Once it{\textquotesingle}s merged, go through and remove the {\textquotedbl}backport pending x.y{\textquotedbl} label from all pull requests containing the commits that have been backported. Do not remove the label from PRs that have not been backported.



The release-x.y branch should now contain all of the new commits. The last thing we want to do to the branch is to adjust the version number. To do this, submit a PR against release-x.y that edits the VERSION file to remove \texttt{-pre} from the version number. Once that{\textquotesingle}s merged, we{\textquotesingle}re ready to tag.



\hypertarget{17687518186078684350}{}


\subsection{Tagging the release}



It{\textquotesingle}s time! Check out the release-x.y branch and make sure that your local copy of the branch is up to date with the remote branch. At the command line, run




\begin{lstlisting}
git tag v$(cat VERSION)
git push --tags
\end{lstlisting}



This creates the tag locally and pushes it to GitHub.



After tagging the release, submit another PR to release-x.y to bump the patch number and add \texttt{-pre} back to the end. This denotes that the branch state reflects a prerelease version of the next point release in the x.y series.



Follow the remaining directions in the Makefile.



\hypertarget{3023354583729859190}{}


\subsection{Signing binaries}



Some of these steps will require secure passwords. To obtain the appropriate passwords, contact Elliot Saba (staticfloat) or Alex Arslan (ararslan). Note that code signing for each platform must be performed on that platform (e.g. Windows signing must be done on Windows, etc.).



\hypertarget{3777966428630428250}{}


\subsubsection{Linux}



Code signing must be done manually on Linux, but it{\textquotesingle}s quite simple. First obtain the file \texttt{julia.key} from the CodeSigning folder in the \texttt{juliasecure} AWS bucket. Add this to your GnuPG keyring using




\begin{lstlisting}
gpg --import julia.key
\end{lstlisting}



This will require entering a password that you must obtain from Elliot or Alex. Next, set the trust level for the key to maximum. Start by entering a \texttt{gpg} session:




\begin{lstlisting}
gpg --edit-key julia
\end{lstlisting}



At the prompt, type \texttt{trust}, then when asked for a trust level, provide the maximum available (likely 5). Exit GnuPG.



Now, for each of the Linux tarballs that were built on the buildbots, enter




\begin{lstlisting}
gpg -u julia --armor --detach-sig julia-x.y.z-linux-<arch>.tar.gz
\end{lstlisting}



This will produce a corresponding .asc file for each tarball. And that{\textquotesingle}s it!



\hypertarget{12383401899447558259}{}


\subsubsection{macOS}



Code signing should happen automatically on the macOS buildbots. However, it{\textquotesingle}s important to verify that it was successful. On a system or virtual machine running macOS, download the .dmg file that was built on the buildbots. For the sake of example, say that the .dmg file is called \texttt{julia-x.y.z-osx.dmg}. Run




\begin{lstlisting}
mkdir ./jlmnt
hdiutil mount -readonly -mountpoint ./jlmnt julia-x.y.z-osx.dmg
codesign -v jlmnt/Julia-x.y.app
\end{lstlisting}



Be sure to note the name of the mounted disk listed when mounting! For the sake of example, we{\textquotesingle}ll assume this is \texttt{disk3}. If the code signing verification exited successfully, there will be no output from the \texttt{codesign} step. If it was indeed successful, you can detach the .dmg now:




\begin{lstlisting}
hdiutil eject /dev/disk3
rm -rf ./jlmnt
\end{lstlisting}



If you get a message like



\begin{quote}
Julia-x.y.app: code object is not signed at all

\end{quote}


then you{\textquotesingle}ll need to sign manually.



To sign manually, first retrieve the OS X certificates from the CodeSigning folder in the \texttt{juliasecure} bucket on AWS. Add the .p12 file to your keychain using Keychain.app. Ask Elliot Saba (staticfloat) or Alex Arslan (ararslan) for the password for the key. Now run




\begin{lstlisting}
hdiutil convert julia-x.y.z-osx.dmg -format UDRW -o julia-x.y.z-osx_writable.dmg
mkdir ./jlmnt
hdiutil mount -mountpoint julia-x.y.z-osx_writable.dmg
codesign -s "AFB379C0B4CBD9DB9A762797FC2AB5460A2B0DBE" --deep jlmnt/Julia-x.y.app
\end{lstlisting}



This may fail with a message like



\begin{quote}
Julia-x.y.app: resource fork, Finder information, or similar detritus not allowed

\end{quote}


If that{\textquotesingle}s the case, you{\textquotesingle}ll need to remove extraneous attributes:




\begin{lstlisting}
xattr -cr jlmnt/Julia-x.y.app
\end{lstlisting}



Then retry code signing. If that produces no errors, retry verification. If all is now well, unmount the writable .dmg and convert it back to read-only:




\begin{lstlisting}
hdiutil eject /dev/disk3
rm -rf ./jlmnt
hdiutil convert julia-x.y.z-osx_writable.dmg -format UDZO -o julia-x.y.z-osx_fixed.dmg
\end{lstlisting}



Verify that the resulting .dmg is in fact fixed by double clicking it. If everything looks good, eject it then drop the \texttt{\_fixed} suffix from the name. And that{\textquotesingle}s it!



\hypertarget{4146385850442974710}{}


\subsubsection{Windows}



Signing must be performed manually on Windows. First obtain the Windows 10 SDK, which contains the necessary signing utilities, from the Microsoft website. We need the \texttt{SignTool} utility which should have been installed somewhere like \texttt{C:{\textbackslash}Program Files (x86){\textbackslash}Windows Kits{\textbackslash}10{\textbackslash}App Certification Kit}. Grab the Windows certificate files from CodeSigning on \texttt{juliasecure} and put them in the same directory as the executables. Open a Windows CMD window, \texttt{cd} to where all the files are, and run




\begin{lstlisting}
set PATH=%PATH%;C:\Program Files (x86)\Windows Kits\10\App Certification Kit;
signtool sign /f julia-windows-code-sign_2017.p12 /p "PASSWORD" ^
   /t http://timestamp.verisign.com/scripts/timstamp.dll ^
   /v julia-x.y.z-win32.exe
\end{lstlisting}



Note that \texttt{{\textasciicircum}} is a line continuation character in Windows CMD and \texttt{PASSWORD} is a placeholder for the password for this certificate. As usual, contact Elliot or Alex for passwords. If there are no errors, we{\textquotesingle}re all good!



\hypertarget{13693621271485293889}{}


\subsection{Uploading binaries}



Now that everything is signed, we need to upload the binaries to AWS. You can use a program like Cyberduck or the \texttt{aws} command line utility. The binaries should go in the \texttt{julialang2} bucket in the appropriate folders. For example, Linux x86-64 goes in \texttt{julialang2/bin/linux/x.y}. Be sure to delete the current \texttt{julia-x.y-latest-linux-<arch>.tar.gz} file and replace it with a duplicate of \texttt{julia-x.y.z-linux-<arch>.tar.gz}.



We also need to upload the checksums for everything we{\textquotesingle}ve built, including the source tarballs and all release binaries. This is simple:




\begin{lstlisting}
shasum -a 256 julia-x.y.z* | grep -v -e sha256 -e md5 -e asc > julia-x.y.z.sha256
md5sum julia-x.y.z* | grep -v -e sha256 -e md5 -e asc > julia-x.y.z.md5
\end{lstlisting}



Note that if you{\textquotesingle}re running those commands on macOS, you{\textquotesingle}ll get very slightly different output, which can be reformatted by looking at an existing file. Mac users will also need to use \texttt{md5 -r} instead of \texttt{md5sum}. Upload the .md5 and .sha256 files to \texttt{julialang2/bin/checksums} on AWS.



Ensure that the permissions on AWS for all uploaded files are set to {\textquotedbl}Everyone: READ.{\textquotedbl}



For each file we{\textquotesingle}ve uploaded, we need to purge the Fastly cache so that the links on the website point to the updated files. As an example:




\begin{lstlisting}
curl -X PURGE https://julialang-s3.julialang.org/bin/checksums/julia-x.y.z.sha256
\end{lstlisting}



Sometimes this isn{\textquotesingle}t necessary but it{\textquotesingle}s good to do anyway.
